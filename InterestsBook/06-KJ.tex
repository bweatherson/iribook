\chapter{Knowledge and Justification}
\markright{Knowledge and Justification}

\section{Interest-Relative Defeaters}

As I said at the top, I've changed my view from Doxastic IRI to Non-Doxastic IRI. The change of heart is occasioned by cases like the following, where the agent is mistaken, and hence ignorant, about the odds at which she is offered a bet on $p$. In fact the odds are much longer than she thinks. Relative to what she stands to win, the stakes are too high.

\subsection{The Coraline Example}
The problem for Doxastic IRI arises because of cases like that of Coraline. Here's what we're going to stipulate about Coraline.

\begin{itemize*}
\item She knows that $p$ and $q$ are independent, so her credence in any conjunction where one conjunct is a member of  $\{p,  \neg p\}$ and the other is a member of $\{q, \neg q\}$ will be the product of her credences in the conjuncts.
\item Her credence in $p$ is 0.99, just as the evidence supports.
\item Her credence in $q$ is also 0.99. This is unfortunate, since the rational credence in $q$ given her evidence is 0.01.
\item The only relevant question for her which is sensitive to $p$ is whether to take or decline a bet with the following payoff structure.\footnote{I'm more interested in the abstract structure of the case than in whether any real-life situation is modelled by just this structure. But it might be worth noting the rough kind of situation where this kind of situation can arise. So let's say Coraline has a particular bank account that is uninsured, but which currently paying 10\% interest, and she is deciding whether to deposit another \$1000 in it. Then $p$ is the proposition that the bank will not collapse, and she'll get her money back, and $q$ is the proposition that the interest will stay at 10\%. To make the model exact, we have to also assume that if the interest rate on her account doesn't stay at 10\%, it falls to 0.1\%. And we have to assume that the interest rate and the bank's collapse are probabilistically independent. Neither of these are at all realistic, but a realistic case would simply be more complicated, and the complications would obscure the philosophically interesting point.} (Assume that the marginal utility of money is close enough to constant that expected dollar returns correlate more or less precisely with expected utility returns.)
\end{itemize*}

\begin{center}
\begin{tabular}{r c c c}
 & \textbf{$p \wedge q$} & \textbf{$p \wedge \neg q$} & \textbf{$\neg p$} \\
\textbf{Take bet} & \$100 & \$1 & $-\$1000$ \\
\textbf{Decline bet} & 0 & 0 & 0 \\
\end{tabular}
\end{center}

\noindent As can be easily computed, the expected utility of taking the bet given her credences is positive, it is just over \$89. And Coraline takes the bet. She doesn't compute the expected utility, but she is sensitive to it.\footnote{If she did compute the expected utility, then one of the things that would be salient for her is the expected utility of the bet. And the expected utility of the bet is different to its expected utility given $p$. So if that expected utility is salient, she doesn't believe $p$. And it's going to be important to what follows that she \textit{does} believe $p$.} That is, had the expected utility given her credences been close to 0, she would have not acted until she made a computation. But from her perspective this looks like basically a free \$100, so she takes it. Happily, this all turns out well enough, since $p$ is true. But it was a dumb thing to do. The expected utility of taking the bet given her evidence is negative, it is a little under -\$8. So she isn't warranted, given her evidence, in taking the bet.

\subsection{What Coraline Knows and What She Believes}

Assume, for \textit{reductio}, that Coraline knows that $p$. Then the choice she faces looks like this.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{$q$} & \textbf{$\neg q$}  \\
\textbf{Take bet} & \$100 & \$1 \\
\textbf{Decline bet} & 0 & 0\\
\end{tabular}
\end{center}

\noindent Since taking the bet dominates declining the bet, she should take the bet if this is the correct representation of her situation. She shouldn't take the bet, so by \textit{modus tollens}, that can't be the correct representation of her situation. If she knew $p$, that would be the correct representation of her situation. So, again by \textit{modus tollens}, she doesn't know $p$.

Now let's consider three possible explanations of why she doesn't know that $p$.

\begin{enumerate*}
\item She doesn't have enough evidence to know that $p$, independent of the practical stakes.
\item In virtue of the practical stakes, she doesn't believe that $p$;
\item In virtue of the practical stakes, she doesn't justifiably believe that $p$, although she does actually believe it.
\item In virtue of the practical stakes, she doesn't know that $p$, although she does justifiably believe it.
\end{enumerate*}

\noindent I think option 1 is implausibly sceptical, at least if applied to all cases like Coraline's. I've said that the probability of $p$ is 0.99, but it should be clear that all that matters to generating a case like this is that $p$ is not completely certain. Unless knowledge requires certainty, we'll be able to generate Coraline-like cases where there is sufficient evidence for knowledge. So that's ruled out.

Option 2 is basically what the Doxastic IRI theorist has to say. If Coraline has enough evidence to know $p$, but doesn't know $p$ due to practical stakes, then the Doxastic IRI theorist is committed to saying that the practical stakes block \textit{belief} in $p$. That's the Doxastic IRI position; stakes matter to knowledge because they matter to belief.

But that's also an implausible description of Coraline's situation. She is very confident that $p$. Her confidence is grounded in the evidence in the right way. She is insensitive in her actual deliberations to the difference between her evidence for $p$ and evidence that guarantees $p$. She would become sensitive to that difference if someone offered her a bet that she knew was a 1000-to-1 bet on $p$, but she doesn't know that's what is on offer. In short, there is no difference between her unconditional attitudes, and her attitudes conditional on $p$, when it comes to any live question. That's enough, I think, for belief. So she believes that $p$. And that's bad news for the Doxastic IRI theorist; since it means here that stakes matter to knowledge without mattering to belief. I conclude, reluctantly, that Doxastic IRI is false.

\subsection{Stakes as Defeaters}

That still leaves two options remaining, what I've called options 3 and 4 above. Option 3, if suitably generalised, says that knowledge is practically sensitive because the justification condition on belief is practically sensitive. Option 4 says that practical considerations impact knowledge directly. As I read them, Jeremy Fantl and Matthew McGrath defend a version of Option 3. In the next and last subsection, I'll argue against that position. But first I want to sketch what a position like option 4 would look like. 

Knowledge, unlike justification, requires a certain amount of internal coherence amongst mental states. Consider the following story from David Lewis:

\begin{quote}
I speak from experience as the repository of a mildly inconsistent corpus. I used to think that Nassau Street ran roughly east-west; that the railroad nearby ran roughly north-south; and that the two were roughly parallel. \citep[436]{Lewis1982c} 
\end{quote}

\noindent I think in that case that Lewis doesn't know that Nassau Street runs roughly east-west. (From here on, call the proposition that Nassau Street runs roughly east-west $N$.) If his belief that it does was acquired and sustained in a suitably reliable way, then he may well have a justified belief that $N$. But the lack of coherence with the rest of his cognitive system, I think, defeats any claim to knowledge he has.

Coherence isn't just a requirement on belief; other states can cohere or be incoherent. Assume Lewis corrects the incoherence in his beliefs, and drops the belief that Nassau Street the railway are roughly parallel. Still, if Lewis believed that $N$, preferred doing $\varphi$ to doing $\psi$ conditional on $N$, but actually preferred doing $\psi$ to doing $\varphi$, his cognitive system would also be in tension. That tension could, I think, be sufficient to defeat a claim to know that $N$.

And it isn't just a requirement on actual states; it can be a requirement on rational states. Assume Lewis believed that $N$, preferred doing $\varphi$ to doing $\psi$ conditional on $N$, and preferred doing $\varphi$ to doing $\psi$, but should have preferred doing $\psi$ to doing $\varphi$ given his interests. Then I think the fact that the last preference is irrational, plus the fact that were it corrected there would be incoherence in his cognitive states defeats the claim to know that $N$.

A concrete example of this helps make clear why such a view is attractive, and why it faces difficulties. Assume there is a bet that wins \$2 if $N$, and loses \$10 if not. Let $\varphi$ be taking that bet, and $\psi$ be declining it. Assume Lewis shouldn't take that bet; he doesnt have enough evidence to do so. Then he clearly doesn't know that $N$. If he knew that $N$, $\varphi$ would dominate $\psi$, and hence be rational. But it isn't, so $N$ isn't known. And that's true whether Lewis's preferences between $\varphi$ and $\psi$ are rational or irrational.

Attentive readers will see where this is going. Change the bet so it wins a penny if $N$, and loses \$1,000 if not. Unless Lewis's evidence that $N$ is incredibly strong, he shouldn't take the bet. So, by the same reasoning, he doesn't know that $N$. And we're back saying that knowledge requires incredibly strong evidence. The solution, I say, is to put a pragmatic restriction on the kinds of incoherence that matter to knowledge. Incoherence with respect to irrelevant questions, such as whether to bet on $N$ at extremely long odds, doesn't matter for knowledge. Incoherence (or coherence obtained only through irrationality) does. The reason, I think, that Non-Doxastic IRI is true is that this coherence-based defeater is sensitive to practical interests.

The string of cases about Lewis and $N$ has ended up close to the Coraline example. We already concluded that Coraline didn't know $p$. Now we have a story about why she doesn't know that $p$. Her belief that $p$ doesn't cohere sufficiently well with what she should believe, namely that it would be wrong to take the bet. If all that is correct, just one question remains: does this coherence-based defeater also defeat Coraline's claim to have a justified belief that $p$? I say it does not, for three reasons.

First, her attitude towards $p$ tracks the evidence perfectly. She is making no mistakes with respect to $p$. She is making a mistake with respect to $q$, but not with respect to $p$. So her attitude towards $p$, i.e. belief, is justified.

Second, talking about beliefs and talking about credences are simply two ways of modelling the very same things, namely minds. If the agent both has a credence 0.99 in $p$, and believes that $p$, these are not two different states. Rather, there is one state of the agent, and two different ways of modelling it. So it is implausible to apply different valuations to the state depending on which modelling tools we choose to use. That is, it's implausible to say that while we're modelling the agent with credences, the state is justified, but when we change tools, and start using beliefs, the state is unjustified. Given this outlook on beliefs and credences, it is natural to say that her belief is justified. Natural, but not compulsory, for reasons Jeremy Fantl pointed out to me.\footnote{The following isn't Fantl's example, but I think it makes much the same point as the examples he suggested.} We don't want a metaphysics on which persons and philosophers are separate entities. Yet we can say that someone is a good person but a bad philosopher. Normative statuses can differ depending on which property of a thing we are considering. That suggests it is at least coherent to say that one and the same state is a good credence but a bad belief. But while this may be coherent, I don't think it is well motivated, and it is natural to have the evaluations go together.

Third, we don't \textit{need} to say that Coraline's belief in $p$ is unjustified in order to preserve other nice theories, in the way that we do need to say that she doesn't know $p$ in order to preserve a nice account of how we understand decision tables. It's this last point that I think Fantl and McGrath, who say that the belief is unjustified, would reject. So let's conclude with a look at their arguments.

\subsection{Fantl and McGrath on Interest-Relativity}

Fantl and McGrath's argue for the principle (JJ), which entails that Coraline is not justified in believing $p$.

\begin{description}
\item[(JJ)] If you are justified in believing that $p$, then $p$ is warranted enough to justify you in $\varphi$-ing, for any $\varphi$. \cite[99]{FantlMcGrath2009}
\end{description}

\noindent In practice, what this means is that there can't be a salient $p, \varphi$ such that:

\begin{itemize*}
\item The agent is justified in believing $p$;
\item The agent is not warranted in doing $\varphi$; but
\item If the agent had more evidence for $p$, and nothing else, the agent would be warranted in doing $\varphi$.
\end{itemize*}

\noindent That is, once you've got enough evidence, or warrant, for justified belief in $p$, then you've got enough evidence for $p$ as matters for any decision you face. This seems intuitive, and Fantl and McGrath back up its intuitiveness with some nicely drawn examples. But I think it is false, and the Coraline example shows it is false. Coraline isn't justified in taking the bet, and is justified in believing $p$, but more evidence for $p$ would suffice for taking the bet. So Coraline's case shows that (JJ) is false. But there are a number of possible objections to that position. I'll spend the rest of this section, and this paper, going over them.\footnote{Thanks here to a long blog comments thread with Jeremy Fantl and Matthew McGrath for making me formulate these points much more carefully. The original thread is at \url{http://tar.weatherson.org/2010/03/31/do-justified-beliefs-justify-action/}.}

%\objrep is my objections and replies code. It is defined in collpapers.
%\argconc is my code for making the next label C. It is defined in collpapers.

\objrep{
The following argument shows that Coraline is not in fact justified in believing that $p$.

\begin{enumerate*}
\item $p$ entails that Coraline should take the bet, and Coraline knows this.
\item If $p$ entails something, and Coraline knows this, and she justifiably believes $p$, she is in a position to justifiably believe the thing entailed.
\item Coraline is not in a position to justifiably believe that she should take the bet.
\argconc
\item So, Coraline does not justifiably believe that $p$
\end{enumerate*}}
{The problem here is that premise 1 is false. What's true is that $p$ entails that Coraline will be better off taking the bet than declining it. But it doesn't follow that she should take the bet. Indeed, it isn't actually true that she should take the bet, even though $p$ is actually true. Not just is the entailment claim false, the world of the example is a counterinstance to it.

It might be controversial to use this very case to reject premise 1. But the falsity of premise 1 should be clear on independent grounds. What $p$ entails is that Coraline will be best off by taking the bet. But there are lots of things that will make me better off that I shouldn't do.  Imagine I'm standing by a roulette wheel, and the thing that will make me best off is betting heavily on the number than will actually come up. It doesn't follow that I should do that. Indeed, I should not do it. I shouldn't place any bets at all, since all the bets have a highly negative expected return. 

In short, all $p$ entails is that taking the bet will have the best consequences. Only a very crude kind of consequentialism would identify what I should do with what will have the best returns, and that crude consequentialism isn't true. So $p$ doesn't entail that Coraline should take the bet. So premise 1 is false.}

\objrep{
Even though $p$ doesn't \textit{entail} that Coraline should take the bet, it does provide inductive support for her taking the bet. So if she could justifiably believe $p$, she could justifiably (but non-deductively) infer that she should take the bet. Since she can't justifiably infer that, she isn't justified in taking the bet.
}
{The inductive inference here looks weak. One way to make the inductive inference work would be to deduce from $p$ that taking the bet will have the best outcomes, and infer from that that the bet should be taken. But the last step doesn't even look like a reliable ampliative inference. The usual situation is that the best outcome comes from taking an \textit{ex ante} unjustifiable risk.

It may seem better to use $p$ combined with the fact that conditional on $p$, taking the bet has the highest \textit{expected} utility. But actually that's still not much of a reason to take the bet. Think again about cases, completely normal cases, where the action with the best outcome is an \textit{ex ante} unjustifiable risk. Call that action $\varphi$, and let $B \varphi$ be the proposition that $\varphi$ has the best outcome. Then $B \varphi$ is true, and conditional on $B \varphi$, $\varphi$ has an excellent expected return. But doing $\varphi$ is still running a dumb risk. Since these kinds of cases are normal, it seems it will very often be the case that this form of inference leads from truth to falsity. So it's not a reliable inductive inference.}

\objrep{In the example, Coraline isn't just in a position to justifiably believe $p$, she is in a position to \textit{know} that she justifiably believes it. And from the fact that she justifiably believes $p$, and the fact that if $p$, then taking the bet has the best option, she can infer that she should take the bet.}
{It's possible at this point that we get to a dialectical impasse. I think this inference is non-deductive, because I think the example we're discussing here is one where the premises are true and the conclusion false. Presumably someone who doesn't like the example will think that it is a good deductive inference.

Having said that, the more complicated example at the end of \cite{Weatherson2005-WEACWD} was designed to raise the same problem without the consequence that if $p$ is true, the bet is sure to return a positive amount. In that example, conditionalising on $p$ means the bet has a positive expected return, but still possibly a negative return. But in that case (JJ) still failed. If accepting there are cases where an agent justifiably believes $p$, and hence justifiably believes taking the bet will return the best outcome, and knows all this, but still can't rationally bet on $p$ is too much to accept, that more complicated example might be more persuasive. Otherwise, I concede that someone who believes (JJ) and thinks rational agents can use it in their reasoning will not think that a particular case is a counterexample to (JJ).}

\objrep{If Coraline were ideal, then she wouldn't believe $p$. That's because if she were ideal, she would have a lower credence in $q$, and if that were the case, her credence in $p$ would have to be much higher (close to 0.999) in order to count as a belief. So her belief is not justified.}
{The premise here, that if Coraline were ideal she would not believe that $p$, is true. The conclusion, that she is not justified in believing $p$, does not follow. It's always a mistake to \textit{identify} what should be done with what is done in ideal circumstances. This is something that has long been known in economics. The \textit{locus classicus} of the view that this is a mistake is \cite{LipseyLancaster}. A similar point has been made in ethics in papers such as \cite{Watson1977} and \cite{KennettSmith1996b, KennettSmith1996a}. And it has been extended to epistemology by \cite{Williamson1998-WILCOK}.

All of these discussions have a common structure. It is first observed that the ideal is both $F$ and $G$. It is then stipulated that whatever happens, the thing being created (either a social system, an action, or a cognitive state) will not be $F$. It is then argued that given the stipulation, the thing being created should not be $G$. That is not just the claim that we shouldn't \textit{aim} to make the thing be $G$. It is, rather, that in many cases being $G$ is not the best way to be, given that $F$-ness will not be achieved. Lipsey and Lancaster argue that (in an admittedly idealised model) that it is actually quite unusual for $G$ to be best given that the system being created will not be $F$.

It's not too hard to come up with examples that fit this structure. Following \cite[209]{Williamson2000-WILKAI}, we might note that I'm justified in believing that there are no ideal cognitive agents, although were I ideal I would not believe this. Or imagine a student taking a ten question mathematics exam who has no idea how to answer the last question. She knows an ideal student would correctly answer an even number of questions, but that's no reason for her to throw out her good answer to question nine. In general, once we have stipulated one departure from the ideal, there's no reason to assign any positive status to other similarities to the idea. In particular, given that Coraline has an irrational view towards $q$, she won't perfectly match up with the ideal, so there's no reason it's good to agree with the ideal in other respects, such as not believing $p$.

Stepping back a bit, there's a reason the interest-relative theory says that the ideal and justification come apart right here. On the interest-relative theory, as on any pragmatic theory of mental states, the \textit{identification} of mental states is a somewhat holistic matter. Something is a belief in virtue of its position in a much broader network. But the \textit{evaluation} of belief is (relatively) atomistic. That's why Coraline is justified in believing $p$, although if she were wiser she would not believe it. If she were wiser, i.e., if she had the right attitude towards $q$, the very same credence in $p$ would not count as a belief. Whether her state counts as a belief, that is, depends on wide-ranging features of her cognitive system. But whether the state is justified depends on more local factors, and in local respects she is doing everything right.}

\objrep{If Coraline is justified in believing $p$, then Coraline can use $p$ as a premise in practical reasoning. If Coraline can use $p$ as a premise in practical reasoning, and $p$ is true, and her belief in $p$ is not Gettiered, then she knows $p$. By hypothesis, her belief is true, and her belief is not Gettiered. So she should know $p$. But she doesn't know $p$. So by several steps of modus tollens, she isn't justified in believing $p$.\footnote{Compare the `subtraction argument' on page 99 of \cite{FantlMcGrath2009}.}}
{This  objection this one turns on an equivocation over the neologism `Gettiered'. Some epistemologists use this to simply mean that a belief is justified and true without constituting knowledge. By that standard, the third sentence is false. Or, at least, we haven't been given any reason to think that it is true. Given everything else that's said, the third sentence is a raw assertion that Coraline knows that $p$, and I don't think we should accept that.

The other way epistemologists sometimes use the term is to pick out justified true beliefs that fail to be knowledge for the reasons that the beliefs in the original examples from \cite{Gettier1963} fail to be knowledge. That is, it picks out a property that beliefs have when they are derived from a false lemma, or whatever similar property is held to be doing the work in the original Gettier examples. Now on this reading, Coraline's belief that $p$ is not Gettiered. But it doesn't follow that it is known. There's no reason, once we've given up on the JTB theory of knowledge, to think that whatever goes wrong in Gettier's examples is the \textit{only} way for a justified true belief to fall short of knowledge. It could be that there's a practical defeater, as in this case. So the second sentence of the objection is false, and the objection again fails.

Once we have an expansive theory of defeaters, as I've adopted here, it becomes problematic to describe the case in the language Fantl and McGrath use. They focus a lot on whether agents like Coraline have `knowledge-level justification' for $p$, which is defined as ``justification strong enough so that shortcomings in your strength of justification stand in the way of your knowing''. \citep[97]{FantlMcGrath2009}. An important part of their argument is that an agent is justified in believing $p$ iff they have knowledge-level justification for $p$. I haven't addressed this argument, so I'm not really addressing the case on their terms.

Well, does Coraline have knowledge-level justification for $p$? I'm not sure, because I'm not sure I grasp this concept. Compare the agent in Harman's dead dictator case \citep[75]{Harman1973}. Does she have knowledge-level justification that the dictator is dead? In one sense yes; it is the existence of misleading news sources that stops her knowing. In another sense no; she doesn't know, but if she had better evidence (e.g., seeing the death happen) she would know. I want to say the same thing about Coraline, and that makes it hard to translate the Coraline case into Fantl and McGrath's terminology.}


\section{Measurement, Justification and Knowledge}

Williamson's core example involves detecting the angle of a pointer on a wheel by eyesight. For various reasons, I find it easier to think about a slightly different example: measuring a quantity using a digital measurement device. This change has some costs relative to Williamson's version -- for one thing, if we are measuring a quantity it might seem that the margin of error is related to the quantity measured. If I eyeball how many stories tall a building is, my margin of error is 0 if the building is 1-2 stories tall, and over 10 if the building is as tall as the World Trade Center. But this problem is not as pressing for digital devices, which are often very \textit{unreliable} for small quantities. And, at least relative to my preferences, the familiarity of quantities makes up for the loss of symmetry properties involved in angular measurement.\footnote{There's one other change that might make a larger difference. When we use a digital device, there's a very clear separation between what the input is, and what we do with that input. That kind of factorisation is not nearly as easy when we are eyeballing something, and may well be impossible. But I think it makes the discussion smoother to have a case where we can easily separate the input from the processing.}

To make things explicit, I'll imagine the agent $S$ is using a digital scale. The scale has a \textbf{margin of error} $m$. That means that if the reading, i.e., the \textbf{apparent mass} is $a$, then the agent is justified in believing that the mass is in $[a-m, a+m]$. We will assume that $a$ and $m$ are luminous; i.e., the agent knows their values, and knows she knows them, and so on. This is a relatively harmless idealisation for $a$; it is pretty clear what a digital scale reads.\footnote{This isn't always true. If a scale flickers between reading 832g and 833g, it takes a bit of skill to determine what \textit{the reading} is. But we'll assume it is clear in this case. On an analogue scale, the luminosity assumption is rather implausible, since it is possible to eyeball with less than perfect accuracy how far between one marker and the next the pointer is.} It is a somewhat less plausible assumption for $m$. But we'll assume that $S$ has been very diligent about calibrating her scale, and that the calibration has been recently and skillfully carried out, so in practice $m$ can be assessed very accurately. 

Note that when I say the scale has a margin of error, this is a tensed claim. The margin will change over time, and with a change of environment and so on. Many scales will come with a designed margin of error, which may be printed on the scale or even on the display. I'm \textit{not} assuming that $m$ is that margin. In fact, it will be easiest in what follows if we assume that $m$ is much much larger than this printed value. This could be because the scale is old, or because it is being used in a noisy environment. What we are assuming is that $S$ has very carefully assessed the accuracy of the scale in the environment she is using it, and correctly concluded that when it reads $a$, she is justified in believing that the mass of the object on the scale is in $[a -m, a+m]$.

We'll make three further assumptions about $m$ that strike me as plausible, but which may I guess be challenged. I need to be a bit careful with terminology to set out the first one. I'll use $V$ and $v$ as variables that both pick out the \textbf{true value} of the mass. The difference is that $v$ picks it out rigidly, while $V$ picks out the value of the mass in any world under consideration. Think of $V$ as shorthand for \textit{the mass of the object} and $v$ as shorthand for \textit{the actual mass of the object}. (More carefully, $V$ is a \textit{random} variable, while $v$ is a standard, rigid, variable.) Our first assumption then is that $m$ is also related to what the agent can know. In particular, we'll assume that if the reading $a$ equals $v$, then the agent can know that $V \in [a-m, a+m]$, and can't know anything stronger than that. That is, the margin of error for justification equals, in the best case, the margin of error for knowledge. The second is that the scale has a readout that is finer than $m$. This is usually the case; the last digit on a digital scale is often not significant. The final assumption is that it is metaphysically possible that the scale has an error on an occasion that is greater than $m$. This is a kind of fallibilism assumption -- saying that the margin of error is $m$ does not mean there is anything incoherent about talking about cases where the error on an occasion is greater than $m$.

This error term will do a lot of work in what follows, so I'll  use $e$ to be the \textbf{error} of the measurement, i.e., $|a-v|$. For ease of exposition, I'll assume that $a \geq v$, i.e., that any error is on the high side. But this is entirely dispensible, and just lets me drop some disjunctions later on.

Now we are in a position to state Williamson's argument. Assume that on a particular occasion, $0 < e < m$. Perhaps $v = 830, m =10$ and $a = 832$, so $e = 2$. Williamson appears to make the following two assumptions.\footnote{I'm not actually sure whether Williamson \textit{makes} the first, or thinks it is the kind of thing anyone who thinks justification is prior to knowledge should make.}

\begin{enumerate*}
\item The agent is justified in believing what they would know if appearances matched reality, i.e., if $V$ equalled $a$.
\item The agent cannot come to know something about $V$ on the basis of a suboptimal measurement that they could not also know on the basis of an optimal measurement.
\end{enumerate*}

\noindent I'm assuming here that the optimal measurement displays the correct mass. I don't assume the actual measurement is \textit{wrong}. That would require saying something implausible about the semantic content of the display. It's not obvious that the display has a content that could be true or false, and if it does have such a content it might be true. (For instance, the content might be that the object on the scale has a mass near to $a$, or that with a high probability it has a mass near to $a$, and both of those things are true. Or it might be that the mass of the object on the scale is within the printed margin of error of the scale. Even if that's false in the case we're imagining, it could be true without $a = v$.) But the optimal measurement would be to have $a = v$, and in this sense the measurement is suboptimal.

The argument then is pretty quick. From the first assumption, we get that the agent is justified in believing that $V \in [a - m, a + m]$. Assume then that the agent forms this justified belief. This belief is incompatible with $V \in [v - m, a - m)$. But if $a$ equalled $v$, then the agent wouldn't be in a position to rule out that $V \in [v - m, a - m)$. So by premise 2 she can't knowledgeably rule it out on the basis of a mismeasurement. So her belief that $V \geq a - m$ cannot be knowledge. So this justified true belief is not knowledge.

If you prefer doing this with numbers, here's the way the example works using the numbers above. The mass of the object is 830. So if the reading was correct, the agent would know just that the mass is between 820 and 840. The reading is 832. So she's justified in believing, and we'll assume she does believe, that the mass is between 822 and 842. That belief is incompatible with the mass being 821. But by premise 2 she can't know the mass is greater than 821. So the belief doesn't amount to knowledge, despite being justified and, crucially, true. After all, 830 is between 822 and 842, so her belief that the mass is in this range is true. So simple reflections on the workings on measuring devices let us generate cases of justified true beliefs that are not knowledge.

I'll end this section with a couple of objections and replies.

%\newcommand{\objrep}[2]{
%\bigskip
%\noindent \textit{Objection}: #1
%
%\medskip
%\noindent \textit{Reply}: #2
%
%}

\objrep{The argument that the agent can't know that $V \in [a - m, a + m]$ is also an argument that the argument can't justifiably believe that $V \in [a - m, a + m]$. After all, why should it be possible to get justification from a suboptimal measurement when it isn't possible to get the same justification from an optimal measurement?}
{It is possible to have justification to believe an outright falsehood. It is widely believed that you can have justification even when none of your evidential sources are even approximately accurate \citep{Cohen1984}. And even most reliabilists will say that you can have false justified beliefs if you use a belief forming method that is normally reliable, but which badly misfires on this occasion. In such cases we clearly get justification to believe something from a mismeasurement that we wouldn't get from a correct measurement. So the objection is based on a mistaken view of justification.}

\objrep{Premise 2 fails in cases using random sampling. Here's an illustration. An experimenter wants to know what percentage of $F$s are $G$. She designs a survey to ask people whether they are $G$. The survey is well designed; everyone gives the correct answer about themselves. And she designs a process for randomly sampling the $F$s to get a good random selection of 500. It's an excellent process; every $F$ had an equal chance of being selected, and the sample fairly represents the different demographically significant subgroups of the $F$s. But by the normal processes of random variation, her group contains slightly more $G$s than the average. In her survey, 28\% of people said (truly!) that they were $G$, while only 26\% of $F$s are $G$s. Assuming a margin of error in such a study of 4\%, it seems plausible to say she knows that between 25 and 32\% of $F$s are $G$s. But that's not something she could have known the survey had come back correctly reporting that 26\% of $F$s are $G$s.}
{I think the core problem with this argument comes in the last sentence. A random survey isn't, in the first instance, a measurement of a population. It's a measurement of those surveyed, from which we draw extrapolations about the population. In that sense, the only \textit{measurement} in the imagined example was as good as it could be; 28\% of surveyed people are in fact $G$. So the survey was correct, and it is fine to conclude that we can in fact know that between 24 and 32 percent of $F$s are $G$s.

There are independent reasons for thinking this is the right way to talk about the case. If a genuine measuring device, like a scale, is off by a small amount, we regard that as a reason for tinkering with the device, and trying to make it more accurate. That's one respect in which the measurement is suboptimal, even if it is correct within the margin of error. This reason to tinker with the scale is a reason that often will be outweighed. Perhaps it is technologically infeasible to make the machine more accurate. More commonly, the only way to guarantee greater accuracy would be more cost and hassle than it is worth. But it remains a reason. The fact that this experiment came out with a deviation between the sample and the population is \textit{not} a reason to think that it could have been run in a better way, or that there is some reason to improve the survey. That's just how random sampling goes. If it were a genuine measurement of the population, the deviation between the `measurement' and what is being measured would be a reason to do things differently. There isn't any such reason, so the sample is not truly a measurement.

So I don't think this objection works, and I think the general principle that you can't get extra knowledge from a suboptimal measurement is right. But note also that we don't need this general principle to suggest that there will be cases of justified true belief without knowledge in the cases of measurement. Consider a special case where $e$ is just less than $m$. For concreteness, say $a = v + 0.95m$, so $e = 0.95m$. Now assume that whatever is justifiedly truly believed in this case is known, so $S$ knows that $V \in [a - m, a + m]$. That is, $S$ knows that $V \in [v - 0.05m, a + m]$.

We don't need any principles about measurement to show this is false; safety considerations will suffice. \cite{Williamson2000-WILKAI} says that a belief that $p$ is safe only if $p$ is true in all nearby worlds. But given how close $v$ is to the edge of the range $[v - 0.05m, a + m]$, the belief that $v$ is in this range clearly isn't safe, so isn't knowledge. Rival conceptions of safety don't help much more than this. The most prominent of these, suggested by \cite{Sainsbury1996}, says that a belief is safe only if the method that produced it doesn't produce a false belief in any nearby world. But if the scale was off by $0.95m$, it could have been off by $1.05m$, so that condition fails too.

I don't want the last two paragraphs to leave too concessive an impression. I think the objection fails because it relies on a misconception of the notion of measurement. But I think that even if the objection works, we can get a safety based argument that some measurement cases will produce justified true beliefs without knowledge. And that will matter for the argument of the next two sections.}

\section{The Class of Gettier Cases is Disjunctive}

There's an unfortunate terminological confusion surrounding gaps between kno\-wledge and justification. Some philosophers use the phrase `Gettier case' to describe any case of a justified true belief that isn't knowledge. Others use it to describe just cases that look like the cases in \cite{Gettier1963}, i.e., cases of true belief derived from justified false belief. I don't particularly have strong views on whether either of these uses is \textit{better}, but I do think it is important to keep them apart.

I'll illustrate the importance of this by discussing a recent argument due to Jeremy Fantl and Matthew McGrath \citep[Ch. 4]{FantlMcGrath2009}. I've previously discussed this argument \citep{Weatherson2011-WEAKBI}, but I don't think I quite got to the heart of why I don't like the kind of reasoning they are using.

The argument concerns an agent, call her $T$, who has the following unfortunate combination of features. She is very confident that $p$. And with good reason; her evidence strongly supports $p$. For normal reasoning, she takes $p$ for granted. That is, she doesn't distinguish between $\varphi$ is best given $p$, and that $\varphi$ is simply best. And that's right too, given the strong evidence that $p$. But she's not crazy. Were she to think that she was facing a bet on extreme odds concerning $p$, she would cease taking $p$ for granted, and revert to trying to maximise expected value given the high probability that $p$. But she doesn't think any such bet is salient, so her disposition to retreat from $p$ to \textit{Probably p} has not been triggered. So far, all is going well. I'm inclined to say that this is enough to say that $T$ justifiedly believes that $p$. She believes that $p$ in virtue of the fact that she takes $p$ for granted in actual reasoning.\footnote{There are some circumlocutions here because I'm being careful to be sensitive to the points raised in \cite{SchroederRoss2012} about the relationship between belief and reasoning. I think there's less distance between the view they put forward and the view I defended in \cite{Weatherson2005-WEACWD} than they do, but this is a subtle matter, and for this paper's purposes I want to go along with Ross and Schroeder's picture of belief.} She's disposed to stop doing so in some circumstances, but until that disposition is triggered, she has the belief. And this is the right way to act given her evidence, so her belief is justified. So far, so good.

Unfortunately, $T$ really does face a bet on long odds about $p$. She knows she has to choose between $\varphi$ and $\psi$. And she knows that $\varphi$ will produce the better outcome iff $p$. But she thinks the amount she'll gain by choosing $\psi$ if $\neg p$ is roughly the same as the amount she'll gain by choosing $\varphi$ if $p$. That's wrong, and her evidence clearly shows it is wrong. If $p$ is false, then $\varphi$ will be \textit{much} worse than $\psi$. In fact, the potential loss here is so great that $\psi$ has the greater expected value given the correct evidential probability of $p$. I think that means she doesn't know that $p$. Someone who knows that $p$ can ignore $\neg p$ possibilities in practical reasoning. And someone who could ignore $\neg p$ possibilities in practical reasoning would choose $\varphi$ over $\psi$, since it is better if $p$. But $T$ isn't in a position to make that choice, so she doesn't know that $p$.

(I've said here that $T$ is wrong about the costs of choosing $\varphi$ if $p$, and her evidence shows she is wrong. In fact I think she doesn't know $p$ if either of those conditions obtain. But here I only want to use the weaker claim that she doesn't know $p$ if both conditions obtain.)

Fantl and McGrath agree about the knowledge claim, but disagree about the justified belief claim. They argue as follows (this is my version of the `Subtraction Argument' from page 97 of their book).

\begin{enumerate*}
\item $T$ is justfied in choosing $\varphi$ iff she knows that $p$.
\item Whether $T$'s belief that $p$ is true is irrelevant to whether she is justified in choosing $\varphi$.
\item Whether $T$'s belief that $p$ is `Gettiered' is irrelevant to whether she is justified in choosing $\varphi$.
\item Knowledge is true, justified, UnGettiered belief.
\item So $T$ is justfied in choosing $\varphi$ iff she is justified in believing that $p$.
\item $T$ is not justified in choosing $\varphi$.
\item So $T$ is not justified in believing that $p$.
\end{enumerate*}

\noindent I think this argument is only plausible if we equivocate on what it is for a belief to be `Gettiered'.

Assume first that `Gettiered' means `derived from a false intermediate step'. Then premise 4 is false, as Williamson's example shows. $S$ has a justified true belief that is neither knowledge nor derived from a false premise.

Assume then that `Gettiered' simply means that the true belief is justified without being known. In that case we have no reason to accept premise 3. After all, the class of true justified beliefs that are not knowledge is pretty open ended. Before reading Williamson, we may not have thought that this class included the beliefs of agents using measuring devices that were functioning properly but imperfectly. But it does. Prior to the end of epistemology, we simply don't know what other kind of beliefs might be in this class. There's no way to survey all the ways for justification to be insufficient for knowledge, and see if all of them are irrelevant to the justification for action. I think one way a justified belief can fall short of knowledge is if it is tied up with false beliefs about the stakes of bets. It's hard to say that that is irrelevant to the justification of action.

It is by now reasonably well known that logical subtraction is a very messy and complicated business. See, for instance, \cite{Humberstone2000} for a clear discussion of the complications. In general, unless it is analytic that $F$s are $G$s and $H$s, for some antecedently understood $G$ and $H$, there's nothing interesting to say about the class of things that are $G$ but not $F$. It will just be a disjunctive shambles. The same is true for knowledge and justification. The class of true beliefs that are justified but not known is messy and disjunctive. We shouldn't expect to have any neat way of overviewing it. That in part means we can't say much interesting about it as a class, contra premise 3 in the above argument. It also means the prospects for `solving the Gettier problem' are weak. We'll turn to that issue next.

\section{There is No Solution to the Gettier Problem}

The kind of example that Edmund \cite{Gettier1963} gives to refute the justified true belief theory of knowledge has what Linda  Zagzebski \citeyearpar[117]{Zagzebski2009} aptly calls a ``double luck'' structure. In Gettier's original cases, there's some bad luck that leads to a justified belief being false. But then there's some good luck that leads to an inference from that being true. As was quickly realised in the literature, the good and bad luck doesn't need to apply to separate inferential steps. It might be that the one belief that would have been false due to bad luck also ends up being true due to good luck.

This has led to a little industry, especially in the virtue epistemology section of the market, of attempts to ``solve the Gettier problem'' by adding an anti-luck condition to justification, truth and belief and hoping that the result is something like an analysis of knowledge. As  \cite{Zagzebski1994} showed, this can't be an \textit{independent} condition on knowledge. If it doesn't entail truth, then we will be able to recreate the Gettier cases. But maybe a `fourth' condition that entails truth (and perhaps belief) will suffice. Let's quickly review some of these proposals. 

So \cite{Zagzebski1996} suggested that the condition is that the belief be true \textit{because} justified. John \cite{Greco2010} says that the extra condition is that the beliefs be ``intellectually creditable''. That is, the primary that the subject ended up with a true belief is that it was the result of her reliable cognitive faculties. Ernest \cite{Sosa2007} said that knowledge is belief that is true because it manifests intellectual competence. John \cite{Turri2011} says that knowledge is belief the truth of which is a manifestation of the agent's intellectual competence.

It should be pretty clear that no such proposal can work if what I've said in earlier sections is remotely right. Assume again that $v = 830, a = 832$ and $m = 10$. The agent believes that $V \in [822, 842]$. This belief is, we've said, justified and true. Does it satisfy these extra conditions?

My short answer is that it does. My longer answer is that it does if any belief derived from the use of a measuring device does, and since some beliefs derived from the use of measuring devices amount to knowledge, the epistemologists are committed to the belief satisfying the extra condition. Let's go through those arguments in turn.

In our story, $S$ demonstrates a range of intellectual competencies. She uses a well-functioning measuring device. It is the right kind of device for the purpose she is using. By hypothesis, she has had the machine carefully checked, and knows exactly the accuracy of the machine. She doesn't form any belief that is too precise to be justified by the machine. And she ends up with a true belief precisely because she has so many competencies.

Note that if we change the story so $a$ is closer to $v + m$, the case that the belief is true in virtue of $S$ being so competent becomes even stronger. Change the case so that $a = 839$, and she forms the true belief that $V \in [829, 849]$. Now if $S$ had not been so competent, she may have formed a belief with a tighter range, since she could easily have guessed that the margin of error of the machine is smaller. So in this case the truth of the belief is very clearly due to her competence. But as we noted at the end of section 1, in the cases where $a$ is near $v + m$, the argument that we have justified true belief without knowledge is particularly strong. Just when the gap between justification and knowledge gets most pronounced, the competence based approach to knowledge starts to issue the strongest verdicts \textit{in favour} of knowledge.

But maybe this is all a mistake. After all, the object doesn't have the mass it has because of $S$'s intellectual competence. The truth of any claim about its mass is not because of $S$'s competence, or a manifestation of that competence. So maybe these epistemologists get the correct verdict that $S$ does not know that $V \in [a - m, a + m]$?

Not so quick. Even had $a$ equalled $v$, all these claims would have been true. And in that case, $S$ would have known that $V$ was within $m$ of the measurement. What is needed for these epistemological theories to be right is that there can be a sense that a belief that $p$ can be true in virtue of some cause $C$ without $C$ being a cause of $p$. I'm inclined to agree with the virtue epistemologists that such a sense can be given. (I think it helps to give up on content essentialism for this project, as suggested by \cite{David2002} and endorsed in\cite{Weatherson2004-WEALMT}.) But I don't think it will help. There's no real way in which a belief is true because of competencies, or in which the truth of a belief manifests competence, in the good case where $a = v$, but not in the bad cases, where $a$ is in $(0, m)$. These proposals from Zagzebski and others might help with explaining why a gap opens between knowledge and justification in `double luck' cases. But that gap can appear in cases that don't have a `double luck' structure. As noted in the previous section, I think the gap in question includes some cases involving false beliefs about the practical significance of $p$, but I don't expect everyone to agree with that. Happily, the Williamsonian cases should be less controversial.



