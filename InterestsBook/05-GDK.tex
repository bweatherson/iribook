\chapter{Games, Decisions and Knowledge}
\markright{Games, Decisions and Knowledge}

\section{The Interest-Relativity of Knowledge}

\subsection{The Struction of Decision Problems}

Professor Dec is teaching introductory decision theory to her undergraduate class. She is trying to introduce the notion of a dominant choice. So she introduces the following problem, with two states, $S_1$ and $S_2$, and two choices, $C_1$ and $C_2$, as is normal for introductory problems.

\begin{center}
\begin{tabular}{r c c}
 & $S_1$ & $S_2$ \\
$C_1$ & -\$200 & \$1000 \\
$C_2$ & -\$100 & \$1500 
\end{tabular}
\end{center}

\noindent She's hoping that the students will see that $C_1$ and $C_2$ are bets, but $C_2$ is clearly the better bet. If $S_1$ is actual, then both bets lose, but $C_2$ loses less money. If $S_2$ is actual, then both bets win, but $C_2$ wins more. So $C_2$ is better. That analysis is clearly wrong if the state is causally dependent on the choice, and controversial if the states are evidentially dependent on the choices. But Professor Dec has not given any reason for the students to think that the states are dependent on the choices in either way, and in fact the students don't worry about that kind of dependence.

That doesn't mean, however, that the students all adopt the analysis that Professor Dec wants them to. One student, Stu, is particularly unwilling to accept that $C_2$ is better than $C_1$. He thinks, on the basis of his experience, that when more than \$1000 is on the line, people aren't as reliable about paying out on bets. So while $C_1$ is guaranteed to deliver \$1000 if $S_2$, if the agent bets on $C_2$, she might face some difficulty in collecting on her money.

Given the context, i.e., that they are in an undergraduate decision theory class, it seems that Stu has misunderstood the question that Professor Dec intended to ask. But it is a little harder than it first seems to specify just exactly what Stu's mistake is. It isn't that he thinks Professor Dec has \textit{misdescribed} the situation. It isn't that he thinks the agent won't collect \$1500 if she chooses $C_2$ and is in $S_2$. He just thinks that she \textit{might} not be able to collect it, so the expected payout might really be a little less than \$1500.

But Stu is not the only problem that Professor Dec has. She also has trouble convincing Dom of the argument. He thinks there should be a third state added, $S_3$. In $S_3$, there is a vengeful God who is about to end the world, and take everyone who chose $C_1$ to heaven, while sending everyone who chose $C_2$ to hell. Since heaven is better than hell, $C_2$ does not dominate $C_1$; it is worse in $S_3$. If decision theory is to be useful, we must say something about why we can leave states like $S_3$ off the decision table.

So in order to teach decision theory, Professor Dec has to answer two questions.\footnote{If we are convinced that the right decision is the one that maximises expected utility, there is a sense in which these questions collapse. For the expected utility theorist, we can solve Dom's question by making sure the states are logically exhaustive, and making the `payouts' in each state be expected payouts. But the theory that the correct decision is the one that maximises expected utility, while plausibly true, is controversial. It shouldn't be assumed when we are investigating the semantics of decision tables.}

\begin{enumerate*}
\item What makes it legitimate to write something on the decision table, such as the `\$1500' we write in the bottom right cell of Dec's table?
\item What makes it legitimate to leave something off a decision table, such as leaving Dom's state $S_3$ off the table?
\end{enumerate*}

\noindent Let's start with a simpler problem that helps with both questions. Alice is out of town on a holiday, and she faces the following decision choice concerning what to do with a token in her hand.

\begin{center}
\begin{tabular}{r c}
\textbf{Choice} & \textbf{Outcome} \\
Put token on table & Win \$1000 \\
Put token in pocket & Win nothing
\end{tabular}
\end{center}

\noindent This looks easy, especially if we've taken Professor Dec's class. Putting the token on the table dominates putting the token in her pocket. It returns \$1000, versus no gain. So she should put the token on the table.

I've left Alice's story fairly schematic; let's fill in some of the details. Alice is on holiday at a casino. It's a fair casino; the probabilities of the outcomes of each of the games is just what you'd expect. And Alice knows this. The table she's standing at is a roulette table. The token is a chip from the casino worth \$1000. Putting the token on the table means placing a bet. As it turns out, it means placing a bet on the roulette wheel landing on 28. If that bet wins she gets her token back and another token of the same value. There are many other bets she could make, but Alice has decided not to make all but one of them. Since her birthday is the 28$^{\text{th}}$, she is tempted to put a bet on 28; that's the only bet she is considering. If she makes this bet, the objective chance of her winning is $\nicefrac{1}{38}$, and she knows this. As a matter of fact she will win, but she doesn't know this. (This is why the description in the table I presented above is truthful, though frightfully misleading.) As you can see, the odds on this bet are terrible. She should have a chance of winning around $\nicefrac{1}{2}$ to justify placing this bet.\footnote{Assuming Alice's utility curve for money curves downwards, she should be looking for a slightly higher chance of winning than $\nicefrac{1}{2}$ to place the bet, but that level of detail isn't relevant to the story we're telling here.} So the above table, which makes it look like placing the bet is the dominant, and hence rational, option, is misleading.

Just how is the table misleading though? It isn't because what is says is false. If Alice puts the token on the table she wins \$1000; and if she doesn't, she stays where she is. It isn't, or isn't just, that Alice doesn't believe the table reflects what will happen if she places the bet. As it turns out, Alice is smart, so she doesn't form beliefs about chance events like roulette wheels. But even if she did, that wouldn't change how misleading the table is. The table suggests that it is rational for Alice to put the token on the table. In fact, that is irrational. And it would still be irrational if Alice believes, \textit{irrationally}, that the wheel will land on 28.

A better suggestion is that the table is misleading because Alice doesn't \textit{know} that it accurately depicts the choice she faced. If she did know that these were the outcomes to putting the token on the table versus in her pocket, it seems it would be rational for her to put it on the table. If we take it as tacit in a presentation of a decision problem that the agent knows that the table accurately depicts the outcomes of various choices in different states, then we can tell a plausible story about what the miscommunication between Professor Dec and her students was. Stu was assuming that if the agent wins \$1500, she might not be able to easily collect. That is, he was assuming that the agent does not know that she'll get \$1500 if she chooses $C_2$ and is in state $S_2$. Professor Dec, if she's anything like other decision theory professors, will have assumed that the agent did know exactly that. And the miscommunication between Professor Dec and Dom also concerns knowledge. When Dec wrote that table up, she was saying that the agent knew that $S_1$ or $S_2$ obtained. And when she says it is best to take dominating options, she means that it is best to take options that one knows to have better outcomes. So here are the answers to Stu and Dom's challenges.

\begin{enumerate*}
\item It is legitimate to write something on the decision table, such as the `\$1500' we write in the bottom right cell of Dec's table, iff the decision maker knows it to be true.
\item It is legitimate to leave something off a decision table, such as leaving Dom's state $S_3$ off the table, iff the decision maker knows it not to obtain.
\end{enumerate*}

\noindent Perhaps those answers are not correct, but what we can clearly see by reflecting on these cases is that the standard presentation of a decision problem presupposes not just that the table states what will happen, but the agent stands in some special doxastic relationship to the information explicitly on the table (such as that Alice will get \$1500 if $C_2$ and $S_2$) and implied by where the table ends (such as that $S_3$ will not happen). Could that relationship be weaker than knowledge? It's true that it is hard to come up with clear counterexamples to the suggestion that the relationship is merely justified true belief. But I think it is somewhat implausible to hold that the standard presentation of an example merely presupposes that the agent has a justified true belief that the table is correct, and does not in addition know that the table is correct.

My reasons for thinking this are similar to one of the reasons Timothy Williamson \citep[Ch. 9]{Williamson2000-WILKAI} gives for doubting that one's evidence is all that one justifiably truly believes. To put the point in Lewisian terms, it seems that knowledge is a much more \textit{natural} relation than justified true belief. And when ascribing contents, especially contents of tacitly held beliefs, we should strongly prefer to ascribe more rather than less natural contents.\footnote{I'm here retracting some things I said a few years ago in a paper on philosophical methodology \citep{Weatherson2003-WEAWGA}. There I argued that identifying knowledge with justified true belief would give us a theory on which knowledge was more natural than a theory on which we didn't identify knowledge with any other epistemic property. I now think that is wrong for a couple of reasons. First, although it's true (as I say in the earlier paper) that knowledge can't be primitive or perfectly natural, this doesn't make it less natural than justification, which is also far from a fundamental feature of reality. Indeed, given how usual it is for languages to have a simple representation of knowledge, we have some evidence that it is very natural for a term from a special science. Second, I think in the earlier paper I didn't fully appreciate the point (there attributed to Peter Klein) that the Gettier cases show that the property of being a justified true belief is not particularly natural. In general, when $F$ and $G$ are somewhat natural properties, then so is the property of being $F \wedge G$. But there are exceptions, especially in cases where these are properties that a whole can have in virtue of a part having the property. In those cases, a whole that has an $F$ part and a $G$ part will be $F \wedge G$, but this won't reflect any distinctive property of the whole. And one of the things the Gettier cases show is that the properties of \textit{being justified} and \textit{being true}, as applied to belief, fit this pattern. 

Note that even if you think that philosophers are generally too quick to move from instinctive reactions to the Gettier case to abandoning the justified true belief theory of knowledge, this point holds up. What is important here is that on sufficient reflection, the Gettier cases show that some justified true beliefs are not knowledge, and that the cases in question also show that being a justified true belief is not a particularly natural or unified property. So the point I've been making in the last this footnote is independent of the point I wanted to stress in ``What Good are Counterexamples?'', namely, that philosophers in some areas (especially epistemology) are insufficiently reformist in their attitude towards our intuitive reactions to cases.}

So the `special doxastic relationship' is not weaker than knowledge. Could it be stronger? Could it be, for example, that the relationship is certainty, or some kind of iterated knowledge? Plausibly in some game-theoretic settings it is stronger -- it involves not just knowing that the table is accurate, but knowing that the other player knows the table is accurate. In some cases, the standard treatment of games will require positing even more iterations of knowledge. For convenience, it is sometimes explicitly stated that iterations continue indefinitely, so each party knows the table is correct, and knows each party knows this, and knows each party knows that, and knows each party knows \textit{that}, and so on. An early example of this in philosophy is in the work by David \cite{Lewis1969a} on convention. But it is usually acknowledged (again in a tradition extending back at least to Lewis) that only the first few iterations are actually needed in any problem, and it seems a mistake to attribute more iterations than are actually used in deriving solutions to any particular game. 

The reason that would be a mistake is that we want game theory, and decision theory, to be applicable to real-life situations. There is very little that we know, and know that we know, and know we know we know, and so on indefinitely \citep[Ch. 4]{Williamson2000-WILKAI}. There is, perhaps, even less that we are certain of. If we only could say that a person is making a particular decision when they stand in these very strong relationships to the parameters of the decision table, then people will almost never be making the kinds of decision we study in decision theory. Since decision theory and game theory are not meant to be that impractical, I conclude that the `special doxastic relationship' cannot be that strong. It could be that in some games, the special relationship will involve a few iterations of knowledge, but in decision problems, where the epistemic states of others are irrelevant, even that is unnecessary, and simple knowledge seems sufficient.

It might be argued here that we shouldn't expect to apply decision theory directly to real-life problems, but only to idealised versions of them, so it would be acceptable to, for instance, require that the things we put in the table are, say, things that have probability exactly 1. In real life, virtually nothing has probability 1. In an idealisation, many things do. But to argue this way seems to involve using `idealisation' in an unnatural sense. There is a sense in which, whenever we treat something with non-maximal probability as simply given in a decision problem that we're ignoring, or abstracting away from, some complication. But we aren't \textit{idealising}. On the contrary, we're modelling the agent as if they were irrationally certain in some things which are merely very very probable. 

So it's better to say that any application of decision theory to a real-life problem will involve ignoring certain (counterfactual) logical or metaphysical possibilities in which the decision table is not actually true. But not any old abstraction will do. We can't ignore just anything, at least not if we want a good model. Which abstractions are acceptable? The response I've offered to Dom's challenge suggests an answer to this: we can abstract away from any possibility in which something the agent actually knows is false. I don't have a knock-down argument that this is the best of all possible abstractions, but nor do I know of any alternative answer to the question of which abstractions are acceptable which is nearly as plausible.

We might be tempted to say that we can abstract away from anything such that the difference between its probability and 1 doesn't make a difference to the ultimate answer to the decision problem. More carefully, the idea would be that we can have the decision table represent that $p$ iff $p$ is true and it wouldn't change what the agent should do if $\Pr(p)$ were raised to 1. I think this is the most plausible story one could tell about decision tables if one didn't like the knowledge first story that I tell. But I also don't think it works, because of cases like the following.

Luc is lucky; he's in a casino where they are offering better than fair odds on roulette. Although the chance of winning any bet is \nicefrac{1}{38}, if Luc bets \$10, and his bet wins, he will win \$400. (That's the only bet on offer.) Luc, like Alice, is considering betting on 28. As it turns out, 28 won't come up, although since this is a fair roulette wheel, Luc doesn't know this. Luc, like most agents, has a declining marginal utility for money. He currently has \$1,000, and for any amount of money $x$, Luc gets utility $u(x) = x^{\nicefrac{1}{2}}$ out of having $x$. So Luc's current utility (from money) is, roughly, 31.622. If he bets and loses, his utility will be, roughly, 31.464. And if he bets and wins, his utility will be, roughly, 37.417. So he stands to gain about 5.794, and to lose about 0.159. So he stands to gain about 36.5 as much as he stands to lose. Since the odds of winning are less than $\nicefrac{1}{36.5}$, his expected utility goes down if he takes the bet, so he shouldn't take it. Of course, if the probability of losing was 1, and not merely $\nicefrac{37}{38}$, he shouldn't take the bet too. Does that mean it is acceptable, in presenting Luc's decision problem, to leave off the table any possibility of him winning, since he won't win, and setting the probability of losing to 1 rather than $\nicefrac{37}{38}$ doesn't change the decision he should make? Of course not; that would horribly misstate the situation Luc finds himself in. It would misrepresent  how sensitive Luc's choice is to his utility function, and to the size of the stakes. If Luc's utility function was $u(x) = x^{\nicefrac{3}{4}}$, then he should take the bet. If his utility function is unchanged, but the bet was \$1 against \$40, rather than \$10 against \$400, he should take the bet. Leaving off the possibility of winning hides these facts, and badly misrepresents Luc's situation.

I've argued that the states we can `leave off' a decision table are the states that the agent knows not to obtain. The argument is largely by elimination. If we can only leave off things that have probability 1, then decision theory would be useless; but it isn't. If we say we can leave off things if setting their probability at 1 is an accepable idealisation, we need a theory of acceptable idealisations. If this is to be a rival to my theory, the idealisation had better not be it's acceptable to treat anything known as having probability 1. But the most natural alternative idealisation badly misrepresents Luc's case. If we say that what can be left off is not what's known not to obtain, but what is, say, justifiably truly believed not to obtain, we need an argument for why people would naturally use such an unnatural standard. This doesn't even purport to be a conclusive argument, but these considerations point me towards thinking that knowledge determines what we can leave off.

I also cheated a little in making this argument. When I described Alice in the casino, I made a few explicit comments about her information states. And every time, I said that she \textit{knew} various propositions. It seemed plausible at the time that this is enough to think those propositions should be incorporated into the table we use to represent her decision. That's some evidence against the idea that more than knowledge, perhaps iterated knowledge or certainty, is needed before we add propositions to the decision table.

\subsection{From Decision Theory to Interest-Relativity}

This way of thinking about decision problems offers a new perspective on the issue of whether we should always be prepared to bet on what we know.\footnote{This issue is of course central to the plotline in \cite{Hawthorne2004}.} To focus intuitions, let's take a concrete case. Barry is sitting in his apartment one evening when he hears a musician performing in the park outside. The musician, call her Beth, is one of Barry's favourite musicians, so the music is familiar to Barry. Barry is excited that Beth is performing in his neighbourhood, and he decides to hurry out to see the show. As he prepares to leave, a genie appears an offers him a bet.\footnote{Assume, perhaps implausibly, that the sudden appearance of the genie is evidentially irrelevant to the proposition that the musician is Beth. The reasons this may be implausible are related to the arguments in \cite[14-15]{RunyonGuysDolls}. Thanks here to Jeremy Fantl.} If he takes the bet, and the musician is Beth, then the genie will give Barry ten dollars. On the other hand, if the musician is not Beth, he will be tortured in the fires of hell for a millenium. Let's put Barry's options in table form.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{Musician is Beth} & \textbf{Musician is not Beth} \\
\textbf{Take Bet} & Win \$10 & 1000 years of torture \\
\textbf{Decline Bet} & Status quo & Status quo \\
\end{tabular}
\end{center}

\noindent Intuitively, it is extremely irrational for Barry to take the bet. People do make mistakes about identifying musicians, even very familiar musicians, by the strains of music that drift up from a park. It's not worth risking a millenium of torture for \$10.

But it also seems that we've misstated the table. Before the genie showed up, it seemed clear that Barry knew that the musician was Beth. That was why he went out to see her perform. (If you don't think this is true, make the sounds from the park clearer, or make it that Barry had some prior evidence that Beth was performing which the sounds from the park remind him of. It shouldn't be too hard to come up with an evidential base such that (a) in normal circumstances we'd say Barry knew who was performing, but (b) he shouldn't take this genie's bet.) Now our decision tables should reflect the knowledge of the agent making the decision. If Barry knows that the musician is Beth, then the second column is one he knows will not obtain. So let's write the table in the standard form.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{Musician is Beth} &  \\
\textbf{Take Bet} & Win \$10  \\
\textbf{Decline Bet} & Status quo &  \\
\end{tabular}
\end{center}

\noindent And it is clear what Barry's decision should be in this situation. Taking the bet dominates declining it, and Barry should take dominating options.

What has happened? It is incredibly clear that Barry should decline the bet, yet here we have an argument that he should take the bet. If you accept that the bet should be declined, then it seems to me that there are three options available.

\begin{enumerate*}
\item Barry never knew that the musician was Beth.
\item Barry did know that the musician was Beth, but this knowledge was destroyed by the genie's offer of the bet.
\item States of the world that are known not to obtain should still be represented in decision problems, so taking the bet is not a dominating option.
\end{enumerate*}

\noindent The first option is basically a form of scepticism. If the take-away message from the above discussion is that Barry doesn't know the musician is Beth, we can mount a similar argument to show that he knows next to nothing.\footnote{The idea that interest-relativity is a way of fending off scepticism is a very prominent theme in \cite{FantlMcGrath2009}.} And the third option would send us back into the problems about interpreting and applying decision theory that we spent the first few pages trying to get out of.

So it seems that the best solution here, or perhaps the least bad solution, is to accept that knowledge is interest-relative. Barry did know that the musician was Beth, but the genie's offer destroyed that knowledge. When Barry was unconcerned with bets at extremely long odds on whether the musician is Beth, he knows Beth is the musician. Now that he is interested in those bets, he doesn't know that.\footnote{On the version of IRI I'm defending, Barry is free to be interested in whatever he likes. If he started wondering about whether it would be rational to take such a bet, he loses the knowledge that Beth is the musician, even if there is no genie and the bet isn't offered. The existence of the genie's offer makes the bet a practical interest; merely wondering about the genie's offer makes the bet a cognitive interest. But both kinds of interests are relevant to knowledge.}

The argument here bears more than a passing resemblance to the arguments in favour of interest-relativity that are made by Hawthorne, Stanley, and Fantl and McGrath. But I think the focus on decision theory shows how we can get to interest-relativity with very weak premises.\footnote{As they make clear in their \citeyearpar{Hawthorne2008-HAWKAA}, Hawthorne and Stanley are interested in defending relatively strong premises linking knowledge and action independently of the argument for the interest-relativity of knowledge. What I'm doing here is showing how that conclusion does not rest on anything nearly as strong as the principles they believe, and so there is plenty of space to disagree with their general principles, but accept interest-relativity. The strategy here isn't a million miles from the point noted in Fantl and McGrath \citeyearpar[72n14]{FantlMcGrath2009} when they note that much weaker premises than the ones they endorse imply a failure of `purism'.} In particular, the only premises I've used to derive an interest-relative conclusion are:

\begin{enumerate*}
\item Before the genie showed up, Barry knew the musician was Beth.
\item It's rationally permissible, \textit{in cases like Barry's}, to take dominating options.
\item It's always right to model decision problems by including what the agent knows in the `framework'. That is, our decision tables should include what the agent knows about the payoffs in different states, and leave off any state the agent knows not to obtain.
\item It is rationally impermissible for Barry to take the genie's offered bet.
\end{enumerate*}

\noindent The second premise there is \textit{much} weaker than the principles linking knowledge and action defended in previous arguments for interest-relativity. It isn't the claim that one can always act on what one knows, or that one can only act on what one knows, or that knowledge always (or only) provides reason to act. It's just the claim that in one very specific type of situation, in particular when one has to make a relatively simple bet, which affects nobody but the person making the bet, it's rationally permissible to take a dominating option. In conjunction with the third premise, it entails that \textit{in those kind of cases}, the fact that one knows taking the bet will lead to a better outcome suffices for making acceptance of the bet rationally permissible. It doesn't say anything about what else might or might not make acceptance rationally permissible. It doesn't say anything about what suffices for rationally permissibility in other kinds of cases, such as cases where someone else's interests are at stake, or where taking the bet might violate a deontological constraint, or any other way in which real-life choices differ from the simplest decision problems.\footnote{I have more to say about those cases in section 2.2.} It doesn't say anything about any other kind of permissibility, e.g., moral permissibility. But it doesn't need to, because we're only in the business of proving that there is \textit{some} interest-relativity to knowledge, and an assumption about practical rationality in some range of cases suffices to prove that.\footnote{Also note that I'm not taking as a premise any claim about what Barry knows after the bet is offered. A lot of work on interest-relativity has used such premises, or premises about related intuitions. This seems like a misuse of the method of cases to me. That's not because we should never use intuitions about cases, just that these cases are too hard to think that snap judgments about them are particularly reliable. In general, we can know a lot about cases by quickly reflecting on them. Similarly, we know a lot about which shelves are level and which are uneven by visual inspection, i.e., `eyeballing'. But when different eyeballs disagree, it's time to bring in other tools. That's the approach of this paper. I don't have a story about why the various eyeballs disagree about cases like Barry's; that seems like a task best undertaken by a psychologist not a philosopher \citep{Ichikawa2009}.}

The case of Barry and Beth also bears some relationship to one of the kinds of case that have motivated contextualism about knowledge. Indeed, it has been widely noted in the literature on interest-relativity that interest-relativity can explain away many of the puzzles that motivate contextualism. And there are difficulties that face any contextualist theory \citep{Weatherson2006-WEAQC}. So I prefer an \textit{invariantist} form of interest-relativity about knowledge. That is, my view is a form of interest-relative-invariantism, or IRI.\footnote{This is obviously not a full argument against contextualism; that would require a much longer paper than this.}

Now everything I've said here leaves it open whether the interest-relativity of knowledge is a natural and intuitive theory, or whether it is a somewhat unhappy concession to difficulties that the case of Barry and Beth raise. I think the former is correct, and interest-relativity is fairly plausible on its own merits, but it would be consistent with my broader conclusions to say that in fact the interest-relative theory of knowledge is very implausible and counterintuitive. If we said that, we could still justify the interest-relative theory by noting that we have on our hands here a paradoxical situation, and any option will be somewhat implausible. This consideration has a bearing on how we should think about the role of intuitions about cases, or principles, in arguments that knowledge is interest-relative. Several critics of the view have argued that the view is counter-intuitive, or that it doesn't accord with the reactions of non-expert judges.\footnote{See, for instance, \cite{MBT2009}, or \cite{FeltzZarpentine2010}.} In a companion paper, ``Defending Interest-Relative Invariantism'', I note that those arguments usually misconstrue what the consequences of interest-relative theories of knowledge are. But even if they don't, I don't think there's any quick argument that if interest-relativity is counter-intuitive, it is false. After all, the only alternatives that seem to be open here are very counter-intuitive.

Finally, it's worth noting that if Barry is rational, he'll stop (fully) believing that the musician is Beth once the genie makes the offer. Assuming the genie allows this, it would be very natural for Barry to try to acquire more information about the singer. He might walk over to the window to see if he can see who is performing in the park. So this case leaves it open whether the interest-relativity of knowledge can be explained fully by the interest-relativity of belief. I used to think it could be; I no longer think that. To see why this is so, it's worth rehearsing how the interest-relative theory of belief runs.

\section{Playing Games with a Lockean}

I'm going to raise problems for Lockeans, and for defenders of regularity in general, by discussing a simple game. The game itself is a nice illustration of how a number of distinct solution concepts in game theory come apart. (Indeed, the use I'll make of it isn't a million miles from the use that  \cite{KohlbergMertens1986} make of it.) To set the problem up, I need to say a few words about how I think of game theory. This won't be at all original - most of what I say is taken from important works by Robert \cite{Stalnaker1994, Stalnaker1996, Stalnaker1998, Stalnaker1999}. But it is different to what I used to think, and perhaps to what some other people think too, so I'll set it out slowly.\footnote{I'm grateful to the participants in a game theory seminar at Arch\'e in 2011, especially Josh Dever and Levi Spectre, for very helpful discussions that helped me see through my previous confusions.}

Start with a simple decision problem, where the agent has a choice between two acts $A_1$ and $A_2$, and there are two possible states of the world, $S_1$ and $S_2$, and the agent knows the payouts for each act-state pair are given by the following able.

\begin{center}
\begin{tabular}{r c c}
 & $S_1$ & $S_2$ \\
$A_1$ & 4 & 0 \\
$A_2$ & 1 & 1 
\end{tabular}
\end{center}

\noindent What to do? I hope you share the intuition that it is radically underdetermined by the information I've given you so far. If $S_2$ is much more probable than $S_1$, then $A_2$ should be chosen; otherwise $A_1$ should be chosen. But I haven't said anything about the relative probability of those two states. Now compare that to a simple game. Row has two choices, which I'll call $A_1$ and $A_2$. Column also has two choices, which I'll call $S_1$ and $S_2$. It is common knowledge that each player is rational, and that the payouts for the pairs of choices are given in the following table. (As always, Row's payouts are given first.)

\begin{center}
\begin{tabular}{r c c}
 & $S_1$ & $S_2$ \\
$A_1$ & 4, 0 & 0, 1 \\
$A_2$ & 1, 0 & 1, 1 
\end{tabular}
\end{center}

\noindent What should Row do? This one is easy. Column gets 1 for sure if she plays $S_2$, and 0 for sure if she plays $S_1$. So she'll play $S_2$. And given that she's playing $S_2$, it is best for Row to play $A_2$.

You probably noticed that the game is just a version of the decision problem that we discussed a couple of paragraphs ago. The relevant states of the world are choices of Column. But that's fine; we didn't say in setting out the decision problem what constituted the states $S_1$ and $S_2$. And note that we solved the problem without explicitly saying anything about probabilities. What we added was some information about Column's payouts, and the fact that Column is rational. From there we deduced something about Column's play, namely that she would play $S_2$. And from that we concluded what Row should do.

There's something quite general about this example. What's distinctive about game theory isn't that it involves any special kinds of decision making. Once we get the probabilities of each move by the other player, what's left is (mostly) expected utility maximisation. (We'll come back to whether the `mostly' qualification is needed below.) The distinctive thing about game theory is that the probabilities aren't specified in the setup of the game; rather, they are solved for. Apart from special cases, such as where one option strictly dominates another, we can't say much about a decision problem with unspecified probabilities. But we can and do say a lot about games where the setup of the game doesn't specify the probabilities, because we can solve for them given the other information we have.

This way of thinking about games makes the description of game theory as `interactive epistemology' \citep{Aumann1999} rather apt. The theorist's work is to solve for what a rational agent should think other rational agents in the game should do. From this perspective, it isn't surprising that game theory will make heavy use of equilibrium concepts. In solving a game, we must deploy a theory of rationality, and attribute that theory to rational actors in the game itself. In effect, we are treating rationality as something of an unknown, but one that occurs in every equation we have to work with. Not surprisingly, there are going to be multiple solutions to the puzzles we face.

This way of thinking lends itself to an epistemological interpretation of one of the most puzzling concepts in game theory, the mixed strategy. Arguably the core solution concept in game theory is the Nash equilibrium. As you probably know, a set of moves is a Nash equilibrium if no player can improve their outcome by deviating from the equilibrium, conditional on no other player deviating. In many simple games, the only Nash equilibria involve mixed strategies. Here's one simple example.

\begin{center}
\begin{tabular}{r c c}
 & $S_1$ & $S_2$ \\
$A_1$ & 0, 1 & 10, 0 \\
$A_2$ & 9, 0 & -1, 1 
\end{tabular}
\end{center}

\noindent This game is reminiscent of some puzzles that have been much discussed in the decision theory literature, namely asymmetric Death in Damascus puzzles. Here Column wants herself and Row to make the `same' choice, i.e., $A_1$ and $S_1$ or $A_2$ and $S_2$. She gets 1 if they do, 0 otherwise. And Row wants them to make different choices, and gets 10 if they do. Row also dislikes playing $A_2$, and this costs her 1 whatever else happens. It isn't too hard to prove that the only Nash equilibrium for this game is that Row plays a mixed strategy playing both $A_1$ and $A_2$ with probability \nicefrac{1}{2}, while Column plays the mixed strategy that gives $S_1$ probability \nicefrac{11}{20}, and $S_2$ with probability \nicefrac{9}{20}.

Now what is a mixed strategy? It is easy enough to take away form the standard game theory textbooks a \textbf{metaphysical} interpretation of what a mixed strategy is. Here, for instance, is the paragraph introducing mixed strategies in Dixit and Skeath's \textit{Games of Strategy}.

\begin{quote}
When players choose to act unsystematically, they pick from among their pure strategies in some random way \dots We call a random mixture between these two pure strategies a mixed strategy. \citep[186]{DixitSkeath2004}
\end{quote}
Dixit and Skeath are saying that it is definitive of a mixed strategy that players use some kind of randomisation device to pick their plays on any particular run of a game. That is, the probabilities in a mixed strategy must be in the world; they must go into the players' choice of play. That's one way, the paradigm way really, that we can think of mixed strategies metaphysically.

But the understanding of game theory as interactive epistemology naturally suggests an \textbf{epistemological} interpretation of mixed strategies. %Insert quotes from Stalnaker1994

\begin{quote}One could easily \dots [model players] \dots turning the choice over to a randomizing device, but while it might be harmless to permit this, players satisfying the cognitive idealizations that game theory and decision theory make could have no motive for playing a mixed strategy. So how are we to understand Nash equilibrium in model theoretic terms as a solution concept? We should follow the suggestion of Bayesian game theorists, interpreting mixed strategy profiles as representations, not of players' choices, but of their beliefs. \citep[57-8]{Stalnaker1994}
\end{quote}
One nice advantage of the epistemological interpretation, as noted by Binmore \citeyearpar[185]{Binmore2007} %Include citation
is that we don't require players to have $n$-sided dice in their satchels, for every $n$, every time they play a game.\footnote{Actually, I guess it is worse than if some games have the only equilibria involving mixed strategies with irrational probabilities. And it might be noted that Binmore's introduction of mixed strategies, on page 44 of his \citeyearpar{Binmore2007}, sounds much more like the metaphysical interpretation. But I think the later discussion is meant to indicate that this is just a heuristic introduction; the epistemological interpretation is the correct one.} But another advantage is that it lets us make sense of the difference between playing a pure strategy and playing a mixed strategy where one of the `parts' of the mixture is played with probability one. 

With that in mind, consider the below game, which I'll call \RG. I've said something different about this game in earlier work \citep{Weatherson2012-WEAGAT}. But I now think that to understand what's going on, we need to think about mixed strategies where one element of the mixture has probability one.

Informally, in this game $A$ and $B$ must each play either a green or red card. I will capitalise $A$'s moves, i.e., $A$ can play GREEN or RED, and italicise $B$'s moves, i.e., $B$ can play \textit{green} or \textit{red}. If two green cards, or one green card and one red card are played, each player gets \$1. If two red cards are played, each gets nothing. Each cares just about their own wealth, so getting \$1 is worth 1 util. All of this is common knowledge. More formally, here is the game table, with $A$ on the row and $B$ on the column.

\begin{center}
\begin{tabular}{r c c}
 & \textit{green} & \textit{red} \\
GREEN & 1, 1 & 1, 1 \\
RED & 1, 1 & 0, 0
\end{tabular}
\end{center}
When I write game tables like this, and I think this is the usual way game tables are to be interpreted \citep{Weatherson2012-WEAKBI}, I mean that the players know that these are the payouts, that the players know the other players to be rational, and these pieces of knowledge are common knowledge to at least as many iterations as needed to solve the game. With that in mind, let's think about how the agents should approach this game.

I'm going to make one big simplifying assumption at first. We'll relax this later, but it will help the discussion a lot I think to start with this assumption. This assumption is that the doctrine of \textbf{Uniqueness} applies here; there is precisely one rational credence to have in any salient proposition about how the game will play. Some philosophers think that Uniqueness always holds \citep{White2005-WHIEP}. I don't, but it does seem like it might \textit{often} hold. Anyway, I'm going to start by assuming that it does hold here.

The first thing to note about the game is that it is symmetric. So the probability of $A$ playing GREEN should be the same as the probability of $B$ playing \textit{green}, since $A$ and $B$ face exactly the same problem. Call this common probability $x$. If $x < 1$, we get a quick contradiction. The expected value, to Row, of GREEN, is 1. Indeed, the known value of GREEN is 1. If the probability of \textit{green} is $x$, then the expected value of RED is $x$. So if $x < 1$, and Row is rational, she'll definitely play GREEN. But that's inconsistent with the claim that $x < 1$, since that means that it isn't definite that Row will play GREEN.

So we can conclude that $x = 1$. Does that mean we can know that Row will play GREEN? No. Assume we could conclude that. Whatever reason we would have for concluding that would be a reason for any rational person to conclude that Column will play \textit{green}. Since any rational person can conclude this, Row can conclude it. So Row knows that she'll get 1 whether she plays GREEN or RED. But then she should be indifferent between playing GREEN and RED. And if we know she's indifferent between playing GREEN and RED, and our only evidence for what she'll play is that she's a rational player who'll maximise her returns, then we can't be in a position to know she'll play GREEN.

I think the arguments of the last two paragraphs are sound. We'll turn to an objection presently, but let's note how bizarre is the conclusion we've reached. One argument has shown that it could not be more probable that Row will play GREEN. A second argument has shown that we can't know that Row will play GREEN. It reminds me of examples involving blindspots \citep{Sorensen1988}. Consider this case:

\begin{enumerate*}
\renewcommand{\labelenumi}{(\Alph{enumi})}
\setcounter{enumi}{1}
\item Brian does not know (B).
\end{enumerate*}
That's true, right? Assume it's false, so I do know (B). Knowledge is factive, so (B) is true. But that contradicts the assumption that it's false. So it's true. But I obviously don't know that it's true; that's what this very true proposition says.

Now I'm not going to rest anything on this case, because there are so many tricky things one can say about blindspots, and about the paradoxes generally. It does suggest that there are other finite cases where one can properly have maximal credence in a true proposition without knowledge.\footnote{As an aside, the existence of these cases is why I get so irritated when epistemologists try to theorise about `Gettier Cases' as a class. What does (B) have in common with inferences from a justified false belief, or with otherwise sound reasoning that is ever so close to issuing in a false conclusion due to relatively bad luck? As far as I can tell, the class of justified true beliefs that aren't knowledge is a disjunctive mess, and this should matter for thinking about the nature of knowledge. For further examples, see \cite{WilliamsonLofoten}.} And, assuming that we shouldn't believe things we know we don't know, that means we can have maximal credence in things we don't believe. All I want to point out is that this phenomena of maximal credence without knowledge, and presumably without full belief, isn't a quirky feature of self-reference, or of games, or of puzzles about infinity; it comes up in a wide range of cases.

For the rest of this section I want to reply to one objection, and weaken an assumption I made earlier. The objection is that I'm wrong to assume that agents will only maximise expected utility. They may have tie-breaker rules, and those rules might undermine the arguments I gave above. The assumption is that there's a uniquely rational credence to have in any given situation.

I argued that if we knew that $A$ would play GREEN, we could show that $A$ had no reason to play GREEN. But actually what we showed was that the expected utility of playing GREEN would be the same as playing RED. Perhaps $A$ has a reason to play GREEN, namely that GREEN weakly dominates RED. After all, there's one possibility on the table where GREEN does better than RED, and none where RED does better. And perhaps that's a reason, even if it isn't a reason that expected utility considerations are sensitive to.

Now I don't want to insist on expected utility maximisation as the only rule for rational decision making. Sometimes, I think some kind of tie-breaker procedure is part of rationality. In the papers by Stalnaker I mentioned above, he often appeals to this kind of weak dominance reasoning to resolve various hard cases. But I don't think weak dominance provides a reason to play GREEN in this particular case. When Stalnaker says that agents should use weak dominance reasoning, it is always in the context of games where the agents' attitude towards the game matrix is different to their attitude towards each other. One case that Stalnaker discusses in detail is where the game table is common knowledge, but there is merely common (justified, true) belief in common rationality. Given such a difference in attitudes, it does seem there's a good sense in which the most salient departure from equilibrium will be one in which the players end up somewhere else on the table. And given that, weak dominance reasoning seems appropriate.

But that's not what we've got here. Assuming that rationality requires playing GREEN/\textit{green}, the players know we'll end up in the top left corner of the table. There's no chance that we'll end up elsewhere. Or, perhaps better, there is just as much chance we'll end up `off the table', as that we'll end up in a non-equilibrium point on the table. To make this more vivid, consider the `possibility' that $B$ will play \textit{blue}, and if $B$ plays \textit{blue}, $A$ will receive 2 if she plays RED, and -1 if she plays GREEN. Well hold on, you might think, didn't I say that \textit{green} and \textit{red} were the only options, and this was common knowledge? Well, yes, I did, but if the exercise is to consider what would happen if something the agent knows to be true doesn't obtain, then the possibility that one agent will play blue certainly seems like one worth considering. It is, after all, a metaphysical possibility. And if we take it seriously, then it isn't true that under \textit{any} possible play of the game, GREEN does better than RED.

We can put this as a dilemma. Assume, for \textit{reductio}, that GREEN/\textit{green} is the only rational play. Then if we restrict our attention to possibilities that are epistemically open to $A$, then GREEN does just as well as RED; they both get 1 in every possibility. If we allow possibilities that are epistemically closed to $A$, then the possibility where $B$ plays \textit{blue} is just as relevant as the possibility that $B$ is irrational. After all, we stipulated that this is a case where rationality is common knowledge. In neither case does the weak dominance reasoning get any purchase.

With that in mind, we can see why we don't need the assumption of Uniqueness. Let's play through how a failure of Uniqueness could undermine the argument. Assume, again for \textbf{reductio}, that we have credence $\varepsilon > 0$ that $A$ will play RED. Since $A$ maximises expected utility, that means $A$ must have credence 1 that $B$ will play \textit{green}. But this is already odd. Even if you think people can have different reactions to the same evidence, it is odd to think that one rational agent could regard a possibility as infinitely less likely than another, given isomorphic evidence. And that's not all of the problems. Even if $A$ has credence 1 that $B$ will play \textit{green}, it isn't obvious that playing RED is rational. After all, relative to the space of epistemic possibilities, GREEN weakly dominates RED. Remember that we're no longer assuming that it can be known what $A$ or $B$ will play. So even without Uniqueness, there are two reasons to think that it is wrong to have credence $\varepsilon > 0$ that $A$ will play RED. So we've still shown that credence 1 doesn't imply knowledge, and since the proof is known to us, and full belief is incompatible with knowing that you can't know, this is a case where credence 1 doesn't imply full belief. So whether $A$ plays GREEN, like whether the coin will ever land tails, is a case the Lockean cannot get right, no matter where they set the threshold for belief; our credence is above the threshold, but we don't believe. 

So I think this case is a real problem for a Lockean view about the relationship between credence and belief. If \textit{A} is rational, she can have credence 1 that \textit{B} will play \textit{green}, but won't believe that \textit{B} will play \textit{green}. But now you might worry that my own account of the relationship between belief and credence is in just as much trouble. After all, I said that to believe $p$ is, roughly, to have the same attitudes towards all salient questions as you have conditional on $p$. And it's hard to identify a question that rational \textit{A} would answer differently upon conditionalising on the proposition that \textit{B} plays \textit{green}.

I think what went wrong in my earlier view was that I'd too quickly equated updating with conditionalisation. The two can come apart. Here's an example from \cite{Gillies2010} that makes the point well. 

\begin{quote}I have lost my marbles. I know that just one of them -- Red or Yellow -- is in the box. But I don't know which. I find myself saying things like \dots ``If Yellow isn't in the box, the Red must be.'' (4:13)
\end{quote}
As Gillies goes on to point out, this isn't really a problem for the Ramsey test view of conditionals.
\begin{quote}
The Ramsey test -- the schoolyard version, anyway -- is a test for when an indicative conditional is acceptable given your beliefs. It says that (if \textit{p})(\textit{q}) is acceptable in belief state \textit{B} iff \textit{q} is acceptable in the derived or subordinate state \textit{B}-plus-the-information-that-\textit{p}. (4:27)
\end{quote}
And he notes that this can explain what goes on with the marbles conditional. Add the information that Yellow isn't in the box, and it isn't just true, but must be true, that Red is in the box.

Note though that while we can explain this conditional using the Ramsey test, we can't explain it using any version of the idea that probabilities of conditionals are conditional probabilities. The probability that Red must be in the box is 0. The probability that Yellow isn't in the box is not 0. So conditional on Yellow not being in the box, the probability that Red must be in the box is still 0. Yet the conditional is perfectly assertable.

There is, and this is Gillies's key point, something about the behaviour of modals in the consequents of conditionals that we can't capture using conditional probabilities, or indeed many other standard tools. And what goes for consequents of conditionals goes for updated beliefs too. Learn that Yellow isn't in the box, and you'll conclude that Red must be. But that learning can't go via conditionalisation; just conditionalise on the new information and the probability that Red must be in the box goes from 0 to 0.

Now it's a hard problem to say exactly how this alternative to updating by conditionalisation should work. But very roughly, the idea is that at least some of the time, we update by eliminating worlds from the space of possibilities. This affects dramatically the probability of propositions whose truth is sensitive to which worlds are in the space of possibiilties.

For example, in the game I've been discussing, we should believe that rational \textit{B} might play \textit{red}. Indeed, the probability of that is, I think, 1. And whether or not \textit{B} might play red is highly salient; it matters to the probability of whether \textbf{A} will play GREEN or RED. Conditionalising on something that has probability 1, such as that \textit{B} will play \textit{green}, can hardly change that probability. But updating on the proposition that \textit{B} will play \textit{green} can make a difference. We can see that by simply noting that the conditional \textit{If B plays green, she might play red} is incoherent.

So I conclude that a theory of belief like mine can handle the puzzle this game poses, as long as it distinguishes between conditionalising and updating, in just the way Gillies suggests. To believe that \textit{p} is to be disposed to not change any attitude towards a salient question on updating that \textit{p}. (Plus some bells and whistles to deal with propositions that are not relevant to salient questions. We'll return to them below.) Updating often goes by conditionalisation, so we can often say that belief means having attitudes that match unconditionally and conditionally on \textit{p}. But not all updating works that way, and the theory of belief needs to acknowledge this.

