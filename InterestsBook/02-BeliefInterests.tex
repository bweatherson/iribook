\chapter{Belief and Interests}
%\markright{Belief and Interests}

\section{Belief and Degree of Belief}

Traditional epistemology deals with beliefs and their justification. Bayesian epistemology deals with degrees of belief and their justification. In some sense they are both talking about the same thing, namely epistemic justification. Two questions naturally arise. Do we really have two subject matters here (degrees of belief and belief \textit{tout court}) or two descriptions of the one subject matter? If just one subject matter, what relationship is there between the two modes of description of this subject matter?

The answer to the first question is I think rather easy. There is no evidence to believe that the mind contains two representational systems, one to represent things as being probable or improbable and the other to represent things as being true or false. The mind probably does contain a vast plurality of representational systems, but they don't divide up the doxastic duties this way. If there are distinct visual and auditory representational systems, they don't divide up duties between degrees of belief and belief \textit{tout court}, for example. If there were two distinct systems, then we should imagine that they could vary independently, at least as much as is allowed by constitutive rationality. But such variation is hard to fathom. So I'll infer that the one representational system accounts for our credences and our categorical beliefs. (It follows from this that the question \cite{Bovens1999} ask, namely what beliefs \textit{should} an agent have given her degrees of belief, doesn't have a non-trivial answer. If fixing the degrees of belief in an environment fixes all her doxastic attitudes, as I think it does, then there is no further question of what she should believe given these are her degrees of belief.) 

The second question is much harder. It is tempting to say that \(S\) believes that \(p\) iff S's credence in \(p\) is greater than some salient number \(r\), where \(r\) is made salient either by the context of belief ascription, or the context that \(S\) is in. I'm following Mark \cite{Kaplan1996} in calling this the threshold view. There are two well-known problems with the threshold view, both of which seem fatal to me.

As Robert Stalnaker \citeyearpar[91]{Stalnaker1984} emphasised, any number \(r\) is bound to seem arbitrary. Unless these numbers are made salient by the environment, there is no special difference between believing \(p\) to degree 0.9786 and believing it to degree 0.9875. But if \(r\) is 0.98755, this will be \textit{the difference} between believing \(p\) and not believing it, which is an important difference. The usual response to this, as found in \cite[Ch. 4]{Foley1993} and \cite{Hunter1996} is to say that the boundary is vague. But it's not clear how this helps. On an epistemic theory of vagueness, there is still a number such that degrees of belief above that count, and degrees below that do not, and any such number is bound to seem unimportant. On supervaluational theories, the same is true. There won't be a \textit{determinate} number, to be sure, but there will a number, and that seems false. My preferred degree of belief theory of vagueness, as set out in \cite{Weatherson2005-WEATTT} has the same consequence. Hunter defends a version of the threshold view combined with a theory of vagueness based around fuzzy logic, which seems to be the only theory that could avoid the arbitrariness objection. But as \cite{Williamson1994-WILV} showed, there are deep and probably insurmountable difficulties with that position. So I think the vagueness response to the arbitrariness objection is (a) the only prima facie plausible response and (b) unsuccessful. 

The second problem concerns conjunction. It is also set out clearly by Stalnaker.

\begin{quote}
Reasoning in this way from accepted premises to their deductive consequences (\(P\), also \(Q\), therefore \(R\)) does seem perfectly straightforward. Someone may object to one of the premises, or to the validity of the argument, but one could not intelligibly agree that the premises are each acceptable and the argument valid, while objecting to the acceptability of the conclusion. \cite[92]{Stalnaker1984}
\end{quote}

\noindent If categorical belief is having a credence above the threshold, then one can coherently do exactly this. Let \(x\) be a number between \(r\) and than \(r\)\textsuperscript{ \(\nicefrac{1}{2}\)}, such that for an atom of type U has probability \(x\) of decaying within a time \(t\), for some \(t\) and U. Assume our agent knows this fact, and is faced with two (isolated) atoms of U. Let \(p\) be that the first decays within \(t\), and \(q\) be that the second decays within \(t\). She should, given her evidence, believe \(p\) to degree \(x, q\) to degree \(x\), and \(p \wedge q\) to degree \(x ^2\). If she believed \(p \wedge q\) to a degree greater than \(r\), she'd have to either have credences that were not supported by her evidence, or credences that were incoherent. (Or, most likely, both.) So this theory violates the platitude. This is a well-known argument, so there are many responses to it, most of them involving something like appeal to the preface paradox. I'll argue in section 4 that the preface paradox doesn't in fact offer the threshold view proponent much support here. But even before we get to there, we should note that the arbitrariness objection gives us sufficient reason to reject the threshold view.

A better move is to start with the functionalist idea that to believe that \(p\) is to treat \(p\) as true for the purposes of practical reasoning. To believe \(p\) is to have preferences that make sense, by your own lights, in a world where \(p\) is true. So, if you prefer A to B and believe that \(p\), you prefer A to B given \(p\). For reasons that will become apparent below, we'll work in this paper with a notion of preference where \textit{conditional} preferences are primary.\footnote{To say the agent prefers A to B given \(q\) is not to say that if the agent were to learn \(q\), she would prefer A to B. It's rather to say that she prefers the state of the world where she does A and \(q\) is true to the state of the world where she does B and \(q\) is true. These two will come apart in cases where learning \(q\) changes the agent's preferences. We'll return to this issue below.} So the core insight we'll work with is the following:

\begin{quote}
If you prefer A to B given \(q\), and you believe that \(p\), then you prefer A to B given \(p \wedge q\)
\end{quote}

\noindent The bold suggestion here is that if that is true for all the A, B and \textit{q }that matter, then you believe \(p\). Put formally, where \textit{Bel}(\(p\)) means that the agent believes that \(p\), and A \(\geq _q\) B means that the agent thinks A is at least as good as B given \(q\), we have the following

\begin{enumerate*}
\item \textit{Bel}(\(p\)) \(\leftrightarrow \forall\)A\(\forall\)B\(\forall q\) (A \(\geq _q\) B \(\leftrightarrow\) A \(\geq _{p \wedge q}\) B)
\end{enumerate*}

\noindent In words, an agent believes that \(p\) iff conditionalising on \(p\) doesn't change any conditional preferences over things that matter.\footnote{This might seem \textit{much} too simple, especially when compared to all the bells and whistles that functionalists usually put in their theories to (further) distinguish themselves from crude versions of behaviourism. The reason we don't need to include those complications here is that they will all be included in the analysis of \textit{preference}. Indeed, the theory here is compatible with a thoroughly anti\nobreakdash-functionalist treatment of preference. The claim is not that we can offer a functional analysis of belief in terms of non-mental concepts, just that we can offer a functionalist reduction of belief to other mental concepts. The threshold view is \textit{also} such a reduction, but it is such a crude reduction that it doesn't obviously fall into any category.} The left-to-right direction of this seems trivial, and the right\nobreakdash-to\nobreakdash-left direction seems to be a plausible way to operationalise the functionalist insight that belief is a functional state. There is some work to be done if (1) is to be interpreted as a truth though.

If we interpret the quantifiers in (1) as unrestricted, then we get the (false) conclusion that just about no one believes no contingent propositions. To prove this, consider a bet that wins iff the statue in front of me waves back at me due to random quantum effects when I wave at it. If I take the bet and win, I get to live forever in paradise. If I take the bet and lose, I lose a penny. Letting A be that I take the bet, B be that I decline the bet, \(q\) be a known tautology (so my preferences given \(q\) are my preferences \textit{tout court}) and \(p\) be that the statue does not wave back, we have that I prefer A to B, but not A to B given \(p\). So by this standard I don't believe that \(p\). This is false -- right now I believe that statues won't wave back at me when I wave at them.

This seems like a problem. But the solution to it is not to give up on functionalism, but to insist on its pragmatic foundations. The quantifiers in (1) should be restricted, with the restrictions motivated pragmatically. What is crucial to the theory is to say what the restrictions on A and B are, and what the restrictions on \(q\) are. We'll deal with these in order.

For better or worse, I don't right now have the option taking that bet and hence spending eternity in paradise if the statue waves back at me. Taking or declining such unavailable bets are not open choices. For any option that is open to me, assuming that statues do not in fact wave does not change its utility. That's to say, I've already factored in the non-waving behaviour of statues into my decision-making calculus. That's to say, I believe statues don't wave.

An action A is a live option for the agent if it is really possible for the agent to perform A. An action A is a salient option if it is an option the agent takes seriously in deliberation. Most of the time gambling large sums of money on internet gambling sites over my phone is a live option, but not a salient option. I know this option is suboptimal, and I don't have to recompute every time whether I should do it. Whenever I'm making a decision, I don't have to add in to the list of choices \textit{bet thousands of dollars on internet gambling sites}, and then rerule that out every time. I just don't consider that option, and properly so. If I have a propensity to daydream, then becoming the centrefielder for the Boston Red Sox might be a salient option to me, but it certainly isn't a live option. We'll say the two initial quantifiers range over the options that are live and salient options for the agent. 

Note that we \textit{don't} say that the quantifiers range over the options that are live and salient for the person making the belief ascription. That would lead us to a form of contextualism for which we have little evidence. We also don't say that an option becomes salient for the agent iff they \textit{should} be considering it. At this stage we are just saying what the agent does believe, not what they should believe, so we don't have any clauses involving normative concepts.

Now we'll look at the restrictions on the quantifier over propositions. Say a proposition is \textit{relevant} if the agent is disposed to take seriously the question of whether it is true (whether or not she is currently considering that question) and conditionalising on that proposition or its negation changes some of the agents \textit{unconditional} preferences over live, salient options.\footnote{Conditionalising on the proposition \textit{There are space aliens about to come down and kill all the people writing epistemology papers} will make me prefer to stop writing this paper, and perhaps grab some old metaphysics papers I could be working on. So that proposition satisfies the second clause of the definition of relevance. But it clearly doesn't satisfy the first clause. This part of the definition of relevance won't do much work until the discussion of agents with mistaken environmental beliefs in section 7.} The first clause is designed to rule out wild hypotheses that the agent does not take at all seriously. If \(q\) is not such a proposition, if the agent is disposed to take it seriously, then it is relevant if there are live, salient A and B such that A \(\geq _q\) B \(\leftrightarrow\) A \(\geq\) B is false. Say a proposition is \textit{salient} if the agent is currently considering whether it is true. Finally, say a proposition is \textit{active} relative to \(p\) iff it is a (possibly degenerate) conjunction of propositions such that each conjunct is either relevant or salient, and such that the conjunction is consistent with \(p\). (By a degenerate conjunction I mean a conjunction with just one conjunct. The consistency requirement is there because it might be hard in some cases to make sense of preferences given inconsistencies.) Then the propositional quantifier in (1) ranges over active propositions.

We will expand and clarify this in the next section, but our current solution to the relationship between beliefs and degrees of belief is that degrees of belief determine an agent's preferences, and she believes that \(p\) iff the claim (1) about her preferences is true when the quantifiers over options are restricted to live, salient actions, and the quantifier over propositions is restricted to salient propositions. The simple view would be to say that the agent believes that \(p\) iff conditioning on \(p\) changes none of her preferences. The more complicated view here is that the agent believes that \(p\) iff conditioning on \(p\) changes none of her conditional preferences over live, salient options, where the conditions are also active relative to \(p\).


\section{Impractical Propositions}

The theory sketched in the previous paragraph seems to me right in the vast majority of cases. It fits in well with a broadly functionalist view of the mind, and as we'll see it handles some otherwise difficult cases with aplomb. But it needs to be supplemented a little to handle beliefs about propositions that are practically irrelevant. I'll illustrate the problem, then note how I prefer to solve it.

I don't know what Julius Caeser had for breakfast the morning he crossed the Rubicon. But I think he would have had \textit{some} breakfast. It is hard to be a good general without a good morning meal after all. Let \(p\) be the proposition that he had breakfast that morning. I believe \(p\). But this makes remarkably little difference to my practical choices in most situations. True, I wouldn't have written this paragraph as I did without this belief, but it is rare that I have to write about Caeser's dietary habits. In general whether \(p\) is true makes no practical difference to me. This makes it hard to give a pragmatic account of whether I believe that \(p\). Let's apply (1) to see whether I really believe that \(p\).

\begin{enumerate*}
\item \textit{Bel}(\(p\)) \(\leftrightarrow \forall\)A\(\forall\)B\(\forall q\) (A \(\geq _q\) B \(\leftrightarrow\) A \(\geq _{p \wedge q}\) B)
\end{enumerate*}

\noindent Since \(p\) makes no practical difference to any choice I have to make, the right hand side is true. So the left hand side is true, as desired. The problem is that the right hand side of (2) is also true here.

\begin{enumerate*}
\setcounter{enumi}{1}
\item \textit{Bel}(\(\neg p\)) \(\leftrightarrow \forall\)A\(\forall\)B\(\forall q\) (A \(\geq _q\) B \(\leftrightarrow\) A \(\geq _{\neg p \wedge q}\) B)
\end{enumerate*}

\noindent Adding the assumption that Caeser had no breakfast that morning doesn't change any of my practical choices either. So I now seem to \textit{inconsistently} believe both \(p\) and \(\neg p\). I have some inconsistent beliefs, I'm sure, but those aren't among them. We need to clarify what (1) claims.

To do so, I supplement the theory sketched in section 2 with the following principles.


\begin{itemize*}
\item A proposition \(p\) is \textit{eligible} \textit{for belief} if it satisfies \(\forall\)A\(\forall\)B\(\forall q\) (A \(\geq _q\) B \(\leftrightarrow\) A \(\geq _{p \wedge q}\) B), where the first two quantifiers range over the open, salient actions in the sense described in section 2.
\item For any proposition \(p\), and any proposition \(q\) that is relevant or salient, among the actions that are (by stipulation!) open and salient with respect to \(p\) are \textit{believing that p}, \textit{believing that q}, \textit{not believing that p} and \textit{not believing that q}
\item For any proposition, the subject prefers believing it to not believing it iff (a) it is eligible for belief and (b) the agents degree of belief in the proposition is greater than \(\nicefrac{1}{2}\). 
\item The previous stipulation holds both unconditionally and conditional on \(p\), for any \(p\).
\item The agent believes that \(p\) iff \(\forall\)A\(\forall\)B\(\forall q\) (A \(\geq _q\) B \(\leftrightarrow\) A \(\geq _{p \wedge q}\) B), where the first two quantifiers range over all actions that are either open and salient \textit{tout court} (i.e. in the sense of section 2) or open and salient with respect to \(p\) (as described above).
\end{itemize*}

\noindent This all looks moderately complicated, but I'll explain how it works in some detail as we go along. One simple consequence is that an agent only believes that \(p\) iff their degree of belief in \(p\) is greater than \(\nicefrac{1}{2}\). Since my degree of belief in Caeser's foodless morning is not greater than \(\nicefrac{1}{2}\), in fact it is considerably less, I don't believe \(\neg p\). On the other hand, since my degree of belief in \(p\) is considerably greater than \(\nicefrac{1}{2}\), I prefer to believe it than disbelieve it, so I believe it.

There are many possible objections to this position, which I'll address sequentially.

\medskip

\noindent \textit{Objection}: Even if I have a high degree of belief in \(p\), I might prefer to not believe \(p\) because I think that belief in \(p\) is bad for some other reason. Perhaps, if \(p\) is a proposition about my brilliance, it might be immodest to believe that \(p\).

\noindent \textit{Reply}: Any of these kinds of considerations should be put into the credences. If it is immodest to believe that you are a great philosopher, it is equally immodest to believe to a high degree that you are a great philosopher.

\medskip

\noindent \textit{Objection}: Belief that \(p\) is not an action in the ordinary sense of the term.

\noindent \textit{Reply}: True, which is why this is described as a supplement to the original theory, rather than just cashing out its consequences.

\medskip

\noindent \textit{Objection}: It is impossible to choose to believe or not believe something, so we shouldn't be applying these kinds of criteria.

\noindent \textit{Reply}: I'm not as convinced of the impossibility of belief by choice as others are, but I won't push that for present purposes. Let's grant that beliefs are always involuntary. So these `actions' aren't open actions in any interesting sense, and the theory is section 2 was really incomplete. As I said, this is a supplement to the theory in section 2.

This doesn't prevent us using principles of constitutive rationality, such as we prefer to believe \(p\) iff our credence in \(p\) is over \(\nicefrac{1}{2}\). Indeed, on most occasions where we use constitutive rationality to infer that a person has some mental state, the mental state we attribute to them is one they could not fail to have. But functionalists are committed to constitutive rationality \citep{Lewis1994b}. So my approach here is consistent with a broadly functionalist outlook.

\medskip

\noindent \textit{Objection}: This just looks like a roundabout way of stipulating that to believe that \(p\), your degree of belief in \(p\) has to be greater than \(\nicefrac{1}{2}\). Why not just add that as an extra clause than going through these little understood detours about preferences about beliefs?

\noindent \textit{Reply}: There are three reasons for doing things this way rather than adding such a clause. 

First, it's nice to have a systematic theory rather than a theory with an ad hoc clause like that. 

Second, the effect of this constraint is much more than to restrict belief to propositions whose credence is greater than \(\nicefrac{1}{2}\). Consider a case where \(p\) and \(q\) and their conjunction are all salient, \(p\) and \(q\) are probabilistically independent, and the agent's credence in each is 0.7. Assume also that \(p, q\) and \(p \wedge q\) are completely irrelevant to any practical deliberation the agent must make. Then the criteria above imply that the agent does not believe that \(p\) or that \(q\). The reason is that the agent's credence in \(p \wedge q\) is 0.49, so she prefers to not believe \(p \wedge q\). But conditional on \(p\), her credence in \(p \wedge q\) is 0.7, so she prefers to believe it. So conditionalising on \(p\) does change her preferences with respect to believing \(p \wedge q\), so she doesn't believe \(p\). So the effect of these stipulations rules out much more than just belief in propositions whose credence is below \(\nicefrac{1}{2}\).

This suggests the third, and most important point. The problem with the threshold view was that it led to violations of closure. Given the theory as stated, we can prove the following theorem. Whenever \(p\) and \(q\) and their conjunction are all open or salient, and both are believed, and the agent is probabilistically coherent, the agent also believes \(p \wedge q\). This is a quite restricted closure principle, but this is no reason to deny that it is \textit{true}, as it fails to be true on the threshold view.

The proof of this theorem is a little complicated, but worth working through. First we'll prove that if the agent believes \(p\), believes \(q\), and \(p\) and \(q\) are both salient, then the agent prefers believing \(p \wedge q\) to not believing it, if \(p \wedge q\) is eligible for belief. In what follows \textit{Pr}(\(x | y\)) is the agent's conditional degree of belief in \(x\) given \(y\). Since the agent is coherent, we'll assume this is a probability function (hence the name).

\begin{enumerate*}
\item Since the agent believes that \(q\), they prefer believing that \(q\) to not believing that \(q\) (by the criteria for belief)
\item So the agent prefers believing that \(q\) to not believing that \(q\) given \(p\) (From 1 and the fact that they believe that \(p\), and that \(q\) is salient)
\item So \textit{Pr}(\(q | p\)) \(> \nicefrac{1}{2}\) (from 2)
\item \textit{Pr}(\(q | p\)) = \textit{Pr}(\(p \wedge q | p\)) (by probability calculus)
\item So \textit{Pr}(\(p \wedge q | p\)) \(> \nicefrac{1}{2}\) (from 3, 4)
\item So, if \(p \wedge q\) is eligible for belief, then the agent prefers believing that \(p \wedge q\) to not believing it, given \(p\) (from 5)
\item So, if \(p \wedge q\) is eligible for belief, the agent prefers believing that \(p \wedge q\) to not believing it (from 6, and the fact that they believe that \(p\), and \(p \wedge q\) is salient)
\end{enumerate*}

\noindent So whenever, \(p, q\) and \(p \wedge q\) are salient, and the agent believes each conjunct, the agent prefers believing the conjunction \(p \wedge q\) to not believing it, if \(p \wedge q\) is eligible. Now we have to prove that \(p \wedge q\) is eligible for belief, to prove that it is actually believed. That is, we have to prove that (5) follows from (4) and (3), where the initial quantifiers range over actions that are open and salient \textit{tout court}.

\begin{enumerate*}
\renewcommand{\labelenumi}{(\arabic{enumi})}
\setcounter{enumi}{2}
\item \(\forall\)A\(\forall\)B\(\forall r\) (A \(\geq_r\) B \(\leftrightarrow\) A \(\geq _p  \wedge r\) B)
\item \(\forall\)A\(\forall\)B\(\forall r\) (A \(\geq_r\) B \(\leftrightarrow\) A \(\geq _q  \wedge r\) B)
\item \(\forall\)A\(\forall\)B\(\forall r\) (A \(\geq_r\) B \(\leftrightarrow\) A \(\geq _{p \wedge q \wedge r}\) B)
\end{enumerate*}

\noindent Assume that (5) isn't true. That is, there are A, B and \(s\) such that \(\neg\)(A \(\geq_s\) B \(\leftrightarrow\) A \(\geq _{p \wedge q \wedge s}\)B). By hypothesis \(s\) is active, and consistent with \(p \wedge q\). So it is the conjunction of relevant, salient propositions. Since \(q\) is salient, this means \(q \wedge s\) is also active. Since \(s\) is consistent with \(p \wedge q\), it follows that \(q \wedge s\) is consistent with \(p\). So \(q \wedge s\) is a possible substitution instance for \(r\) in (3). Since (3) is true, it follows that A \(\geq _{q \wedge s}\) B \(\leftrightarrow\) A \(\geq _{p \wedge q \wedge s}\) B. By similar reasoning, it follows that \(s\) is a permissible substitution instance in (4), giving us A \(\geq_s\) B \(\leftrightarrow\) A \(\geq _{q \wedge s}\) B. Putting the last two biconditionals together we get A \(\geq_s\) B \(\leftrightarrow\) A \(\geq _{p \wedge q \wedge s}\)B, contradicting our hypothesis that there is a counterexample to (5). So whenever (3) and (4) are true, (5) is true as well, assuming \(p, q\) and \(p \wedge q\) are all salient.


\section{The Interest-Relativity of Belief}

\subsection{Interests and Functional Roles}
The previous section was largely devoted to proving an existential claim: there is \textit{some} interest-relativity to knowledge. Or, if you prefer, it proved a negative claim: the best theory of knowledge is \textit{not} interest-neutral. But this negative conclusion invites a philosophical challenge: what is the best explanation of the interest-relativity of knowledge? My answer is in two parts. Part of the interest-relativity of knowledge comes from the interest-relativity of belief, and part of it comes from the fact that interests generate certain kinds of doxastic defeaters. It's the second part, the part that is new to this paper, that makes the theory a version of non-doxastic IRI.

Here's my theory of belief. $S$ believes that $p$ iff conditionalising on $p$ doesn't change $S$'s answer to any relevant question. I'm using `relevance' here in a non-technical sense; I say a lot more about how to cash out the notion in my \citeyearpar{Weatherson2005-WEACWD}. The key thing to note is that relevance is interest-relative, so the theory of belief is interest-relative. There is a bit more to say about what kind of \textit{questions} are important for this definition of belief. In part because I've changed my mind a little bit on this since the earlier paper, I'll spend a bit more time on it. The following four kinds of questions are the most important.

\begin{itemize*}
\item How probable is $q$?
\item Is $q$ or $r$ more probable?
\item How good an idea is it to do $\varphi$?
\item Is it better to do $\varphi$ or $\psi$?
\end{itemize*}

\noindent The theory of belief says that someone who believes that $p$doesn't change their answer to any of these questions upon conditionalising on $p$. Putting this formally, and making the restriction to relevant questions explicit, we get the following theorems of our theory of belief.\footnote{In the last two lines, I use $U(\varphi)$ to denote the expected utility of $\varphi$, and $U(\varphi | p)$ to denote the expected utility of $\varphi$ conditional on $p$. It's often easier to write this as simply $U(\varphi \wedge p)$, since the utility of $\varphi$ conditional on $p$ just is the utility of doing $\varphi$ in a world where $p$ is true. That is, it is the utility of $\varphi \wedge p$ being realised. But we get a nicer symmetry between the probabilistic principles and the utility principles if we use the explictly conditional notation for each.}

\begin{description*}
\item[BAP] For all relevant $q, x$, if $p$ is believed then $\Pr(q) = x$ iff $\Pr(q | p) = x$.
\item[BCP] For all relevant $q, r$, if $p$ is believed then $\Pr(q) \geq \Pr(r)$ iff $\Pr(q | p) \geq \Pr(r | p)$.
\item[BAU] For all relevant $\varphi, x$, if $p$ is believed then $U(\varphi) = x$ iff $U(\varphi | p) = x$.
\item[BCU] For all relevant $\varphi, \psi$, if $p$ is believed then $U(\varphi) \geq U(\psi)$ iff $U(\varphi | p) \geq U(\psi | p)$.
\end{description*}

\noindent In the earlier paper I focussed on \textbf{BAU} and \textbf{BCU}. But \textbf{BAP} and \textbf{BCP} are important as well. Indeed, focussing on them lets us derive a nice result. 

Charlie is trying to figure out exactly what the probability of $p$ is. That is, for any $x \in [0, 1]$, whether $\Pr(p) = x$ is a relevant question. Now Charlie is well aware that $\Pr(p | p) = 1$. So unless $\Pr(p) = 1$, Charlie will give a different answer to the questions \textit{How probable is p?} and \textit{Given p, how probable is p?}. So unless Charlie holds that $\Pr(p)$ is 1, she won't count as believing that $p$. One consequence of this is that Charlie can't reason, ``The probability of $p$ is exactly 0.978, so $p$.'' That's all to the good, since that looks like bad reasoning. And it looks like bad reasoning even though in some circumstances Charlie can rationally believe propositions that she (rationally) gives credence 0.978 to. Indeed, in some circumstances she can rationally believe something \textit{in virtue} of it being 0.978 probable.

That's because the reasoning in the previous paragraph assumes that every question of the form \textit{Is the probability of p equal to x?} is relevant. In practice, fewer questions than that will be relevant. Let's say that the only questions relevant to Charlie are of the form \textit{What is the probability of $p$ to one decimal place?}. And assume that no other questions become relevant in the course of her inquiry into this question.\footnote{This is probably somewhat unrealistic. It's hard to think about whether $\Pr(p)$ is closer to 0.7 or 0.8 without raising to salience questions about, for example, what the second decimal place in $\Pr(p)$ is. This is worth bearing in mind when coming up with intuitions about the cases in this paragraph.} Charlie decides that to the first decimal place, $\Pr(p) = 1.0$, i.e., $\Pr(p) > 0.95$. That is compatible with simply believing that $p$. And that seems right; if for practical purposes, the probability of $p$ is indistinguishable from 1, then the agent is confident enough in $p$ to believe it.

So there are some nice features of this theory of belief. Indeed, there are several reasons to believe it. It is, I have argued, the best functionalist account of belief. I'm not going to argue for functionalism about the mind, since the argument would take at least a book. (The book in question might look a lot like \cite{DBMJackson2007}.) But I do think functionalism is true, and so the best functionalist theory of belief is the best theory of belief.

The argument for this theory of belief in my  \citeyearpar{Weatherson2005-WEACWD} rested heavily on the flaws of rival theories. We can see those flaws by looking at a tension that any theory of the relationship between belief and credence must overcome. Each of the following three principles seems to be plausible.

\begin{enumerate*}
\item If $S$ has a greater credence in $p$ than in $q$, and she believes $q$, then she believes $p$ as well; and if her credences in both $p$ and $q$ are rational, and her belief in $q$ is rational, then so is her belief in $p$.
\item If $S$ rationally believes $p$ and rationally believes $q$, then it is open to her to rationally believe $p \wedge q$ without changing her credences.
\item $S$ can rationally believe $p$ while having credence of less than 1 in $p$.
\end{enumerate*}

\noindent But these three principles, together with some principles that are genuinely uncontroversial, entail an absurd result. By 3, there is some $p$ such that \textit{Cr}$(p) = x < 1$, and $p$ is believed. (\textit{Cr} is the function from any proposition to our agent's credence in that propositions.) Let $S$ know that a particular fair lottery has $l$ tickets, where $l > \nicefrac{1}{1-x}$. The uncontroversial principle we'll use is that in such a case $S$'s credence that any given ticket will lose should be $\nicefrac{l-1}{l}$. Since $\nicefrac{l-1}{l} > x$, it follows by 1 that $S$ believes of each ticket that it will lose. Since her credences are rational, these beliefs are rational. By repeated applications of 2 then, the agent can rationally believe that each ticket will lose. But she rationally gives credence 0 to the proposition that each ticket will lose. So by 1 she can rationally believe any proposition in which her credence is greater than 0. This is absurd.\footnote{See \cite{Sturgeon2008-STURAT} for discussion of a similar puzzle for anyone trying to tell a unified story of belief and credence.}

I won't repeat all the gory details here, but one of the consequences of the discussion in \cite{Weatherson2005-WEACWD} was that we could hold on to 3, and to restricted versions of 1 and 2. In particular, if we restricted 1 and 2 to relevant propositions (in some sense) they became true, although the unrestricted version is false. A key part of the argument of the earlier paper was that this was a better option than the more commonly taken option of holding on to unrestricted versions of 1 and 3, at the cost of abandoning 2 even in clear cases. But one might wonder why I'm holding so tightly on to 3. After all, there is a functionalist argument that 3 is false.

A key functional role of credences is that if an agent has credence $x$ in $p$ she should be prepared to buy a bet that returns 1 util if $p$, and 0 utils otherwise, iff the price is no greater than $p$ utils. A key functional role of belief is that if an agent believes $p$, and recognises that $\varphi$ is the best thing to do given $p$, then she'll do $\varphi$. Given $p$, it's worth paying any price up to 1 util for a bet that pays 1 util if $p$. So believing $p$ seems to mean being in a functional state that is like having credence 1 in $p$.

But this argument isn't quite right. If we spell out more carefully what the functional roles of credence and belief are, a loophole emerges in the argument that belief implies credence 1. The interest-relative theory of belief turns out to exploit that loophole. What's the difference, in functional terms, between having credence $x$ in $p$, and having credence $x + \varepsilon$ in $p$? Well, think again about the bet that pays 1 util if $p$, and 0 utils otherwise. And imagine that bet is offered for $x + \nicefrac{\varepsilon}{2}$ utils. The person whose credence is $x$ will decline the offer; the person whose credence is $x + \varepsilon$ will accept it. Now it will usually be that no such bet is on offer.\footnote{There are exceptions, especially in cases where $p$ concerns something significant to financial markets, and the agent trades financial products. If you work through the theory that I'm about to lay out, one consequence is that such agents should have very few unconditional beliefs about financially-sensitive information, just higher and lower credences. I think that's actually quite a nice outcome, but I'm not going to rely on that in the argument for the view.} No matter; as long as one agent is \textit{disposed} to accept the offer, and the other agent is not, that suffices for a difference in credence.

The upshot of that is that differences in credences might be, indeed usually will be, constituted by differences in dispositions concerning how to act in choice situations far removed from actuality. I'm not usually in a position of having to accept or decline a chance to buy a bet for 0.9932 utils that the local coffee shop is currently open. Yet whether I would accept or decline such a bet matters to whether my credence that the coffee shop is open is 0.9931 or 0.9933. This isn't a problem with the standard picture of how credences work. It's just an observation that the high level of detail embedded in the picture relies on taking the constituents of mental states to involve many dispositions.

One of the crucial features of the theory of belief I'm defending is that what an agent believes is in general \textit{insensitive} to such abtruse dispositions, although it is very sensitive to dispositions about practical matters. It's true that if I believe that $p$, and I'm rational enough, I'll act as if $p$ is true. Is it also true that if I believe $p$, I'm disposed to act as if $p$ is true no matter what choices are placed in front of me? The theory being defended here says no, and that seems plausible. As we say in the case of Barry and Beth, Barry can believe that $p$, but be disposed to \textit{lose that belief} rather than act on it if odd choices, like that presented by the genie, emerge.

%Quantifier domain variation
This suggests the key difference between belief and credence 1. For a rational agent, a credence of 1 in $p$ means that the agent is disposed to answer a wide range of questions the same way she would answer that question conditional on $p$. That follows from the fact that these four principles are trivial theorems of the orthodox theory of expected utility.\footnote{The presentation in this section, as in the earlier paper, assumes at least a weak form of consequentialism in the sense of \cite{Hammond1988}. This was arguably a weakness of the earlier paper. We'll return to the issue of what happens in cases where the agent doesn't, and perhaps shouldn't, maximise expected utility, at the end of the section.} 

\begin{description*}
\item[C1AP] For all $q, x$, if $\Pr(p) = 1$ then $\Pr(q) = x$ iff $\Pr(q | p) = x$.
\item[C1CP] For all $q, r$, if $\Pr(p) = 1$ then $\Pr(q) \geq \Pr(r)$ iff $\Pr(q | p) \geq \Pr(r | p)$.
\item[C1AU] For all $\varphi, x$, if $\Pr(p) = 1$ then $U(\varphi) = x$ iff $U(\varphi | p) = x$.
\item[C1CP] For all $\varphi, \psi$, if $\Pr(p) = 1$ then $U(\varphi) \geq U(\psi)$ iff $U(\varphi | p) \geq U(\psi | p)$.
\end{description*}

\noindent Those look a lot like the theorems of the theory of belief that we discussed above. But note that these claims are \textit{unrestricted}, whereas in the theory of belief, we restricted attention to relevant actions, propositions, utilities and probabilities. That turns out to be the difference between belief and credence 1. Since that difference is interest-relative, belief is interest-relative.

I used to think that that was all the interest-relativity we needed in epistemology. Now I don't, for reasons that I'll go through in section three. (Readers who care more about the theory of knowledge than the theory of belief may want to skip ahead to that section.) But first I want to clean up some loose ends in the acount of belief.

\subsection{Two Caveats}
The theory sketched so far seems to me right in the vast majority of cases. It fits in well with a broadly functionalist view of the mind, and it handles difficult cases, like that of Charlie, nicely. But it needs to be supplemented and clarified a little to handle some other difficult cases. In this section I'm going to supplement the theory a little to handle what I call `impractical propositions', and say a little about morally loaded action.

Jones has a false geographic belief: he believes that Los Angeles is west of Reno, Nevada.\footnote{I'm borrowing this example from Fred Dretske, who uses it to make some interesting points about dispositional belief.} This isn't because he's ever thought about the question. Rather, he's just disposed to say ``Of course'' if someone asks, ``Is Los Angeles west of Reno?'' That disposition has never been triggered, because no one's ever bothered to ask him this. Call the proposition that Los Angeles is west of Reno $p$. 

The theory given so far will get the right result here: Jones does believe that $p$. But it gets the right answer for an odd reason. Jones, it turns out, has very little interest in American geography right now. He's a schoolboy in St Andrews, Scotland, getting ready for school and worried about missing his schoolbus. There's no inquiry he's currently engaged in for which $p$ is even close to relevant. So conditionalising on $p$ doesn't change the answer to any inquiry he's engaged in, but that would be true no matter what his credence in $p$ is.

There's an immediate problem here. Jones believes $p$, since conditionalising on $p$ doesn't change the answer to any relevant inquiry. But for the very same reason, conditionalising on $\neg p$ doesn't change the answer to any relevant inquiry. It seems our theory has the bizarre result that Jones believes $\neg p$ as well. That is both wrong and unfair. We end up attributing inconsistent beliefs to Jones simply because he's a harried schoolboy who isn't currently concerned with the finer points of geography of the American southwest.

Here's a way out of this problem in four relatively easy steps.\footnote{The recipe here is similar to that given in \cite{Weatherson2005-WEACWD}, but the motivation is streamlined. Thanks to Jacob Ross for helpful suggestions here.} First, we say that which questions are relevant questions is not just relative to the agent's interests, but also relevant to the proposition being considered. A question may be relevant relative to $p$, but not relative to $q$. Second, we say that relative to $p$, the question of whether $p$ is more probable than $\neg p$ is a relevant question. Third, we infer from that that an agent only believes $p$ if their credence in $p$ is greater than their credence in $\neg p$, i.e., if their credence in $p$ is greater than $\nicefrac{1}{2}$. Finally, we say that when the issue is whether the subject believes that $p$, the question of whether $p$ is more probable than $\neg p$ is not only relevant on its own, but it stays being a relevant question conditional on any $q$ that is relevant to the subject. In the earlier paper \citep{Weatherson2005-WEACWD} I argue that this solves the problem raised by impractical propositions in a smooth and principled way.

That's the first caveat. The second is one that isn't discussed in the earlier paper. If the agent is merely trying to get the best outcome for themselves, then it makes sense to represent them as a utility maximiser. And within orthodox decision theory, it is easy enough to talk about, and reason about, conditional utilities. That's important, because conditional utilities play an important role in the theory of belief offered at the start of this section. But if the agent faces moral constraints on her decision, it isn't always so easy to think about conditional utilities.

When agents have to make decisions that might involve them causing harm to others if certain propositions turn out to be true, then I think it is best to supplement orthodox decision theory with an extra assumption. The assumption is, roughly, that for choices that may harm others, expected value is absolute value. It's easiest to see what this means using a simple case of three-way choice. The kind of example I'm considering here has been used for (slightly) different purposes by Frank \cite{Jackson1991}. 

The agent has to do $\varphi$ or $\psi$. Failure to do either of these will lead to disaster, and is clearly unacceptable. Either $\varphi$ or $\psi$ will avert the disaster, but one of them will be moderately harmful and the other one will not. The agent has time before the disaster to find out which of $\varphi$ and $\psi$ is harmful and which is not for a nominal cost. Right now, her credence that $\varphi$ is the harmful one is, quite reasonably, $\nicefrac{1}{2}$. So the agent has three choices:

\begin{itemize*}
\item Do $\varphi$;
\item Do $\psi$; or
\item Wait and find out which one is not harmful, and do it.
\end{itemize*}

\noindent We'll assume that other choices, like letting the disaster happen, or finding out which one is harmful and doing it, are simply out of consideration. In any case, they are clearly dominated options, so the agent shouldn't do them. Let $p$ be the propostion that $\varphi$ is the harmful one. Then if we assume the harm in question has a disutility of 10, and the disutility of waiting to act until we know which is the harmful one is 1, the values of the possible outcomes are as follows:

\begin{center}
\begin{tabular}{r c c}
 & $p$ & $\neg p$ \\
\textbf{Do $\varphi$} & -10 & 0 \\
\textbf{Do $\psi$} & 0 & -10 \\
Find out which is harmful & -1 & -1 \\
\end{tabular}
\end{center}

\noindent Given that $Pr(p) = \nicefrac{1}{2}$, it's easy to compute that the expected value of doing either $\varphi$ or $\psi$ is -5, while the expected value of finding out which is harmful is -1, so the agent should find out which thing is to be done before acting. So far most consequentialists would agree, and so probably would most non-consequentialists for most ways of fleshing out the abstract example I've described.\footnote{Some consequentialists say that what the agent should do depends on whether $p$ is true. If $p$ is true, she should do $\psi$, and if $p$ is false she should do $\varphi$. As we'll see, I have reasons for thinking this is rather radically wrong.}

But most consequentialists would also say something else about the example that I think is not exactly true. Just focus on the column in the table above where $p$ is true. In that column, the highest value, 0, is alongside the action \textit{Do} $\psi$. So you might think that conditional on $p$, the agent should do $\psi$. That is, you might think the conditional expected value of doing $\psi$, conditional on $p$ being true, is 0, and that's higher than the conditional expected value of any other act, conditional on $p$. If you thought that, you'd certainly be in agreement with the orthodox decision-theoretic treatment of this problem.

In the abstract statement of the situation above, I said that one of the options would be \textit{harmful}, but I didn't say who it would be harmful to. I think this matters. I think what I called the orthodox treatment of the situation is correct when the harm accrues to the person making the decision. But when the harm accrues to another person, particularly when it accrues to a person that the agent has a duty of care towards, then I think the orthodox treatment isn't quite right.

My reasons for this go back to Jackson's original discussion of the puzzle. Let the agent be a doctor, the actions $\varphi$ and $\psi$ be her prescribing different medications to a patient, and the harm a severe allergic reaction that the patient will have to one of the medications. Assume that she can run a test that will tell her which medication the patient is allergic to, but the test will take a day. Assume that the patient will die in a month without either medication; that's the disaster that must be averted. And assume that the patient is is some discomfort that either medication would relieve; that's the small cost of finding out which medication is the risk. Assume finally that there is no chance the patient will die in the day it takes to run the test, so the cost of running the test really is nominal.

A good doctor in that situation will find out which medication the patient is allergic to before prescribing either medicine. It would be \textit{reckless} to prescribe a medicine that is unnecessary and that the patient might be allergic to. It is worse than reckless if the patient is actually allergic to the medicine prescribed, and the doctor harms the patient. But even if she's lucky and prescribes the `right' medication, the recklessness remains. It was still, it seems, the wrong thing for her to do.

All of that is in Jackson's discussion of the case, though I'm not sure he'd agree with the way I'm about to incorporate these ideas into the formal decision theory. Even under the assumption that $p$, prescribing $\psi$ is still wrong, because it is reckless. That should be incorporated into the values we ascribe to different actions in different circumstances. The way I do it is to associate the value of each action, in each circumstance, with its actual expected value. So the decision table for the doctor's decision looks something like this.

\begin{center}
\begin{tabular}{r c c}
 & $p$ & $\neg p$ \\
\textbf{Do $\varphi$} & -5 & -5 \\
\textbf{Do $\psi$} & -5 & -5 \\
Find out which is harmful & -1 & -1 \\
\end{tabular}
\end{center}

\noindent In fact, the doctor is making a decision under certainty. She knows that the value of prescribing either medicine is -5, and the value of running the tests is -1, so she should run the tests.

In general, when an agent has a duty to maximise the expected value of some quantity $Q$, then the value that goes into the agent's decision table in a cell is \textit{not} the value of $Q$ in the world-action pair the agent represents. Rather, it's the expected value of $Q$ given that world-action pair. In situations like this one where the relevant facts (e.g., which medicine the patient is allergic to) don't affect the evidence the agent has, the decision is a decision under \textit{certainty}. This is all as things should be. When you have obligations that are drawn in terms of the expected value of a variable, the actual values of that variable cease to be directly relevant to the decision problem.

Similar morals carry across to theories that offer a smaller role to expected utility in determining moral value. In particular, it's often true that decisions where it is uncertain what course of action will produce the best outcome might still, in the morally salient sense, be decisions under certainty. That's because the uncertainty doesn't impact how we should weight the different possible outcomes, as in orthodox utility theory, but how we should value them. That's roughly what I think is going on in cases like this one, which Jessica Brown has argued are problematic for the epistemological theories John Hawthorne and Jason Stanley have recently been defending.\footnote{The target here is not directly the interest-relativity of their theories, but more general principles about the role of knowledge in action and assertion. Since my theories are close enough, at least in consequences, to Hawthorne and Stanley's, it is important to note how my theory handles the case.}

\begin{quote}
A student is spending the day shadowing a surgeon. In the morning he observes her in clinic examining patient A who has a diseased left kidney. The decision is taken to remove it that afternoon. Later, the student observes the surgeon in theatre where patient A is lying anaesthetised on the operating table. The operation hasn't started as the surgeon is consulting the patient's notes. The student is puzzled and asks one of the nurses what's going on: 

\textbf{Student}: I don't understand. Why is she looking at the patient's records? She was in clinic with the patient this morning. Doesn't she even know which kidney it is? 

\textbf{Nurse}: Of course, she knows which kidney it is. But, imagine what it would be like if she removed the wrong kidney. She shouldn't operate before checking the patient's records. \citep[1144-1145]{Brown2008-BROKAP}
\end{quote}

\noindent It is tempting, but for reasons I've been going through here mistaken, to represent the surgeon's choice as follows. Let \textbf{Left} mean the left kidney is diseased, and \textbf{Right} mean the right kidney is diseased.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{Left} & \textbf{Right} \\
\textbf{Remove left kidney} & $1$ & $-1$ \\
\textbf{Remove right kidney} & $-1$ & $1$ \\
\textbf{Check notes} & $1-\varepsilon$ & $1-\varepsilon$ \\
\end{tabular}
\end{center}

\noindent Here $\varepsilon$ is the trivial but non-zero cost of checking the chart. Given this table, we might reason that since the surgeon knows that she's in the left column, and removing the left kidney is the best option in that column, she should remove the left kidney rather than checking the notes.

But that reasoning assumes that the surgeon does not have any epistemic obligations over and above her duty to maximise expected utility. And that's very implausible. It's totally implausible on a non-consequentialist moral theory. A non-consequentialist may think that some people have just the same obligations that the consequentialist says they have -- legislators are frequently mentioned as an example -- but surely they wouldn't think \textit{surgeons} are in this category. And even a consequentialist who thinks that surgeons have special obligations in terms of their institutional role should think that the surgeon's obligations go above and beyond the obligation every agent has to maximise expected utility.

It's not clear exactly what the obligation the surgeon has. Perhaps it is an obligation to not just know which kidney to remove, but to know this on the basis of evidence she has obtained while in the operating theatre. Or perhaps it is an obligation to make her belief about which kidney to remove as sensitive as possible to various possible scenarios. Before she checked the chart, this counterfactual was false: \textit{Had she misremembered which kidney was to be removed, she would have a true belief about which kidney was to be removed.} Checking the chart makes that counterfactual true, and so makes her belief that the left kidney is to be removed a little more sensitive to counterfactual possibilities. 

However we spell out the obligation, it is plausible given what the nurse says that the surgeon has some such obligation. And it is plausible that the `cost' of violating this obligation, call it $\delta$ is greater than the cost of checking the notes. So here is the decision table the surgeon faces.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{Left} & \textbf{Right} \\
\textbf{Remove left kidney} & $1-\delta$ & $-1-\delta$ \\
\textbf{Remove right kidney} & $-1-\delta$ & $1-\delta$ \\
\textbf{Check notes} & $1-\varepsilon$ & $1-\varepsilon$ \\
\end{tabular}
\end{center}

\noindent And it isn't surprising, or a problem for an interest-relative theory of knowledge or belief, that the surgeon should check the notes, even if she believes \textit{and knows} that the left kidney is the diseased one.
 
\section{Playing Games with a Lockean}

I'm going to raise problems for Lockeans, and for defenders of regularity in general, by discussing a simple game. The game itself is a nice illustration of how a number of distinct solution concepts in game theory come apart. (Indeed, the use I'll make of it isn't a million miles from the use that  \cite{KohlbergMertens1986} make of it.) To set the problem up, I need to say a few words about how I think of game theory. This won't be at all original - most of what I say is taken from important works by Robert \cite{Stalnaker1994, Stalnaker1996, Stalnaker1998, Stalnaker1999}. But it is different to what I used to think, and perhaps to what some other people think too, so I'll set it out slowly.\footnote{I'm grateful to the participants in a game theory seminar at Arch\'e in 2011, especially Josh Dever and Levi Spectre, for very helpful discussions that helped me see through my previous confusions.}

Start with a simple decision problem, where the agent has a choice between two acts $A_1$ and $A_2$, and there are two possible states of the world, $S_1$ and $S_2$, and the agent knows the payouts for each act-state pair are given by the following able.

\begin{center}
\begin{tabular}{r c c}
 & $S_1$ & $S_2$ \\
$A_1$ & 4 & 0 \\
$A_2$ & 1 & 1 
\end{tabular}
\end{center}

\noindent What to do? I hope you share the intuition that it is radically underdetermined by the information I've given you so far. If $S_2$ is much more probable than $S_1$, then $A_2$ should be chosen; otherwise $A_1$ should be chosen. But I haven't said anything about the relative probability of those two states. Now compare that to a simple game. Row has two choices, which I'll call $A_1$ and $A_2$. Column also has two choices, which I'll call $S_1$ and $S_2$. It is common knowledge that each player is rational, and that the payouts for the pairs of choices are given in the following table. (As always, Row's payouts are given first.)

\begin{center}
\begin{tabular}{r c c}
 & $S_1$ & $S_2$ \\
$A_1$ & 4, 0 & 0, 1 \\
$A_2$ & 1, 0 & 1, 1 
\end{tabular}
\end{center}

\noindent What should Row do? This one is easy. Column gets 1 for sure if she plays $S_2$, and 0 for sure if she plays $S_1$. So she'll play $S_2$. And given that she's playing $S_2$, it is best for Row to play $A_2$.

You probably noticed that the game is just a version of the decision problem that we discussed a couple of paragraphs ago. The relevant states of the world are choices of Column. But that's fine; we didn't say in setting out the decision problem what constituted the states $S_1$ and $S_2$. And note that we solved the problem without explicitly saying anything about probabilities. What we added was some information about Column's payouts, and the fact that Column is rational. From there we deduced something about Column's play, namely that she would play $S_2$. And from that we concluded what Row should do.

There's something quite general about this example. What's distinctive about game theory isn't that it involves any special kinds of decision making. Once we get the probabilities of each move by the other player, what's left is (mostly) expected utility maximisation. (We'll come back to whether the `mostly' qualification is needed below.) The distinctive thing about game theory is that the probabilities aren't specified in the setup of the game; rather, they are solved for. Apart from special cases, such as where one option strictly dominates another, we can't say much about a decision problem with unspecified probabilities. But we can and do say a lot about games where the setup of the game doesn't specify the probabilities, because we can solve for them given the other information we have.

This way of thinking about games makes the description of game theory as `interactive epistemology' \citep{Aumann1999} rather apt. The theorist's work is to solve for what a rational agent should think other rational agents in the game should do. From this perspective, it isn't surprising that game theory will make heavy use of equilibrium concepts. In solving a game, we must deploy a theory of rationality, and attribute that theory to rational actors in the game itself. In effect, we are treating rationality as something of an unknown, but one that occurs in every equation we have to work with. Not surprisingly, there are going to be multiple solutions to the puzzles we face.

This way of thinking lends itself to an epistemological interpretation of one of the most puzzling concepts in game theory, the mixed strategy. Arguably the core solution concept in game theory is the Nash equilibrium. As you probably know, a set of moves is a Nash equilibrium if no player can improve their outcome by deviating from the equilibrium, conditional on no other player deviating. In many simple games, the only Nash equilibria involve mixed strategies. Here's one simple example.

\begin{center}
\begin{tabular}{r c c}
 & $S_1$ & $S_2$ \\
$A_1$ & 0, 1 & 10, 0 \\
$A_2$ & 9, 0 & -1, 1 
\end{tabular}
\end{center}

\noindent This game is reminiscent of some puzzles that have been much discussed in the decision theory literature, namely asymmetric Death in Damascus puzzles. Here Column wants herself and Row to make the `same' choice, i.e., $A_1$ and $S_1$ or $A_2$ and $S_2$. She gets 1 if they do, 0 otherwise. And Row wants them to make different choices, and gets 10 if they do. Row also dislikes playing $A_2$, and this costs her 1 whatever else happens. It isn't too hard to prove that the only Nash equilibrium for this game is that Row plays a mixed strategy playing both $A_1$ and $A_2$ with probability \nicefrac{1}{2}, while Column plays the mixed strategy that gives $S_1$ probability \nicefrac{11}{20}, and $S_2$ with probability \nicefrac{9}{20}.

Now what is a mixed strategy? It is easy enough to take away form the standard game theory textbooks a \textbf{metaphysical} interpretation of what a mixed strategy is. Here, for instance, is the paragraph introducing mixed strategies in Dixit and Skeath's \textit{Games of Strategy}.

\begin{quote}
When players choose to act unsystematically, they pick from among their pure strategies in some random way \dots We call a random mixture between these two pure strategies a mixed strategy. \citep[186]{DixitSkeath2004}
\end{quote}
Dixit and Skeath are saying that it is definitive of a mixed strategy that players use some kind of randomisation device to pick their plays on any particular run of a game. That is, the probabilities in a mixed strategy must be in the world; they must go into the players' choice of play. That's one way, the paradigm way really, that we can think of mixed strategies metaphysically.

But the understanding of game theory as interactive epistemology naturally suggests an \textbf{epistemological} interpretation of mixed strategies. %Insert quotes from Stalnaker1994

\begin{quote}One could easily \dots [model players] \dots turning the choice over to a randomizing device, but while it might be harmless to permit this, players satisfying the cognitive idealizations that game theory and decision theory make could have no motive for playing a mixed strategy. So how are we to understand Nash equilibrium in model theoretic terms as a solution concept? We should follow the suggestion of Bayesian game theorists, interpreting mixed strategy profiles as representations, not of players' choices, but of their beliefs. \citep[57-8]{Stalnaker1994}
\end{quote}
One nice advantage of the epistemological interpretation, as noted by Binmore \citeyearpar[185]{Binmore2007} %Include citation
is that we don't require players to have $n$-sided dice in their satchels, for every $n$, every time they play a game.\footnote{Actually, I guess it is worse than if some games have the only equilibria involving mixed strategies with irrational probabilities. And it might be noted that Binmore's introduction of mixed strategies, on page 44 of his \citeyearpar{Binmore2007}, sounds much more like the metaphysical interpretation. But I think the later discussion is meant to indicate that this is just a heuristic introduction; the epistemological interpretation is the correct one.} But another advantage is that it lets us make sense of the difference between playing a pure strategy and playing a mixed strategy where one of the `parts' of the mixture is played with probability one. 

With that in mind, consider the below game, which I'll call \RG. I've said something different about this game in earlier work \citep{Weatherson2012-WEAGAT}. But I now think that to understand what's going on, we need to think about mixed strategies where one element of the mixture has probability one.

Informally, in this game $A$ and $B$ must each play either a green or red card. I will capitalise $A$'s moves, i.e., $A$ can play GREEN or RED, and italicise $B$'s moves, i.e., $B$ can play \textit{green} or \textit{red}. If two green cards, or one green card and one red card are played, each player gets \$1. If two red cards are played, each gets nothing. Each cares just about their own wealth, so getting \$1 is worth 1 util. All of this is common knowledge. More formally, here is the game table, with $A$ on the row and $B$ on the column.

\begin{center}
\begin{tabular}{r c c}
 & \textit{green} & \textit{red} \\
GREEN & 1, 1 & 1, 1 \\
RED & 1, 1 & 0, 0
\end{tabular}
\end{center}
When I write game tables like this, and I think this is the usual way game tables are to be interpreted \citep{Weatherson2012-WEAKBI}, I mean that the players know that these are the payouts, that the players know the other players to be rational, and these pieces of knowledge are common knowledge to at least as many iterations as needed to solve the game. With that in mind, let's think about how the agents should approach this game.

I'm going to make one big simplifying assumption at first. We'll relax this later, but it will help the discussion a lot I think to start with this assumption. This assumption is that the doctrine of \textbf{Uniqueness} applies here; there is precisely one rational credence to have in any salient proposition about how the game will play. Some philosophers think that Uniqueness always holds \citep{White2005-WHIEP}. I don't, but it does seem like it might \textit{often} hold. Anyway, I'm going to start by assuming that it does hold here.

The first thing to note about the game is that it is symmetric. So the probability of $A$ playing GREEN should be the same as the probability of $B$ playing \textit{green}, since $A$ and $B$ face exactly the same problem. Call this common probability $x$. If $x < 1$, we get a quick contradiction. The expected value, to Row, of GREEN, is 1. Indeed, the known value of GREEN is 1. If the probability of \textit{green} is $x$, then the expected value of RED is $x$. So if $x < 1$, and Row is rational, she'll definitely play GREEN. But that's inconsistent with the claim that $x < 1$, since that means that it isn't definite that Row will play GREEN.

So we can conclude that $x = 1$. Does that mean we can know that Row will play GREEN? No. Assume we could conclude that. Whatever reason we would have for concluding that would be a reason for any rational person to conclude that Column will play \textit{green}. Since any rational person can conclude this, Row can conclude it. So Row knows that she'll get 1 whether she plays GREEN or RED. But then she should be indifferent between playing GREEN and RED. And if we know she's indifferent between playing GREEN and RED, and our only evidence for what she'll play is that she's a rational player who'll maximise her returns, then we can't be in a position to know she'll play GREEN.

I think the arguments of the last two paragraphs are sound. We'll turn to an objection presently, but let's note how bizarre is the conclusion we've reached. One argument has shown that it could not be more probable that Row will play GREEN. A second argument has shown that we can't know that Row will play GREEN. It reminds me of examples involving blindspots \citep{Sorensen1988}. Consider this case:

\begin{enumerate*}
\renewcommand{\labelenumi}{(\Alph{enumi})}
\setcounter{enumi}{1}
\item Brian does not know (B).
\end{enumerate*}
That's true, right? Assume it's false, so I do know (B). Knowledge is factive, so (B) is true. But that contradicts the assumption that it's false. So it's true. But I obviously don't know that it's true; that's what this very true proposition says.

Now I'm not going to rest anything on this case, because there are so many tricky things one can say about blindspots, and about the paradoxes generally. It does suggest that there are other finite cases where one can properly have maximal credence in a true proposition without knowledge.\footnote{As an aside, the existence of these cases is why I get so irritated when epistemologists try to theorise about `Gettier Cases' as a class. What does (B) have in common with inferences from a justified false belief, or with otherwise sound reasoning that is ever so close to issuing in a false conclusion due to relatively bad luck? As far as I can tell, the class of justified true beliefs that aren't knowledge is a disjunctive mess, and this should matter for thinking about the nature of knowledge. For further examples, see \cite{WilliamsonLofoten}.} And, assuming that we shouldn't believe things we know we don't know, that means we can have maximal credence in things we don't believe. All I want to point out is that this phenomena of maximal credence without knowledge, and presumably without full belief, isn't a quirky feature of self-reference, or of games, or of puzzles about infinity; it comes up in a wide range of cases.

For the rest of this section I want to reply to one objection, and weaken an assumption I made earlier. The objection is that I'm wrong to assume that agents will only maximise expected utility. They may have tie-breaker rules, and those rules might undermine the arguments I gave above. The assumption is that there's a uniquely rational credence to have in any given situation.

I argued that if we knew that $A$ would play GREEN, we could show that $A$ had no reason to play GREEN. But actually what we showed was that the expected utility of playing GREEN would be the same as playing RED. Perhaps $A$ has a reason to play GREEN, namely that GREEN weakly dominates RED. After all, there's one possibility on the table where GREEN does better than RED, and none where RED does better. And perhaps that's a reason, even if it isn't a reason that expected utility considerations are sensitive to.

Now I don't want to insist on expected utility maximisation as the only rule for rational decision making. Sometimes, I think some kind of tie-breaker procedure is part of rationality. In the papers by Stalnaker I mentioned above, he often appeals to this kind of weak dominance reasoning to resolve various hard cases. But I don't think weak dominance provides a reason to play GREEN in this particular case. When Stalnaker says that agents should use weak dominance reasoning, it is always in the context of games where the agents' attitude towards the game matrix is different to their attitude towards each other. One case that Stalnaker discusses in detail is where the game table is common knowledge, but there is merely common (justified, true) belief in common rationality. Given such a difference in attitudes, it does seem there's a good sense in which the most salient departure from equilibrium will be one in which the players end up somewhere else on the table. And given that, weak dominance reasoning seems appropriate.

But that's not what we've got here. Assuming that rationality requires playing GREEN/\textit{green}, the players know we'll end up in the top left corner of the table. There's no chance that we'll end up elsewhere. Or, perhaps better, there is just as much chance we'll end up `off the table', as that we'll end up in a non-equilibrium point on the table. To make this more vivid, consider the `possibility' that $B$ will play \textit{blue}, and if $B$ plays \textit{blue}, $A$ will receive 2 if she plays RED, and -1 if she plays GREEN. Well hold on, you might think, didn't I say that \textit{green} and \textit{red} were the only options, and this was common knowledge? Well, yes, I did, but if the exercise is to consider what would happen if something the agent knows to be true doesn't obtain, then the possibility that one agent will play blue certainly seems like one worth considering. It is, after all, a metaphysical possibility. And if we take it seriously, then it isn't true that under \textit{any} possible play of the game, GREEN does better than RED.

We can put this as a dilemma. Assume, for \textit{reductio}, that GREEN/\textit{green} is the only rational play. Then if we restrict our attention to possibilities that are epistemically open to $A$, then GREEN does just as well as RED; they both get 1 in every possibility. If we allow possibilities that are epistemically closed to $A$, then the possibility where $B$ plays \textit{blue} is just as relevant as the possibility that $B$ is irrational. After all, we stipulated that this is a case where rationality is common knowledge. In neither case does the weak dominance reasoning get any purchase.

With that in mind, we can see why we don't need the assumption of Uniqueness. Let's play through how a failure of Uniqueness could undermine the argument. Assume, again for \textbf{reductio}, that we have credence $\varepsilon > 0$ that $A$ will play RED. Since $A$ maximises expected utility, that means $A$ must have credence 1 that $B$ will play \textit{green}. But this is already odd. Even if you think people can have different reactions to the same evidence, it is odd to think that one rational agent could regard a possibility as infinitely less likely than another, given isomorphic evidence. And that's not all of the problems. Even if $A$ has credence 1 that $B$ will play \textit{green}, it isn't obvious that playing RED is rational. After all, relative to the space of epistemic possibilities, GREEN weakly dominates RED. Remember that we're no longer assuming that it can be known what $A$ or $B$ will play. So even without Uniqueness, there are two reasons to think that it is wrong to have credence $\varepsilon > 0$ that $A$ will play RED. So we've still shown that credence 1 doesn't imply knowledge, and since the proof is known to us, and full belief is incompatible with knowing that you can't know, this is a case where credence 1 doesn't imply full belief. So whether $A$ plays GREEN, like whether the coin will ever land tails, is a case the Lockean cannot get right, no matter where they set the threshold for belief; our credence is above the threshold, but we don't believe. 

So I think this case is a real problem for a Lockean view about the relationship between credence and belief. If \textit{A} is rational, she can have credence 1 that \textit{B} will play \textit{green}, but won't believe that \textit{B} will play \textit{green}. But now you might worry that my own account of the relationship between belief and credence is in just as much trouble. After all, I said that to believe $p$ is, roughly, to have the same attitudes towards all salient questions as you have conditional on $p$. And it's hard to identify a question that rational \textit{A} would answer differently upon conditionalising on the proposition that \textit{B} plays \textit{green}.

I think what went wrong in my earlier view was that I'd too quickly equated updating with conditionalisation. The two can come apart. Here's an example from \cite{Gillies2010} that makes the point well. 

\begin{quote}I have lost my marbles. I know that just one of them -- Red or Yellow -- is in the box. But I don't know which. I find myself saying things like \dots ``If Yellow isn't in the box, the Red must be.'' (4:13)
\end{quote}
As Gillies goes on to point out, this isn't really a problem for the Ramsey test view of conditionals.
\begin{quote}
The Ramsey test -- the schoolyard version, anyway -- is a test for when an indicative conditional is acceptable given your beliefs. It says that (if \textit{p})(\textit{q}) is acceptable in belief state \textit{B} iff \textit{q} is acceptable in the derived or subordinate state \textit{B}-plus-the-information-that-\textit{p}. (4:27)
\end{quote}
And he notes that this can explain what goes on with the marbles conditional. Add the information that Yellow isn't in the box, and it isn't just true, but must be true, that Red is in the box.

Note though that while we can explain this conditional using the Ramsey test, we can't explain it using any version of the idea that probabilities of conditionals are conditional probabilities. The probability that Red must be in the box is 0. The probability that Yellow isn't in the box is not 0. So conditional on Yellow not being in the box, the probability that Red must be in the box is still 0. Yet the conditional is perfectly assertable.

There is, and this is Gillies's key point, something about the behaviour of modals in the consequents of conditionals that we can't capture using conditional probabilities, or indeed many other standard tools. And what goes for consequents of conditionals goes for updated beliefs too. Learn that Yellow isn't in the box, and you'll conclude that Red must be. But that learning can't go via conditionalisation; just conditionalise on the new information and the probability that Red must be in the box goes from 0 to 0.

Now it's a hard problem to say exactly how this alternative to updating by conditionalisation should work. But very roughly, the idea is that at least some of the time, we update by eliminating worlds from the space of possibilities. This affects dramatically the probability of propositions whose truth is sensitive to which worlds are in the space of possibiilties.

For example, in the game I've been discussing, we should believe that rational \textit{B} might play \textit{red}. Indeed, the probability of that is, I think, 1. And whether or not \textit{B} might play red is highly salient; it matters to the probability of whether \textbf{A} will play GREEN or RED. Conditionalising on something that has probability 1, such as that \textit{B} will play \textit{green}, can hardly change that probability. But updating on the proposition that \textit{B} will play \textit{green} can make a difference. We can see that by simply noting that the conditional \textit{If B plays green, she might play red} is incoherent.

So I conclude that a theory of belief like mine can handle the puzzle this game poses, as long as it distinguishes between conditionalising and updating, in just the way Gillies suggests. To believe that \textit{p} is to be disposed to not change any attitude towards a salient question on updating that \textit{p}. (Plus some bells and whistles to deal with propositions that are not relevant to salient questions. We'll return to them below.) Updating often goes by conditionalisation, so we can often say that belief means having attitudes that match unconditionally and conditionally on \textit{p}. But not all updating works that way, and the theory of belief needs to acknowledge this.

\section{The Power of Theoretical Interests}
So I think we should accept that credences exist. And we can just about reduce beliefs to credences. In previous work I argued that we could do such a reduction. I'm not altogether sure whether the amendments to that view I'm proposing here means it no longer should count as a reductive view; we'll come back to that question in the conclusion.

The view I defended in previous work is that the reduction comes through the relationship between conditional and unconditional attitudes. Very roughly, to believe that \emph{p} is simply to have the same attitudes, towards all salient questions, unconditionally as you have conditional on \emph{p}. In a syrupy slogan, belief means never having to say you've conditionalised. For reasons I mentioned in section 1, I now think that was inaccurate; I should have said that belief means never having to say you've updated, or at least that you've updated your view on any salient question.

The restriction to salient questions is important. Consider any \emph{p} that I normally take for granted, but such that I wouldn't bet on it at insane odds. I prefer declining such a bet to taking it. But conditional on \emph{p}, I prefer taking the bet. So that means I don't believe any such \emph{p}. But just about any \emph{p} satisfies that description, for at least some `insane' odds. So I believe almost nothing. That would be a \emph{reductio} of the position. I respond by saying that the choice of whether to take an insane bet is not normally salient.

But now there's a worry that I've let in too much. For many \emph{p}, there is no salient decision that they even bear on. What I would do conditional on \emph{p}, conditional on $\neg p$, and unconditionally is exactly the same, over the space of salient choices. (And this isn't a case where updating and conditionalising come apart; I'll leave this proviso mostly implicit from now on.) So with the restriction in place, I believe \emph{p} and $\neg p$. That seems like a \emph{reductio} of the view too. I probably do have inconsistent beliefs, but not in virtue of \emph{p} being irrelevant to me right now. I've changed my mind a little about what the right way to avoid this problem is, in part because of some arguments by Jacob Ross and Mark Schroeder. 

They have what looks like, on the surface, a rather different view to mine. They say that to believe \emph{p} is to have a \textbf{default reasoning disposition} to use \emph{p} in reasoning. Here's how they describe their own view.

\begin{quote}

What we should expect, therefore, is that for some propositions we would have a \emph{defeasible} or \emph{default} disposition to treat them as true in our reasoning--a disposition that can be overridden under circumstances where the cost of mistakenly acting as if these propositions are true is particularly salient. And this expectation is confirmed by our experience. We do indeed seem to treat some uncertain propositions as true in our reasoning; we do indeed seem to treat them as true automatically, without first weighing the costs and benefits of so treating them; and yet in contexts such as High where the costs of mistakenly treating them as true is salient, our natural tendency to treat these propositions as true often seems to be overridden, and instead we treat them as merely probable.

But if we concede that we have such defeasible dispositions to treat particular propositions as true in our reasoning, then a hypothesis naturally arises, namely, that beliefs consist in or involve such dispositions. More precisely, at least part of the functional role of belief is that believing that \emph{p} defeasibly disposes the believer to treat \emph{p} as true in her reasoning. Let us call this hypothesis the \emph{reasoning disposition account} of belief.
\end{quote}

\noindent There are, relative to what I'm interested in, three striking characteristics of Ross and Schroeder's view.

\begin{enumerate*}
\item Whether you believe \emph{p} is sensitive to how you reason; that is, your theoretical interests matter.

\item How you would reason about some questions that are not live is relevant to whether you believe \emph{p}.

\item Dispositions can be masked, so you can believe \emph{p} even though you don't actually use \emph{p} in reasoning now.

\end{enumerate*}

I think they take all three of these points to be reasons to favour their view over mine. As I see it, we agree on point 1 (and I always had the resources to agree with them), I can accommodate point 2 with a modification to my theory, and point 3 is a cost of their theory, not a benefit. Let's take those points in order.

There are lots of reasons to dislike what Ross and Schroeder call \emph{Pragmatic Credal Reductionism} (PCR). This is, more or less, the view that the salient questions, in the sense relevant above, are just those which are practically relevant to the agent. So to believe $p$ just is to have the same attitude towards all practically relevant questions unconditionally as conditional on $p$. There are at least three reasons to resist this view.

One reason comes from the discussions of Ned Block's example Blockhead ~\citep{Block1978}. As Braddon-Mitchell and Jackson point out, the lesson to take from that example is that beliefs are constituted in part by their relations to other mental states ~\citep[114ff]{DBMJackson2007}. There's a quick attempted refutation of PCR via the Blockhead case which doesn't quite work. We might worry that if all that matters to belief given PCR is how it relates to action, PCR will have the implausible consequence that Blockhead has a rich set of beliefs. That isn't right; PCR is compatible with the view that Blockhead doesn't have credences, and hence doesn't have credences that constitute beliefs. But the Blockhead examples value isn't exhausted by its use in quick refutations.\footnote{The point I'm making here is relevant I think to recent debates about the proper way to formalise counterexamples in philosophy, as in ~\citep{Williamson2007-WILTPO-17, IchikawaJarvis2009, Malmgren2011}. I worry that too much of that debate is focussed on the role that examples play in one-step refutations. But there's more, much more, to a good example than that.} The lesson is that beliefs are, by their nature, interactive. It seems to me that PCR doesn't really appreciate that lesson.

Another reason comes from recent work by Jessica \cite{Brown2013}. Compare these two situations.

\begin{enumerate*}
\item \emph{S} is in circumstances \emph{C}, and has to decide whether to do \emph{X}.

\item \emph{S} is in completely different circumstances to \emph{C}, but is seriously engaged in planning for future contingencies. She's currently trying to decide whether in circumstances \emph{C} to do \emph{X}.

\end{enumerate*}
Intuitively, \emph{S} can bring exactly the same evidence, knowledge and beliefs to bear on the two problems. If \emph{C} is a particularly high stakes situation, say it is a situation where one has to decide what to feed someone with a severe peanut allergy, then a lot of things that can ordinarily be taken for granted cannot, in this case, be taken for granted. And that's true whether \emph{S} is actually in \emph{C}, or she is just planning for the possibility that she finds herself in \emph{C}.

So I conclude that both practical and theoretical interests matter for what we can take for granted in inquiry. The things we can take for granted into a theoretical inquiry into what to do in high stakes contexts as restricted, just as they are when we are in a high stakes context, and must make a practical decision. Since the latter restriction on what we can take for granted is explained by (and possibly constituted by) a restriction on what we actually believe in those contexts, we should similarly conclude that agents simply believe less when they are reasoning about high stakes contexts, whatever their actual context.

A third reason to dislike PCR comes from the `Renzi' example in Ross and Schroeder's paper. I'll run through a somewhat more abstract version of the case, because I don't think the details are particularly important. Start with a standard decision problem. The agent knows that X is better to do if \emph{p}, and Y is better to do if $\neg p$. The agent should then go through calculating the relative gains to doing X or Y in the situations they are better, and the probability of \emph{p}. But the agent imagined doesn't do that. Rather, the agent divides the possibility space in four, taking the salient possibilities to be $p \wedge q, p \wedge \neg q, \neg p \wedge q$ and $\neg p \wedge \neg q$, and then calculates the expected utility of X and Y accordingly. This is a bad bit of reasoning on the agent's part. In the cases we are interested in, \emph{q} is exceedingly likely. Moreover, the expected utility of each act doesn't change a lot depending on \emph{q}'s truth value. So it is fairly obvious that we'll end up making the same decision whether we take the `small worlds' in our decision model to be just the world where \emph{p}, and the world where $\neg p$, or the four worlds this agent uses. But the agent does use these four, and the question is what to say about them.

Ross and Schroeder say that such an agent should not be counted as believing that \emph{q}. If they are consciously calculating the probability that \emph{q}, and taking $\neg q$ possibilities into account when calculating expected utilities, they regard \emph{q} as an open question. And regarding \emph{q} as open in this way is incompatible with believing it. I agree with all this.

They also think that PCR implies that the agent \emph{does} believe \emph{q}. The reason is that conditionalising on \emph{q} doesn't change the agent's beliefs about any practical question. I think that's right too, at least on a natural understanding of what `practical' is.

My response to all these worries is to say that whether someone believes that \emph{p} depends not just on how conditionalising (or more generally updating) on \emph{p} would affect someone's action, but on how it would affect their reasoning. That is, just as we learned from the Blockhead example, to believe that \emph{p} requires having a mental state that is connected to the rest of one's cognitive life in roughly the way a belief that \emph{p} should be connected. Let's go through both the last two cases to see how this works on my theory.

One of the things that happens when the stakes go up is that conditionalising on very probable things can change the outcome of interesting decisions. Make the probability that some nice food is peanut-free be high, but short of one. Conditional on it being peanut-free, it's a good thing to give to a peanut-allergic guest. But unconditionally, it's a bad thing to give to such a guest, because the niceness of the food doesn't outweigh the risk of killing them. And that's true whether the guest is actually there, or you're just thinking about what to do should such a guest arrive in the future. In general, the same questions will be relevant whether you're in \emph{C} trying to decide whether to do \emph{X}, or simply trying to decide whether to \emph{X} in \emph{C}. In one case they will be practically relevant questions, in the other they will be theoretically relevant questions. But this feels a lot like a distinction without a difference, since the agent should have similar beliefs in the two cases.

The same response works for Ross and Schroeder's case. The agent was trying to work out the expected utility of X and Y by working out the utility of each action in each of four `small worlds', then working out the probability of each of these. Conditional on \emph{q}, the probability of two of them ($p \wedge \neg q, \neg p \wedge \neg q$), will be 0. Unconditionally, this probability won't be 0. So the agent has a different view on some question they have taken an interest in unconditionally to their view conditional on \emph{q}. So they don't believe \emph{q}. The agent shouldn't care about that question, and conditional on each question they should care about, they have the same attitude unconditionally and conditional on \emph{q}. But they do care about these probabilistic questions, so they don't believe \emph{q}. (In ~\citep{Weatherson2005-WEACWD} I said that to justifiably believe \emph{q} was to have a justified credence in \emph{q} that was sufficiently high to count as a belief. The considerations of the last two sentences puts some pressure on that reductive theory of justification for beliefs.)

So I think that Ross and Schroeder and I agree on point 1; something beyond practical interests is relevant to belief.

They have another case that I think does suggest a needed revision to my theory. I'm going to modify their case a little to change the focus a little, and to avoid puzzles about vagueness. (What follows is a version of their example about Dal\'\i's moustache, purged of any worries about vagueness, and without the focus on consistency. I don't think the problem they true to press on me, that my theory allows excessive inconsistency of belief among rational agents, really sticks. Everyone will have to make qualifications to consistency to deal with the preface paradox, and for reasons I went over in ~\citep{Weatherson2005-WEACWD}, I think the qualifications I make are the best ones to make.)

Let \emph{D} be the proposition that the number of games the Detroit Tigers won in 1976 (in the MLB regular season) is not a multiple of 3. At most times, \emph{D} is completely irrelevant to anything I care about, either practically or theoretically. My attitudes towards any relevant question are the same unconditionally as conditional on \emph{D}. So there's a worry that I'll count as believing \emph{D}, and believing $\neg D$, by default.

In earlier work, I added a clause meant to help with cases like this. I said that for determining whether an agent believes that \emph{p}, we should treat the question of whether \emph{p}'s probability is above or below 0.5 as salient, even if the agent doesn't care about it. Obviously this won't help with this particular case. The probability of \emph{D} is around \nicefrac{2}{3}, and is certainly above 0.5. My `fix' avoids the consequence that I implausibly count as believing $\neg D$. But I still count, almost as implausibly, as believing \emph{D}. This needs to be fixed.

Here's my proposed change. For an agent to count as believing \emph{p}, it must be possible for \emph{p} to do some work for them in reasoning. Here's what I mean by work. Consider a very abstract set up of a decision problem, as follows.

\begin{center}
\begin{tabular}{rcc}
&\emph{p}&\emph{q}\\
%\midrule
X&4&1\\
Y&3&2\\
\end{tabular}
\end{center}

%\begin{table}[htbp]
%\begin{minipage}{\linewidth}
%\setlength{\tymax}{0.5\linewidth}
%\centering
%\small
%\begin{tabulary}{\textwidth}{@{}RCC@{}} \toprule
%&\emph{p}&\emph{q}\\
%\midrule
%X&4&1\\
%Y&3&2\\
%
%\bottomrule
%
%\end{tabulary}
%\end{minipage}
%\end{table}
%

That table encodes a lot of information. It encodes that $p \vee q$ is true; otherwise there are some columns missing. It encodes that the only live choices are X or Y; otherwise there are rows missing. It encodes that doing X is better than doing Y if \emph{p}, and worse if \emph{q}. 

For any agent, and any decision problem, there is a table like this that they would be disposed to use to resolve that problem. Or, perhaps, there are a series of tables and there is no fact about which of them they would be most disposed to use.

Given all that terminology, here's my extra constraint on belief. To believe that \emph{p}, there must be some decision problem such that some table the agent would be disposed to use to solve it encodes that \emph{p}. If there is no such problem, the agent does not believe that \emph{p}. For anything that I intuitively believe, this is an easy condition to satisfy. Let the problem be whether to take a bet that pays 1 if \emph{p}, and loses 1 otherwise. Here's the table I'd be disposed to use to solve the problem.

\begin{center}
\begin{tabular}{rc}
&\emph{p}\\
%\midrule
Take bet&1\\
Decline bet&0\\
\end{tabular}
\end{center}
%\begin{table}[htbp]
%\begin{minipage}{\linewidth}
%\setlength{\tymax}{0.5\linewidth}
%\centering
%\small
%\begin{tabulary}{\textwidth}{@{}RC@{}} \toprule
%&\emph{p}\\
%\midrule
%Take bet&1\\
%Decline bet&0\\
%
%\bottomrule
%
%\end{tabulary}
%\end{minipage}
%\end{table}
%
%
This table encodes that \emph{p}, so it is sufficient to count as believing that \emph{p}. And it doesn't matter that this bet isn't on the table. I'm disposed to use this table, so that's all that matters.

But might there be problems in the other direction. What about an agent who, if offered such a bet on \emph{D}, would use such a simple table? I simply say that they believe that \emph{D}. I would not use any such table. I'd use this table.

\begin{center}
\begin{tabular}{rcc}
&\emph{D}&$\neg D$\\
%\midrule
Take bet&1&--1\\
Decline bet&0&0\\
\end{tabular}
\end{center}
%\begin{table}[htbp]
%\begin{minipage}{\linewidth}
%\setlength{\tymax}{0.5\linewidth}
%\centering
%\small
%\begin{tabulary}{\textwidth}{@{}RCC@{}} \toprule
%&\emph{D}&$\neg D$\\
%\midrule
%Take bet&1&--1\\
%Decline bet&0&0\\
%
%\bottomrule
%
%\end{tabulary}
%\end{minipage}
%\end{table}
%
Now given the probability of \emph{D}, I'd still end up taking the bet; it has an expected return of \nicefrac{2}{3}. (Well, actually I'd probably decline the bet because being offered the bet would change the probability of \emph{D} for reasons made clear in ~\citep[14--15]{RunyonGuysDolls}. But that hardly undermines the point I'm making.) But this isn't some analytic fact about me, or even I think some respect in which I'm obeying the dictates of rationality. It's simply a fact that I wouldn't take \emph{D} for granted in any inquiry. And that's what my non-belief that \emph{D} consists in.

This way of responding to the Tigers example helps respond to a nice observation that Ross and Schroeder make about correctness. A belief that \emph{p} is, in some sense, \emph{incorrect} if $\neg p$. It isn't altogether clear how to capture this sense given a simple reduction of beliefs to credences. I propose to capture it using this idea that decision tables encode propositions. A table is incorrect if it encodes something that's false. To believe something is, \emph{inter alia}, to be disposed to use a table that encodes it. So if it is false, it involves a disposition to do something incorrect.

It also helps capture Holton's observation that beliefs should be resilient. If someone is disposed to use decision tables that encode that \emph{p}, that disposition should be fairly resilient. And to the extent that it is resilient, they will satisfy all the other clauses in my preferred account of belief. So anyone who believes \emph{p} should have a resilient belief that \emph{p}.

The last point is where I think my biggest disagreement with Ross and Schroeder lies. They think it is very important that a theory of belief vindicate a principle they call \textbf{Stability}.

\begin{quote}

\textbf{Stability}: A fully rational agent does not change her beliefs purely in virtue of an evidentially irrelevant change in her credences or preferences. (20)
\end{quote}
Here's the kind of case that is meant to motivate Stability, and show that views like mine are in tension with it.

\begin{quote}
Suppose Stella is extremely confident that steel is stronger than Styrofoam, but she's not so confident that she'd bet her life on this proposition for the prospect of winning a penny. PCR implies, implausibly, that if Stella were offered such a bet, she'd cease to believe that steel is stronger than Styrofoam, since her credence would cease to rationalize acting as if this proposition is true. (22)
\end{quote}
Ross and Schroeder's own view is that if Stella has a defeasible disposition to treat as true the proposition that steel is stronger than Styrofoam, that's enough for her to believe it. And that can be true if the disposition is not only defeasible, but actually defeated in the circumstances Stella is in. This all strikes me as just as implausible as the failure of Stability. Let's go over its costs.

The following propositions are clearly not mutually consistent, so one of them must be given up. We're assuming that Stella is facing, and knows she is facing, a bet that pays a penny if steel is stronger than Styrofoam, and costs her life if steel is not stronger than Styrofoam.

\begin{enumerate*}
\item Stella believes that steel is stronger than Styrofoam.

\item Stella believes that if steel is stronger than Styrofoam, she'll win a penny and lose nothing by taking the bet.

\item If 1 and 2 are true, and Stella considers the question of whether she'll win a penny and lose nothing by taking the bet, she'll believe that she'll win a penny and lose nothing by taking the bet.

\item Stella prefers winning a penny and losing nothing to getting nothing.

\item If Stella believes that she'll win a penny and lose nothing by taking the bet, and prefers winning a penny and losing nothing to getting nothing, she'll take the bet.

\item Stella won't take the bet.

\end{enumerate*}
It's part of the setup of the problem that 2 and 4 are true. And it's common ground that 6 is true, at least assuming that Stella is rational. So we're left with 1, 3 and 5 as the possible candidates for falsehood.

Ross and Schroeder say that it's implausible to reject 1. After all, Stella believed it a few minutes ago, and hasn't received any evidence to the contrary. And I guess rejecting 1 isn't the most intuitive philosophical conclusion I've ever drawn. But compare the alternatives!

If we reject 3, we must say that Stella will simply refuse to infer \emph{r} from \emph{p}, \emph{q} and $(p \wedge q) \rightarrow r$. Now it is notoriously hard to come up with a general principle for closure of beliefs. But it is hard to see why this particular instance would fail. And in any case, it's hard to see why Stella wouldn't have a general, defeasible, disposition to conclude \emph{r} in this case, so by Ross and Schroeder's own lights, it seems 3 should be acceptable.

That leaves 5. It seems on Ross and Schroeder's view, Stella simply must violate a very basic principle of means-end reasoning. She desires something, she believes that taking the bet will get that thing, and come with no added costs. Yet, she refuses to take the bet. And she's rational to do so! At this stage, I think I've lost what's meant to be belief-like about their notion of belief. I certainly think attributing this kind of practical incoherence to Stella is much less plausible than attributing a failure of Stability to her.

Put another way, I don't think presenting Stability on its own as a desideratum of a theory is exactly playing fair. The salient question isn't whether we should accept or reject Stability. The salient question is whether giving up Stability is a fair price to pay for saving basic tenets of means-end rationality. And I think that it is. Perhaps there will be some way of understanding cases like Stella's so that we don't have to choose between theories of belief that violate Stability constraints, and theories of belief that violate coherence constraints. But I don't see one on offer, and I'm not sure what such a theory could look like.

I have one more argument against Stability, but it does rest on somewhat contentious premises. There's often a difference between the best \emph{methodology} in an area, and the correct \emph{epistemology} of that area. When that happens, it's possible that there is a good methodological rule saying that if such-and-such happens, re-open a certain inquiry. But that rule need not be epistemologically significant. That is, it need not be the case that the happening of such-and-such provides evidence against the conclusion of the inquiry. It just provides a reason that a good researcher will re-open the inquiry. And, as we've stated above, an open inquiry is incompatible with belief.

Here's one way that might happen. Like other non-conciliationists about disagreement, e.g., ~\citep{Kelly2010-KELPDA}, I hold that disagreement by peers with the same evidence as you doesn't provide \emph{evidence} that you are wrong. But it might provide an excellent reason to re-open an inquiry. We shouldn't draw conclusions about the methodological significance of disagreement from the epistemology of disagreement. So learning that your peers all disagree with a conclusion might be a reason to re-open inquiry into that conclusion, and hence lose belief in the conclusion, without providing evidence that the conclusion is false. This example rests on a very contentious claim about the epistemology of disagreement. But any gap that opens up between methodology and epistemology will allow such an example to be constructed, and hence provide an independent reason to reject Stability.
