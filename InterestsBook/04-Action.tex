\chapter{Beliefs and Action}
\markright{Beliefs and Action}

\section{Examples of Pragmatic Encroachment}

Fantl and McGrath's case for pragmatic encroachment starts with cases like the following. (The following case is not quite theirs, but is similar enough to suit their plan, and easier to explain in my framework.)

\begin{quote}
\textit{Local and Express}

\noindent There are two kinds of trains that run from the city to the suburbs: the local, which stops at all stations, and the express, which skips the first eight stations. Harry and Louise want to go to the fifth station, so they shouldn't catch the Express. Though if they do it isn't too hard to catch a local back the other way, so it isn't usually a large cost. Unfortunately, the trains are not always clearly labelled. They see a particular train about to leave. If it's a local they are better off catching it, if it is an express they should wait for the next local, which they can see is already boarding passengers and will leave in a few minutes. While running towards the train, they hear a fellow passenger say ``It's a local.'' This gives them good, but far from overwhelming, reason to believe that the train is a local. Passengers get this kind of thing wrong fairly frequently, but they don't have time to get more information. So each of them face a gamble, which they can take by getting on the train. If the train is a local, they will get home a few minutes early. If it is an express they will get home a few minutes later. For Louise, this is a low stakes gamble, as nothing much turns on whether she is a few minutes early or late, but she does have a weak preference for arriving earlier rather than later. But for Harry it is a high stakes gamble, because if he is late he won't make the start of his daughter's soccer game, which will highly upset her. There is no large payoff for Harry arriving early. 
\end{quote}

\noindent What should each of them do? What should each of them believe?

The first question is relatively easy. Louise should catch the train, and Harry should wait for the next. For each of them that's the utility maximising thing to do. The second one is harder. Fantl and McGrath suggest that, despite being in the same epistemic position with respect to everything except their interests, Louise is justified in believing the train is a local and Harry is not. I agree. (If you don't think the particular case fits this pattern, feel free to modify it so the difference in interests grounds a difference in what they are justified in believing.) Does this show that our notion of epistemic justification has to be pragmatically sensitive? I'll argue that it does not.

The fundamental assumption I'm making is that what is primarily subject to epistemic evaluation are degrees of belief, or what are more commonly called states of confidence in ordinary language. When we think about things this way, we see that Louise and Harry are justified in adopting \textit{the very same degrees of belief}. Both of them should be confident, but not absolutely certain, that the train is a local. We don't have even the appearance of a counterxample to Probabilistic Evidentialism here. If we like putting this in numerical terms, we could say that each of them is justified in assigning a probability of around 0.9 to the proposition \textit{That train is a local}.\footnote{I think putting things numerically is misleading because it suggests that the kind of bets we usually use to measure degrees of belief are open, salient options for Louise and Harry. But if those bets were open and salient, they wouldn't \textit{believe} the train is a local. Using qualitative rather than quantitative language to describe them is just as accurate, and doesn't have misleading implications about their practical environment.} So as long as we adopt a Probability First epistemology, where we in the first instance evaluate the probabilities that agents assign to propositions, Harry and Louise are evaluated alike iff they do the same thing.

How then can we say that Louise alone is justified in believing that the train is a local? Because that state of confidence they are justified in adopting, the state of being fairly confident but not absolutely certain that the train is a local, counts as believing that the train is a local given Louise's context but not Harry's context. Once Louise hears the other passenger's comment, conditionalising on \textit{That's a local} doesn't change any of her preferences over open, salient actions, including such `actions' as believing or disbelieving propositions. But conditional on the train being a local, Harry prefers catching the train, which he actually does not prefer.

In cases like this, interests matter not because they affect the degree of confidence that an agent can reasonably have in a proposition's truth. (That is, not because they matter to epistemology.) Rather, interests matter because they affect whether those reasonable degrees of confidence amount to belief. (That is, because they matter to philosophy of mind.) There is no reason here to let pragmatic concerns into epistemology.


\section{Justification and Practical Reasoning}

The discussion in the last section obviously didn't show that there is no encroachment of pragmatics into epistemology. There are, in particular, two kinds of concerns one might have about the prospects for extending my style of argument to block all attempts at pragmatic encroachment. The biggest concern is that it might turn out to be impossible to defend a Probability First epistemology, particularly if we do not allow ourselves pragmatic concerns. For instance, it is crucial to this project that we have a notion of evidence that is not defined in terms of traditional epistemic concepts (e.g. as knowledge), or in terms of interests. This is an enormous project, and I'm not going to attempt to tackle it here. The second concern is that we won't be able to generalise the discussion of that example to explain the plausibility of (JP) without conceding something to the defenders of pragmatic encroachment.

\begin{description*}
\item[(JP)] If \(S\)  justifiably believes that \(p\), then \(S\)  is justified in using \(p\) as a premise in practical reasoning.
\end{description*}

\noindent And that's what we will look at in this section. To start, we need to clarify exactly what (JP) means. Much of this discussion will be indebted to Fantl and McGrath's discussion of various ways of making (JP) more precise. To see some of the complications at issue, consider a simple case of a bet on a reasonably well established historical proposition. The agent has a lot of evidence that supports \(p\), and is offered a bet that returns \$1 if \(p\) is true, and loses \$500 if \(p\) is false. Since her evidence doesn't support \textit{that} much confidence in \(p\), she properly declines the bet. One might try to reason intuitively as follows. Assume that she justifiably believed that \(p\). Then she'd be in a position to make the following argument.

\begin{quote}
\(p\) \\
If \(p\), then I should take the bet \\
So, I should take the bet
\end{quote}

\noindent Since she isn't in a position to draw the conclusion, she must not be in a position to endorse both of the premises. Hence (arguably) she isn't justified in believing that \(p\). But we have to be careful here. If we assume also that \(p\) is true (as Fantl and McGrath do, because they are mostly concerned with knowledge rather than justified belief), then the second premise is clearly false, since it is a conditional with a true antecedent and a false consequent. So the fact that she can't draw the conclusion of this argument only shows that she can't endorse \textit{both} of the premises, and that's not surprising since one of the premises is most likely false. (I'm not assuming here that the conditional is true iff it has a true antecendent or a false consequent, just that it is only true if it has a false antecedent or a true consequent.)

In order to get around this problem, Fantl and McGrath suggest a few other ways that our agent might reason to the bet. They suggest each of the following principles.

\begin{quote}
S knows that p only if, for any act A, if \(S\) knows that if p, then A is the best thing she can do, then \(S\) is rational to do A. (72)

\noindent \(S\) knows that p only if, for any states of affairs A and B, if \(S\) knows that if p, then A is better for her than B, then \(S\) is rational to prefer A to B. (74)

\noindent \textbf{(PC)} \(S\) is justified in believing that p only if \(S\) is rational to prefer as if p. (77)
\end{quote}

\noindent Hawthorne \citeyearpar[174-181]{Hawthorne2004} appears to endorse the second of these principles. He considers an agent who endorses the following implication concerning a proposed sell of a lottery ticket for a cent, which is well below its actuarially fair value.

\begin{quote}
I will lose the lottery. \\
\noindent If I keep the ticket, I will get nothing. \\
\noindent If I sell the ticket, I will get a cent. \\
\noindent So I ought to sell the ticket. (174)
\end{quote}

\noindent (To make this fully explicit, it helps to add the tacit premise that a cent is better than nothing.) Hawthorne says that this is intuitively a \textit{bad} argument, and concludes that the agent who attempts to use it is not in a position to know its first premise. But that conclusion only follows if we assume that the argument form is acceptable. So it is plausible to conclude that he endorses Fantl and McGrath's second principle.

The interesting question here is whether the theory endorsed in this paper can validate the true principles that Fantl and McGrath articulate. (Or, more precisely, we can validate the equivalent true principles concerning justified belief, since knowledge is outside the scope of the paper.) I'll argue that it can in the following way. First, I'll just note that given the fact that the theory here implies the closure principles we outlined in section 5, we can easily enough endorse Fantl and McGrath's first two principles. This is good, since they seem true. The longer part of the argument involves arguing that their principle (PC), which doesn't hold on the theory endorsed here, is in fact incorrect.

One might worry that the qualification on the closure principles in section 5 mean that we can't fully endorse the principles Fantl and McGrath endorse. In particular, it might be worried that there could be an agent who believes that \(p\), believes that if \(p\), then A is better than B, but doesn't put these two beliefs together to infer that A is better than B. This is certainly a possibility given the qualifications listed above. But note that in this position, if those two beliefs were justified, the agent would certainly be \textit{rational} to conclude that A is better than B, and hence rational to prefer A to B.  So the constraints on the closure principles don't affect our ability to endorse these two principles.

The real issue is (PC). Fantl and McGrath offer a lot of cases where (PC) holds, as well as arguing that it is plausibly true given the role of implications in practical reasoning. What's at issue is that (PC) is stronger than a deductive closure principle. It is, in effect, equivalent to endorsing the following schema as a valid principle of implication.

\begin{quote}
\(p\) \\
\noindent Given \(p\), A is preferable to B \\
\noindent So, A is preferable to B
\end{quote}

\noindent I call this Practical Modus Ponens, or PMP. The middle premise in PMP is \textit{not} a conditional. It is not to be read as \textit{If p, then A is preferable to B}. Conditional valuations are not conditionals. To see this, again consider the proposed bet on (true) \(p\) at exorbitant odds, where A is the act of taking the bet, and B the act of declining the bet. It's true that given \(p\), A is preferable to B. But it's not true that if \(p\), then A is preferable to B. Even if we restrict our attention to cases where the preferences in question are perfectly valid, this is a case where PMP is invalid. Both premises are true, and the conclusion is false. It might nevertheless be true that whenever an agent is justified in believing both of the premises, she is justified in believing the conclusion. To argue against this, we need a \textit{very} complicated case, involving embedded bets and three separate agents, Quentin, Robby and Thom. All of them have received the same evidence, and all of them are faced with the same complex bet, with the following properties.

\begin{itemize*}
\item \(p\) is an historical proposition that is well (but not conclusively) supported by their evidence, and happens to be true. All the agents have a high credence in \(p\), which is exactly what the evidence supports.
\item The bet A, which they are offered, wins if \(p\) is true, and loses if \(p\) is false.
\item If they win the bet, the prize is the bet B.
\item \(s\) is also an historical proposition, but the evidence tells equally for and against it. All the agents regard \(s\) as being about as likely as not. Moreover, \(s\) turns out to be false.
\item The bet B is worth \$2 if \(s\) is true, and worth -\$1 if \(s\) is false. Although it is actually a losing bet, the agents all rationally value it at around 50 cents.
\item How much A costs is determined by which proposition from the partition \{\(q, r, s\)\} is true.
\item If \(q\) is true, A costs \$2
\item If \(r\) is true, A costs \$500
\item If \(t\) is true, A costs \$1
\item The evidence the agents has strongly supports \(r\), though \(t\) is in fact true
\item Quentin believes \(q\)
\item Robby believes \(r\)
\item Thom believes \(t\)
\end{itemize*}

\noindent All of the agents make the utility calculations that their beliefs support, so Quentin and Thom take the bet and lose a dollar, while Robby declines it. Although Robby has a lot of evidence in favour of \(p\), he correctly decides that it would be unwise to bet on \(p\) at effective odds of 1000 to 1 against. I'll now argue that both Quentin and Thom are potential counterexamples to (PC). There are three possibilities for what we can say about those two.

First, we could say that they are justified in believing \(p\), and rational to take the bet. The problem with this position is that if they had rational beliefs about the partition \{\(q, r, t\)\} they would realise that taking the bet does not maximise expected utility. If we take rational decisions to be those that maximise expected utility given a rational response to the evidence, then the decisions are clearly not rational.

Second, we could say that although Quentin and Thom are not rational in accepting the bet, nor are they justified in believing that \(p\). This doesn't seem particularly plausible for several reasons. The irrationality in their belief systems concerns whether \(q, r\) or \(t\) is true, not whether \(p\) is true. If Thom suddenly got a lot of evidence that \(t\) is true, then all of his (salient) beliefs would be well supported by the evidence. But it is bizarre to think that whether his belief in \(p\) is rational turns on how much evidence he has for \(t\). Finally, even if we accept that agents in higher stakes situations need more evidence to have justified beliefs, the fact is that the agents are in a low-risk situation, since \(t\) is actually true, so the most they could lose is \$1.

So it seems like the natural thing to say is that Quentin and Thom \textit{are} justified in believing that \(p\), and are justified in believing that given \(p\), it maximises expected utility to take the bet, but they are not rational to take the bet. (At least, in the version of the story where they are thinking about which of \(q, r\) and \(t\) are correct given their evidence when thinking about whether to take the bet they are counterexamples to (PC).) Against this, one might respond that if belief in \(p\) is justified, there are arguments one might make to the conclusion that the bet should be taken. So it is inconsistent to say that the belief is justified, but the decision to take the bet is not rational. The problem is finding a premise that goes along with \(p\) to get the conclusion that taking the bet is rational. Let's look at some of the premises the agent might use.

\begin{itemize*}
\item If \(p\), then the best thing to do is to take the bet.
\end{itemize*}

\noindent This isn't true (\(p\) is true, but the best thing to do isn't to take the bet). More importantly, the agents think this is only true if \(s\) is true, and they think \(s\) is a 50/50 proposition. So they don't believe this premise, and it would not be rational to believe it.

\begin{itemize*}
\item If \(p\), then probably the best thing to do is to take the bet.
\end{itemize*}

\noindent Again this isn't true, and it isn't well supported, and it doesn't even support the conclusion, for it doesn't follow from the fact that \(x\) is probably the best thing to do that \(x\) should be done.

\begin{itemize*}
\item If \(p\), then taking the bet maximises rational expected utility.
\end{itemize*}

\noindent This isn't true -- it is a conditional with a true antecedent and a false consequent. Moreover, if Quentin and Thom were rational, like Robby, they would recognise this.

\begin{itemize*}
\item If \(p\), then taking the bet maximises expected utility relative to their beliefs.
\end{itemize*}

\noindent This is true, and even reasonable to believe, but it doesn't imply that they should take the bet. It doesn't follow from the fact that doing something maximises expected utility relative to my crazy beliefs that I should do that thing.

\begin{itemize*}
\item Given \(p\), taking the bet maximises rational expected utility.
\end{itemize*}

\noindent This is true, and even reasonable to believe, but it isn't clear that it supports the conclusion that the agents should take the bet. The implication appealed to here is PMP, and in this context that's close enough to equivalent to (PC). If we think that this case is a prima facie problem for (PC), as I think is intuitively plausible, then we can't use (PC) to show that it \textit{doesn't} post a problem. We could obviously continue for a while, but it should be clear it will be very hard to find a way to justify taking the bet even spotting the agents \(p\) as a premise they can use in rational deliberation. So it seems to me that (PC) is not in general true, which is good because as we'll see in cases like this one the theory outlined here does not support it.

The theory we have been working with says that belief that \(p\) is justified iff the agent's degree of belief in \(p\) is sufficient to amount to belief in their context, and they are justified in believing \(p\) to that degree. Since by hypothesis Quentin and Thom are justified in believing \(p\) to the degree that they do, the only question left is whether this amounts to belief. This turns out not to be settled by the details of the case as yet specified. At first glance, assuming there are no other relevant decisions, we might think they believe that \(p\) because (a) they prefer (in the relevant sense) believing \(p\) to not believing \(p\), and (b) conditionalising on \(p\) doesn't change their attitude towards the bet. (They prefer taking the bet to declining it, both unconditionally and conditional on \(p\).) 

But that isn't all there is to the definition of belief \textit{tout court}. We must also ask whether conditionalising on \(p\) changes any preferences conditional on any active proposition. And that may well be true. Conditional on \(r\), Quentin and Thom prefer not taking the bet to taking it. But conditional on \(r\) and \(p\), they prefer taking the bet to not taking it. So if \(r\) is an active proposition, they don't believe that \(p\). If \(r\) is not active, they do believe it. In more colloquial terms, if they are concerned about the possible truth of \(r\) (if it is salient, or at least not taken for granted to be false) then \(p\) becomes a potentially high-stakes proposition, so they don't believe it without extraordinary evidence (which they don't have). Hence they are only a counterexample to (PC) if \(r\) is not active. But if \(r\) is not active, our theory predicts that they are a counterexample to (PC), which is what we argued above is intuitively correct.

Still, the importance of \(r\) suggests a way of saving (PC). Above I relied on the position that if Quentin and Thom are not maximising rational expected utility, then they are being irrational. This is perhaps too harsh. There is a position we could take, derived from some suggestions made by Gilbert Harman in \textit{Change in View}, that an agent can rationally rely on their beliefs, even if those beliefs were not rationally formed, if they cannot be expected to have kept track of the evidence they used to form that belief. If we adopt this view, then we might be able to say that (PC) is compatible with the correct normative judgments about this case.

To make this compatibility explicit, let's adjust the case so Quentin takes \(q\) for granted, and cannot be reasonably expected to have remembered the evidence for \(q\). Thom, on the other hand, forms the belief that \(t\) rather than \(r\) is true in the course of thinking through his evidence that bears on the rationality of taking or declining the bet. (In more familiar terms, \(t\) is part of the inference Thom uses in coming to conclude that he should take the bet, though it is not part of the final implication he endorses whose conclusion is that he should take the bet.) Neither Quentin nor Thom is a counterexample to (PC) thus understood. (That is, with the notion of rationality in (PC) understood as Harman suggests that it should be.) Quentin is not a counterexample, because he is \textit{rational} in taking the bet. And Thom is not a counterexample, because in his context, where \(r\) is active, his credence in \(p\) does not amount to belief in \(p\), so he is not justified in believing \(p\).

We have now two readings of (PC). On the strict reading, where a rational choice is one that maximises rational expected utility, the principle is subject to counterexample, and seems generally to be implausible. On the loose reading, where we allow agents to rely on beliefs formed irrationally in the past in rational decision making, (PC) \textit{is} plausible. Happily, the theory sketched here agrees with (PC) on the plausible loose reading, but not on the implausible strict reading. In the previous section I argued that the theory also accounts for intuitions about particular cases like \textit{Local and Express}. And now we've seen that the theory accounts for our considered opinions about which principles connecting justified belief to rational decision making we should endorse. So it seems at this stage that we can account for the intuitions behind the pragmatic encroachment view while keeping a concept of probabilistic epistemic justification that is free of pragmatic considerations.

\section{Odds and Stakes}

It is common to describe IRI as a theory where in `high stakes' situations, more evidence is needed for knowledge than in `low stakes' situations. But this is at best misleading. What really matters are the odds on any bet-like decision the agent faces with respect to the target proposition. More precisely, interests affect belief because  whether someone believes \(p\) depends \textit{inter alia} on whether their credence in \(p\) is high enough that any bet on \(p\) they actually face is a good bet. Raising the stakes of any bet on \(p\) does not directly change that, but changing the odds of the bets on \(p\) they face does change it. Now in practice due to the declining marginal utility of material goods, high stakes situations will usually be situations where an agent faces long odds. But it is the odds that matter to knowledge, not the stakes.

Some confusion on this point may have been caused by the Bank Cases that Stanley uses, and the Train Cases that Fantl and McGrath use, to motivate IRI. In those cases, the authors lengthen the odds the relevant agents face by increasing the potential losses the agent faces by getting the bet wrong. But we can make the same point by decreasing the amount the agent stands to gain by taking the bet. Let's go through a pair of cases, which I'll call the Map Cases, that illustrate this.

\begin{description*}
\item[High Cost Map:] Zeno is walking to the Mysterious Bookshop in lower Manhattan. He's pretty confident that it's on the corner of Warren Street and West Broadway. But he's been confused about this in the past, forgetting whether the east-west street is Warren or Murray, and whether the north-south street is Greenwich, West Broadway or Church. In fact he's right about the location this time, but he isn't justified in having a credence in his being correct greater than about 0.95. While he's walking there, he has two options. He could walk to where he thinks the shop is, and if it's not there walk around for a few minutes to the nearby corners to find where it is. Or he could call up directory assistance, pay \$1, and be told where the shop is. Since he's confident he knows where the shop is, and there's little cost to spending a few minutes walking around if he's wrong, he doesn't do this, and walks directly to the shop.
\item[Low Cost Map:] Just like the previous case, except that Zeno has a new phone with more options. In particular, his new phone has a searchable map, so with a few clicks on the phone he can find where the store is. Using the phone has some very small costs. For example, it distracts him a little, which marginally raises the likelihood of bumping into another pedestrian. But the cost is very small compared to the cost of getting the location wrong. So even though he is very confident about where the shop is, he double checks while walking there.
\end{description*}

\noindent I think the Map Cases are like the Bank Cases, Train Cases etc., in all important respects. I think Zeno knows where the shop is in High Cost Map, and doesn't know in Low Cost Map. And he doesn't know in Low Cost Map because the location of the shop has suddenly become the subject matter of a bet at very long odds. You should think of Zeno's not checking the location of the shop on his phone-map as a bet on the location of the shop. If he wins the bet, he wins a few seconds of undistracted strolling. If he loses, he has to walk around a few blocks looking for a store. The disutility of the loss seems easily twenty times greater than the utility of the gain, and by hypothesis the probability of winning the bet is no greater than 0.95. So he shouldn't take the bet. Yet  if he knew where the store was, he would be justified in taking the bet. So he doesn't know where the store is. Now this is not a case where higher \textit{stakes} defeat knowledge. If anything, the stakes are lower in Low Cost Map. But the relevant odds are longer, and that's what matters to knowledge.\footnote{Note that I'm not claiming that it is intuitive that Zeno has knowledge in High Cost Map, but not that Low Cost Map. Nor am I claiming that we should believe IRI because it gets the Map Cases right. In fact, I don't believe either of those things. Instead I believe Zeno has knowledge in High Cost Map and not in Low Cost Map because I believe IRI is correct, and that's what IRI says about the case. It is sometimes assumed, e.g, in the experimental papers I'll discuss in section \ref{sect:xphi}, that pairs of cases like these are meant to \textit{motivate}, and not just \textit{illustrate}, IRI. I can't speak for everyone's motivations, but I'm only using these cases as illustrations, not motivations.}


\section{The Power of Theoretical Interests}
So I think we should accept that credences exist. And we can just about reduce beliefs to credences. In previous work I argued that we could do such a reduction. I'm not altogether sure whether the amendments to that view I'm proposing here means it no longer should count as a reductive view; we'll come back to that question in the conclusion.

The view I defended in previous work is that the reduction comes through the relationship between conditional and unconditional attitudes. Very roughly, to believe that \emph{p} is simply to have the same attitudes, towards all salient questions, unconditionally as you have conditional on \emph{p}. In a syrupy slogan, belief means never having to say you've conditionalised. For reasons I mentioned in section 1, I now think that was inaccurate; I should have said that belief means never having to say you've updated, or at least that you've updated your view on any salient question.

The restriction to salient questions is important. Consider any \emph{p} that I normally take for granted, but such that I wouldn't bet on it at insane odds. I prefer declining such a bet to taking it. But conditional on \emph{p}, I prefer taking the bet. So that means I don't believe any such \emph{p}. But just about any \emph{p} satisfies that description, for at least some `insane' odds. So I believe almost nothing. That would be a \emph{reductio} of the position. I respond by saying that the choice of whether to take an insane bet is not normally salient.

But now there's a worry that I've let in too much. For many \emph{p}, there is no salient decision that they even bear on. What I would do conditional on \emph{p}, conditional on $\neg p$, and unconditionally is exactly the same, over the space of salient choices. (And this isn't a case where updating and conditionalising come apart; I'll leave this proviso mostly implicit from now on.) So with the restriction in place, I believe \emph{p} and $\neg p$. That seems like a \emph{reductio} of the view too. I probably do have inconsistent beliefs, but not in virtue of \emph{p} being irrelevant to me right now. I've changed my mind a little about what the right way to avoid this problem is, in part because of some arguments by Jacob Ross and Mark Schroeder. 

They have what looks like, on the surface, a rather different view to mine. They say that to believe \emph{p} is to have a \textbf{default reasoning disposition} to use \emph{p} in reasoning. Here's how they describe their own view.

\begin{quote}

What we should expect, therefore, is that for some propositions we would have a \emph{defeasible} or \emph{default} disposition to treat them as true in our reasoning--a disposition that can be overridden under circumstances where the cost of mistakenly acting as if these propositions are true is particularly salient. And this expectation is confirmed by our experience. We do indeed seem to treat some uncertain propositions as true in our reasoning; we do indeed seem to treat them as true automatically, without first weighing the costs and benefits of so treating them; and yet in contexts such as High where the costs of mistakenly treating them as true is salient, our natural tendency to treat these propositions as true often seems to be overridden, and instead we treat them as merely probable.

But if we concede that we have such defeasible dispositions to treat particular propositions as true in our reasoning, then a hypothesis naturally arises, namely, that beliefs consist in or involve such dispositions. More precisely, at least part of the functional role of belief is that believing that \emph{p} defeasibly disposes the believer to treat \emph{p} as true in her reasoning. Let us call this hypothesis the \emph{reasoning disposition account} of belief.
\end{quote}

\noindent There are, relative to what I'm interested in, three striking characteristics of Ross and Schroeder's view.

\begin{enumerate*}
\item Whether you believe \emph{p} is sensitive to how you reason; that is, your theoretical interests matter.

\item How you would reason about some questions that are not live is relevant to whether you believe \emph{p}.

\item Dispositions can be masked, so you can believe \emph{p} even though you don't actually use \emph{p} in reasoning now.

\end{enumerate*}

I think they take all three of these points to be reasons to favour their view over mine. As I see it, we agree on point 1 (and I always had the resources to agree with them), I can accommodate point 2 with a modification to my theory, and point 3 is a cost of their theory, not a benefit. Let's take those points in order.

There are lots of reasons to dislike what Ross and Schroeder call \emph{Pragmatic Credal Reductionism} (PCR). This is, more or less, the view that the salient questions, in the sense relevant above, are just those which are practically relevant to the agent. So to believe $p$ just is to have the same attitude towards all practically relevant questions unconditionally as conditional on $p$. There are at least three reasons to resist this view.

One reason comes from the discussions of Ned Block's example Blockhead ~\citep{Block1978}. As Braddon-Mitchell and Jackson point out, the lesson to take from that example is that beliefs are constituted in part by their relations to other mental states ~\citep[114ff]{DBMJackson2007}. There's a quick attempted refutation of PCR via the Blockhead case which doesn't quite work. We might worry that if all that matters to belief given PCR is how it relates to action, PCR will have the implausible consequence that Blockhead has a rich set of beliefs. That isn't right; PCR is compatible with the view that Blockhead doesn't have credences, and hence doesn't have credences that constitute beliefs. But the Blockhead examples value isn't exhausted by its use in quick refutations.\footnote{The point I'm making here is relevant I think to recent debates about the proper way to formalise counterexamples in philosophy, as in ~\citep{Williamson2007-WILTPO-17, IchikawaJarvis2009, Malmgren2011}. I worry that too much of that debate is focussed on the role that examples play in one-step refutations. But there's more, much more, to a good example than that.} The lesson is that beliefs are, by their nature, interactive. It seems to me that PCR doesn't really appreciate that lesson.

Another reason comes from recent work by Jessica \cite{Brown2013}. Compare these two situations.

\begin{enumerate*}
\item \emph{S} is in circumstances \emph{C}, and has to decide whether to do \emph{X}.

\item \emph{S} is in completely different circumstances to \emph{C}, but is seriously engaged in planning for future contingencies. She's currently trying to decide whether in circumstances \emph{C} to do \emph{X}.

\end{enumerate*}
Intuitively, \emph{S} can bring exactly the same evidence, knowledge and beliefs to bear on the two problems. If \emph{C} is a particularly high stakes situation, say it is a situation where one has to decide what to feed someone with a severe peanut allergy, then a lot of things that can ordinarily be taken for granted cannot, in this case, be taken for granted. And that's true whether \emph{S} is actually in \emph{C}, or she is just planning for the possibility that she finds herself in \emph{C}.

So I conclude that both practical and theoretical interests matter for what we can take for granted in inquiry. The things we can take for granted into a theoretical inquiry into what to do in high stakes contexts as restricted, just as they are when we are in a high stakes context, and must make a practical decision. Since the latter restriction on what we can take for granted is explained by (and possibly constituted by) a restriction on what we actually believe in those contexts, we should similarly conclude that agents simply believe less when they are reasoning about high stakes contexts, whatever their actual context.

A third reason to dislike PCR comes from the `Renzi' example in Ross and Schroeder's paper. I'll run through a somewhat more abstract version of the case, because I don't think the details are particularly important. Start with a standard decision problem. The agent knows that X is better to do if \emph{p}, and Y is better to do if $\neg p$. The agent should then go through calculating the relative gains to doing X or Y in the situations they are better, and the probability of \emph{p}. But the agent imagined doesn't do that. Rather, the agent divides the possibility space in four, taking the salient possibilities to be $p \wedge q, p \wedge \neg q, \neg p \wedge q$ and $\neg p \wedge \neg q$, and then calculates the expected utility of X and Y accordingly. This is a bad bit of reasoning on the agent's part. In the cases we are interested in, \emph{q} is exceedingly likely. Moreover, the expected utility of each act doesn't change a lot depending on \emph{q}'s truth value. So it is fairly obvious that we'll end up making the same decision whether we take the `small worlds' in our decision model to be just the world where \emph{p}, and the world where $\neg p$, or the four worlds this agent uses. But the agent does use these four, and the question is what to say about them.

Ross and Schroeder say that such an agent should not be counted as believing that \emph{q}. If they are consciously calculating the probability that \emph{q}, and taking $\neg q$ possibilities into account when calculating expected utilities, they regard \emph{q} as an open question. And regarding \emph{q} as open in this way is incompatible with believing it. I agree with all this.

They also think that PCR implies that the agent \emph{does} believe \emph{q}. The reason is that conditionalising on \emph{q} doesn't change the agent's beliefs about any practical question. I think that's right too, at least on a natural understanding of what `practical' is.

My response to all these worries is to say that whether someone believes that \emph{p} depends not just on how conditionalising (or more generally updating) on \emph{p} would affect someone's action, but on how it would affect their reasoning. That is, just as we learned from the Blockhead example, to believe that \emph{p} requires having a mental state that is connected to the rest of one's cognitive life in roughly the way a belief that \emph{p} should be connected. Let's go through both the last two cases to see how this works on my theory.

One of the things that happens when the stakes go up is that conditionalising on very probable things can change the outcome of interesting decisions. Make the probability that some nice food is peanut-free be high, but short of one. Conditional on it being peanut-free, it's a good thing to give to a peanut-allergic guest. But unconditionally, it's a bad thing to give to such a guest, because the niceness of the food doesn't outweigh the risk of killing them. And that's true whether the guest is actually there, or you're just thinking about what to do should such a guest arrive in the future. In general, the same questions will be relevant whether you're in \emph{C} trying to decide whether to do \emph{X}, or simply trying to decide whether to \emph{X} in \emph{C}. In one case they will be practically relevant questions, in the other they will be theoretically relevant questions. But this feels a lot like a distinction without a difference, since the agent should have similar beliefs in the two cases.

The same response works for Ross and Schroeder's case. The agent was trying to work out the expected utility of X and Y by working out the utility of each action in each of four `small worlds', then working out the probability of each of these. Conditional on \emph{q}, the probability of two of them ($p \wedge \neg q, \neg p \wedge \neg q$), will be 0. Unconditionally, this probability won't be 0. So the agent has a different view on some question they have taken an interest in unconditionally to their view conditional on \emph{q}. So they don't believe \emph{q}. The agent shouldn't care about that question, and conditional on each question they should care about, they have the same attitude unconditionally and conditional on \emph{q}. But they do care about these probabilistic questions, so they don't believe \emph{q}. (In ~\citep{Weatherson2005-WEACWD} I said that to justifiably believe \emph{q} was to have a justified credence in \emph{q} that was sufficiently high to count as a belief. The considerations of the last two sentences puts some pressure on that reductive theory of justification for beliefs.)

So I think that Ross and Schroeder and I agree on point 1; something beyond practical interests is relevant to belief.

They have another case that I think does suggest a needed revision to my theory. I'm going to modify their case a little to change the focus a little, and to avoid puzzles about vagueness. (What follows is a version of their example about Dal\'\i's moustache, purged of any worries about vagueness, and without the focus on consistency. I don't think the problem they true to press on me, that my theory allows excessive inconsistency of belief among rational agents, really sticks. Everyone will have to make qualifications to consistency to deal with the preface paradox, and for reasons I went over in ~\citep{Weatherson2005-WEACWD}, I think the qualifications I make are the best ones to make.)

Let \emph{D} be the proposition that the number of games the Detroit Tigers won in 1976 (in the MLB regular season) is not a multiple of 3. At most times, \emph{D} is completely irrelevant to anything I care about, either practically or theoretically. My attitudes towards any relevant question are the same unconditionally as conditional on \emph{D}. So there's a worry that I'll count as believing \emph{D}, and believing $\neg D$, by default.

In earlier work, I added a clause meant to help with cases like this. I said that for determining whether an agent believes that \emph{p}, we should treat the question of whether \emph{p}'s probability is above or below 0.5 as salient, even if the agent doesn't care about it. Obviously this won't help with this particular case. The probability of \emph{D} is around \nicefrac{2}{3}, and is certainly above 0.5. My `fix' avoids the consequence that I implausibly count as believing $\neg D$. But I still count, almost as implausibly, as believing \emph{D}. This needs to be fixed.

Here's my proposed change. For an agent to count as believing \emph{p}, it must be possible for \emph{p} to do some work for them in reasoning. Here's what I mean by work. Consider a very abstract set up of a decision problem, as follows.

\begin{center}
\begin{tabular}{rcc}
&\emph{p}&\emph{q}\\
%\midrule
X&4&1\\
Y&3&2\\
\end{tabular}
\end{center}

%\begin{table}[htbp]
%\begin{minipage}{\linewidth}
%\setlength{\tymax}{0.5\linewidth}
%\centering
%\small
%\begin{tabulary}{\textwidth}{@{}RCC@{}} \toprule
%&\emph{p}&\emph{q}\\
%\midrule
%X&4&1\\
%Y&3&2\\
%
%\bottomrule
%
%\end{tabulary}
%\end{minipage}
%\end{table}
%

That table encodes a lot of information. It encodes that $p \vee q$ is true; otherwise there are some columns missing. It encodes that the only live choices are X or Y; otherwise there are rows missing. It encodes that doing X is better than doing Y if \emph{p}, and worse if \emph{q}. 

For any agent, and any decision problem, there is a table like this that they would be disposed to use to resolve that problem. Or, perhaps, there are a series of tables and there is no fact about which of them they would be most disposed to use.

Given all that terminology, here's my extra constraint on belief. To believe that \emph{p}, there must be some decision problem such that some table the agent would be disposed to use to solve it encodes that \emph{p}. If there is no such problem, the agent does not believe that \emph{p}. For anything that I intuitively believe, this is an easy condition to satisfy. Let the problem be whether to take a bet that pays 1 if \emph{p}, and loses 1 otherwise. Here's the table I'd be disposed to use to solve the problem.

\begin{center}
\begin{tabular}{rc}
&\emph{p}\\
%\midrule
Take bet&1\\
Decline bet&0\\
\end{tabular}
\end{center}
%\begin{table}[htbp]
%\begin{minipage}{\linewidth}
%\setlength{\tymax}{0.5\linewidth}
%\centering
%\small
%\begin{tabulary}{\textwidth}{@{}RC@{}} \toprule
%&\emph{p}\\
%\midrule
%Take bet&1\\
%Decline bet&0\\
%
%\bottomrule
%
%\end{tabulary}
%\end{minipage}
%\end{table}
%
%
This table encodes that \emph{p}, so it is sufficient to count as believing that \emph{p}. And it doesn't matter that this bet isn't on the table. I'm disposed to use this table, so that's all that matters.

But might there be problems in the other direction. What about an agent who, if offered such a bet on \emph{D}, would use such a simple table? I simply say that they believe that \emph{D}. I would not use any such table. I'd use this table.

\begin{center}
\begin{tabular}{rcc}
&\emph{D}&$\neg D$\\
%\midrule
Take bet&1&--1\\
Decline bet&0&0\\
\end{tabular}
\end{center}
%\begin{table}[htbp]
%\begin{minipage}{\linewidth}
%\setlength{\tymax}{0.5\linewidth}
%\centering
%\small
%\begin{tabulary}{\textwidth}{@{}RCC@{}} \toprule
%&\emph{D}&$\neg D$\\
%\midrule
%Take bet&1&--1\\
%Decline bet&0&0\\
%
%\bottomrule
%
%\end{tabulary}
%\end{minipage}
%\end{table}
%
Now given the probability of \emph{D}, I'd still end up taking the bet; it has an expected return of \nicefrac{2}{3}. (Well, actually I'd probably decline the bet because being offered the bet would change the probability of \emph{D} for reasons made clear in ~\citep[14--15]{RunyonGuysDolls}. But that hardly undermines the point I'm making.) But this isn't some analytic fact about me, or even I think some respect in which I'm obeying the dictates of rationality. It's simply a fact that I wouldn't take \emph{D} for granted in any inquiry. And that's what my non-belief that \emph{D} consists in.

This way of responding to the Tigers example helps respond to a nice observation that Ross and Schroeder make about correctness. A belief that \emph{p} is, in some sense, \emph{incorrect} if $\neg p$. It isn't altogether clear how to capture this sense given a simple reduction of beliefs to credences. I propose to capture it using this idea that decision tables encode propositions. A table is incorrect if it encodes something that's false. To believe something is, \emph{inter alia}, to be disposed to use a table that encodes it. So if it is false, it involves a disposition to do something incorrect.

It also helps capture Holton's observation that beliefs should be resilient. If someone is disposed to use decision tables that encode that \emph{p}, that disposition should be fairly resilient. And to the extent that it is resilient, they will satisfy all the other clauses in my preferred account of belief. So anyone who believes \emph{p} should have a resilient belief that \emph{p}.

The last point is where I think my biggest disagreement with Ross and Schroeder lies. They think it is very important that a theory of belief vindicate a principle they call \textbf{Stability}.

\begin{quote}

\textbf{Stability}: A fully rational agent does not change her beliefs purely in virtue of an evidentially irrelevant change in her credences or preferences. (20)
\end{quote}
Here's the kind of case that is meant to motivate Stability, and show that views like mine are in tension with it.

\begin{quote}
Suppose Stella is extremely confident that steel is stronger than Styrofoam, but she's not so confident that she'd bet her life on this proposition for the prospect of winning a penny. PCR implies, implausibly, that if Stella were offered such a bet, she'd cease to believe that steel is stronger than Styrofoam, since her credence would cease to rationalize acting as if this proposition is true. (22)
\end{quote}
Ross and Schroeder's own view is that if Stella has a defeasible disposition to treat as true the proposition that steel is stronger than Styrofoam, that's enough for her to believe it. And that can be true if the disposition is not only defeasible, but actually defeated in the circumstances Stella is in. This all strikes me as just as implausible as the failure of Stability. Let's go over its costs.

The following propositions are clearly not mutually consistent, so one of them must be given up. We're assuming that Stella is facing, and knows she is facing, a bet that pays a penny if steel is stronger than Styrofoam, and costs her life if steel is not stronger than Styrofoam.

\begin{enumerate*}
\item Stella believes that steel is stronger than Styrofoam.

\item Stella believes that if steel is stronger than Styrofoam, she'll win a penny and lose nothing by taking the bet.

\item If 1 and 2 are true, and Stella considers the question of whether she'll win a penny and lose nothing by taking the bet, she'll believe that she'll win a penny and lose nothing by taking the bet.

\item Stella prefers winning a penny and losing nothing to getting nothing.

\item If Stella believes that she'll win a penny and lose nothing by taking the bet, and prefers winning a penny and losing nothing to getting nothing, she'll take the bet.

\item Stella won't take the bet.

\end{enumerate*}
It's part of the setup of the problem that 2 and 4 are true. And it's common ground that 6 is true, at least assuming that Stella is rational. So we're left with 1, 3 and 5 as the possible candidates for falsehood.

Ross and Schroeder say that it's implausible to reject 1. After all, Stella believed it a few minutes ago, and hasn't received any evidence to the contrary. And I guess rejecting 1 isn't the most intuitive philosophical conclusion I've ever drawn. But compare the alternatives!

If we reject 3, we must say that Stella will simply refuse to infer \emph{r} from \emph{p}, \emph{q} and $(p \wedge q) \rightarrow r$. Now it is notoriously hard to come up with a general principle for closure of beliefs. But it is hard to see why this particular instance would fail. And in any case, it's hard to see why Stella wouldn't have a general, defeasible, disposition to conclude \emph{r} in this case, so by Ross and Schroeder's own lights, it seems 3 should be acceptable.

That leaves 5. It seems on Ross and Schroeder's view, Stella simply must violate a very basic principle of means-end reasoning. She desires something, she believes that taking the bet will get that thing, and come with no added costs. Yet, she refuses to take the bet. And she's rational to do so! At this stage, I think I've lost what's meant to be belief-like about their notion of belief. I certainly think attributing this kind of practical incoherence to Stella is much less plausible than attributing a failure of Stability to her.

Put another way, I don't think presenting Stability on its own as a desideratum of a theory is exactly playing fair. The salient question isn't whether we should accept or reject Stability. The salient question is whether giving up Stability is a fair price to pay for saving basic tenets of means-end rationality. And I think that it is. Perhaps there will be some way of understanding cases like Stella's so that we don't have to choose between theories of belief that violate Stability constraints, and theories of belief that violate coherence constraints. But I don't see one on offer, and I'm not sure what such a theory could look like.

I have one more argument against Stability, but it does rest on somewhat contentious premises. There's often a difference between the best \emph{methodology} in an area, and the correct \emph{epistemology} of that area. When that happens, it's possible that there is a good methodological rule saying that if such-and-such happens, re-open a certain inquiry. But that rule need not be epistemologically significant. That is, it need not be the case that the happening of such-and-such provides evidence against the conclusion of the inquiry. It just provides a reason that a good researcher will re-open the inquiry. And, as we've stated above, an open inquiry is incompatible with belief.

Here's one way that might happen. Like other non-conciliationists about disagreement, e.g., ~\citep{Kelly2010-KELPDA}, I hold that disagreement by peers with the same evidence as you doesn't provide \emph{evidence} that you are wrong. But it might provide an excellent reason to re-open an inquiry. We shouldn't draw conclusions about the methodological significance of disagreement from the epistemology of disagreement. So learning that your peers all disagree with a conclusion might be a reason to re-open inquiry into that conclusion, and hence lose belief in the conclusion, without providing evidence that the conclusion is false. This example rests on a very contentious claim about the epistemology of disagreement. But any gap that opens up between methodology and epistemology will allow such an example to be constructed, and hence provide an independent reason to reject Stability.

