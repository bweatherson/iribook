# Debunking Objections

Many arguments against IRI are, in effect, debunking arguments. The objector's immediate conclusion is not that IRI is false, but that it is unsupported by the arguments given for it. 

Arguments that people do not have the intuition that, for exaple, Bojan lacks knowledge that his paper is typo-free, do not immediately show thtat IRI is false. That's because the truth of IRI can be made compatible with that intuition in two ways. For one thing, it is possible that people think Bojan knows because they think Bojan betting that his paper is typo free is, in the circumstances, a good bet. [^Compare the response to [#FeltzZarpentine2010;] that I make in [ยง1][#WeathersonDIRI;], or the response to [#Lackey2010;] by Masashi [ยง5][#Kasaki2014;].] For another thing, intuitions around here might be unreliable. Remember that one of the original motivations for IRI was that it was the lowest cost solution to the preface paradox and lottery paradox. We shouldn't expect intuitions to be reliable in the presence of serious paradox. That consideration cuts both ways; it makes  debunking objections to arguments for IRI from intuitions about cases look very promising. And I think those objections are promising; but they don't show IRI is false.

Similarly, objections to the premises of the argument from principles don't strictly entail that IRI is false. After all, IRI is an existential thesis; it says sometimes interests matter. The principles used to defend it are universal claims; they say (for example) it is always permissible to act on knowledge. Weaker versions of these principles might still be consistent with, or even supporting of, IRI. Dustin [#Locke2015;] adopts a version of this strategy. 

And there are two other two methodological points worth remembering. Sometimes it seems that critics of principles like K-Suff reason that K-Suff entails IRI, and IRI is antecedently implausible, so we should start out suspicious of K-Suff. Now why might IRI be antecedently implausible? 

I think to some extent it is because it is thought to be so revolutionary. The denial of interest-relativity is often taken to be a "traditional" view. This phrasing appears, for example, in [#Boyd2015;], and in [#IckikawaEtAl2012;], and even in the title of [#Buckwalter2014;]. And if this were correct, that would be a mark against interest-relativity. The ''inherited experience and acumen of many generations of men'' [11][#Austin1956] should not be lightly forsaken. The problem is that it isn't true; there is nothing particularly novel about IRI. As Stephen R. [#Grimm2015;] points out, you can see a version of the view in Locke, and in Clifford. What's really radical, as Descartes acknowledged, is to think the perspective of the Cartesian meditator is the right one for epistemology.

Perhaps what is unintuitive about IRI is that it makes knowledge depend on factors that are not 'truth-directed', or 'truth-conducive'. We'll return to this in the next section, since the best versions of this  objection are really arguments for the negation of IRI. The short version of my reply is that there are different ways to disambiguate the objector's claim, and the plausible ways are all consistent with IRI. 

These are all reasons to think that IRI is not antecedently implausible. There is one reason to think it is antecedently plausible. On a functionalist theory of mind, belief is a practical notion. And it is plausible that knowledge is a kind of success condition for belief. Now it's possible to have non-practical success conditions for a state our concept of which is practical. But I don't find that a natural starting assumption. It's mucn more intuitive, to me at least, that the norms of belief and the metaphysics of belief would be tightly integrated. And that suggests that IRI is, if anything, a natural default.

That's not an argument for IRI, or of course for K-Suff. And there are important direct objections to K-Suff. Jessica [#Brown2008;] and Jennifer [#Lackey2010;] have examples of people in high stakes situations who they say are intuitively described as knowing something, but not being in a position to act on it. I'm sympathetic to the two-part reply that Masashi [#Kasaki2014;] makes to these examples. The first thing to note is that these are hard cases, in areas where several paradoxes (e.g., lottery, preface, sceptical) are lurking. Intuitions are less reliable than usual around here. But another thing to notice is that it is very hard to say what actions are justified by taking _p_ for granted in various settings. Brown and Lackey both describe cases where doctors have lots of evidence for _p_, and given _p_ a certain action would maximise patient-welfare, but where intuitively it would be wrong for the doctor to act that way. As it stands, that's a problem for IRI only if doctors should maximise epistemic expected patient-welfare, and that principle isn't true. Kasaki argues that there isn't a way to fill out Lackey's example to get around this problem, and I suspect the same is true for Brown's example.

Finally, note that K-Suff is an extensional claim. Kenneth [#Boyd2015;] and Baron [#Reed2015;] object to a principle much stronger than K-Suff: the principle that what an agent knows should explain why some choices are rational for them. Both of them say that if IRI is inconsistent with the stronger principle, that is a serious problem for IRI. (In Boyd's case this is part of an argument that IRI is unmotivated; in Reed's case he takes it to be a direct objection to IRI.) Now I think IRI is inconsistent with this principle. Chika doesn't know the Red Sox won because she can't rationally choose the red ticket, not the other way around. But I don't see why the principle is so plausible. It seems plausible to me that something else (e.g., evidence) explains both rational choice and knowledge, and the way it explains both things makes IRI true.

# Direct Objections

Let's close with direct arguments against IRI. There are two kinds of arguments that I won't address here. One of these is the argument, developed in [#IchikawaEtAl2012;] that there isn't a good way to say how far interest-relativity should extend. As I noted above, I agree this is a deep problem, and don't think there is a good answer to it in the existing literature. The other kind are objections that only apply to the Stakes version of IRI, not the Odds version. One instance of this kind is the Dutch Book argument deployed by Baron [#Reed2014;]. I think several instances of that kind of argument are successful. But the theory they succeed against is not IRI, but a sub-optimal version of IRI. So I'll stick to objections that apply to the Odds version.

IRI does allow knowledge to depend on some unexpected factors. But so do most contemporary theories of knowledge. Most contemporary theories allow for knowledge to be defeated in certain ways, such as by available but unaccessed evidence [75][#Harman1973], or by nearby possibilities of error [#Goldman1976], or by mistakes in the background reasoning. The last category of cases aren't really contemporary; they trace back at least to Dharmottara [58][#Nagel2014;]. And the theories allow for defeaters to be defeated. And once we work through the details of what can defeat a defeater, it turns out many surprising things can affect knowledge.

Indeed, for just about any kind of defeater, it is possible to imagine something that in some ways makes the agent's epistemic position worse, while simultaneously defeating the defeater.[^The argument of the last two sentences is expanded on greatly in [ยง3][#WeathersonPAS;], where it is credited to Martin Smith. The idea that knowledge allows for defeaters is criticised by Maria [#Lasonen-Aarnio2014;].] If interests matter to knowledge because they matter to defeaters, as is true on my version of IRI, we should expect strange events to correlate with gaining knowledge. For example, it isn't surprising that one can gain knowledge that _p_ at exactly the moment one's evidential support for _p_ falls. This consequence of IRI is taken to be obviously unacceptable by  [#EatonPickvance2015;], but it's just a consequence of how defeaters generally work.

IRI has been criticised for making knowledge depend on agents not allowing agents to get knowledge by not caring, as in these vivid quotes:

> Not giving a damn, however enviable in other respects, should not be knowledge-making . [433][#RussellDoris2008]

> If you don't now whether penguins eat fish, but want to know, you might think ... you have to gather evidence. [But if IRI] were correct, though, you have another option: You could take a drink or shoot heroin. [1044-5][#CappelenLepore2006]

Let's walk through Cappelen and Lepore's case. IRI says that there are people who both have high confidence that penguins eat fish, and they have this confidence for reasons that are appropriately connected to the fact that penguins eat fish. But one of them really worries about sceptical doubts, and so won't regard the question of what penguins eat as settled. The other brushes off excessive sceptical doubts, and rightly so; they are, after all, excessive. IRI says that the latter knows and the former does not. If the former were to care a little less, in particular if they cared a little less about evil demons and the like, they'd know. Perhaps they could get themselves to care a little less by having a drink. That doesn't sound like a bad plan; if a sceptical doubt is destroying knowledge, and there is no gain from holding on to it, then just let it go. From this perspective, Cappelen and Lepore's conclusion does not seem like a reductio. Excessive doubt can destroy knowledge, so people with strong, non-misleading evidence can gain knowledge by setting aside doubts. And drink can set aside doubt. So drink can lead to knowledge.[^[#Wright2004;] notes that there often is not value in holding on to sceptical doubts, and the considerations of this paragraph are somewhat inspired by his views. My sense is that sceptical doubts are less debilitating than heroin addiction, so don't go down that route.]

But note that the drink doesn't generate the knowledge. It blocks, or defeats, something that threatens to block knowledge. We should say the same thing to Russell and Doris's objection. Not giving a damn, about scepticism for example, is not knowledge-making, but it is knowledge-causing. In general, things that cause by double prevention do not make things happen, although later things are counterfactually dependent on them [#Lewis2004a]. And the same is true of not caring.

Finally, it has been argued that IRI makes knowledge unstable in a certain kind of way [#Lutz2014, Anderson2015]. Practical circumstances can change quickly; something can become a live choice and cease being one at a moment's notice. If knowledge is sensitive to what choices are live, then knowledge can change this quickly too. But, say the objectors, it is counterintuitive that knowledge changes this quickly.

Now I'm not sure this is counterintuitive. I think that part of what it takes to know _p_ is to treat the question of whether _p_ as closed. It sounds incoherent to say, _I know a is the F, but the question of who is the F is still open_. And whether a question is treated as open or closed does, I think, change quite rapidly. One can treat a question as closed, get some new reason to open it (perhaps new evidence, perhaps an interlocutor who treats it as open), and then quickly dismiss that reason. So I'm not sure this is even a problem.

But to the extent that it is, it is only a problem for a somewhat half-hearted version of IRI. The puzzles the objectors raise turn on cases where the relevant practical options change quickly. But even once a practical option has ceased to be available, it can be hard in practice to dismiss it from one's mind. One may often still think about what to do if it becomes available again, or about exactly how unfortunate it is that the option went away. As long as theoretical as well as practical interests matter to knowledge, it will be unlikely that knowledge will be unstable in just this way. Practical interests may change quickly; theoretical ones typically do not.

* Wrong kind of reasons
* Evidence against makes for knowledge (it always can)
* Instability problem from Grimm (might be same problem)
* Anderson on stability, also Lutz 2014, 1723 (intellectual helps)