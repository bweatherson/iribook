## Interests as Defeaters {#defeat}

Knowledge, unlike rational belief, requires a certain amount of internal coherence among mental states. Consider the following story from David Lewis:

> I speak from experience as the repository of a mildly inconsistent corpus. I used to think that Nassau Street ran roughly east-west; that the railroad nearby ran roughly north-south; and that the two were roughly parallel. [@Lewis1982c 436]

I think in that case that Lewis doesn't know that Nassau Street runs roughly east-west. (From here on, call the proposition that Nassau Street runs roughly east-west $N$.) If his belief that it does was acquired and sustained in a suitably reliable way, then he may well have a rational belief that $N$. But the lack of coherence with the rest of his cognitive system, I think, defeats any claim to knowledge he has.

Coherence isn't just a requirement on belief; other states can cohere or be incoherent. Assume Lewis corrects the incoherence in his beliefs, and
drops the belief that Nassau Street the railway are roughly parallel. Still, if Lewis believed that $N$, preferred doing $\varphi$ to doing $\psi$ conditional on $N$, but actually preferred doing $\psi$ to doing $\varphi$, his cognitive system would also be in tension. That tension could, I think, be sufficient to defeat a claim to know that $N$.

And it isn't just a requirement on actual states; it can be a requirement on rational states. Assume Lewis believed that $N$, preferred doing $\varphi$ to doing $\psi$ conditional on $N$, and preferred doing $\varphi$ to doing $\psi$, but should have preferred doing $\psi$ to doing $\varphi$ given his interests. Then I think the fact that the last preference is irrational, plus the fact that were it
corrected there would be incoherence in his cognitive states defeats the claim to know that $N$.

A concrete example of this helps make clear why such a view is attractive, and why it faces difficulties. Assume there is a bet that wins \$2 if $N$, and loses \$10 if not. Let $\varphi$ be taking that bet, and $\psi$ be declining it. Assume Lewis shouldn't take that bet; he doesnt have enough evidence to do so. Then he clearly doesn't know that $N$. If he knew that $N$, $\varphi$ would dominate $\psi$, and
hence be rational. But it isn't, so $N$ isn't known. And that's true whether Lewis's preferences between $\varphi$ and $\psi$ are rational or irrational.

Attentive readers will see where this is going. Change the bet so it wins a penny if $N$, and loses \$1,000 if not. Unless Lewis's evidence that $N$ is incredibly strong, he shouldn't take the bet. So, by the same reasoning, he doesn't know that $N$. And we're back saying that knowledge requires incredibly strong evidence. The solution, I say, is to put a pragmatic restriction on the kinds of incoherence that matter to knowledge. Incoherence with respect to irrelevant questions, such as whether to bet on $N$ at extremely long odds, doesn't matter for
knowledge. Incoherence (or coherence obtained only through irrationality) to relevant questions does. And this is one key way that interests matter to knowledge. The irrelevance of a question can mean that something that would be a defeater to knwwledge, incoherence between one's attitude to $p$ and one's answer to that question, is not a defeater. When the question becomes relevant, the incoherence again becomes a defeater. So knowledge is sensitive to practical considerations. 
interests.

The string of cases about Lewis and $N$ has ended up close to the Coraline example. We already concluded that Coraline didn't know $p$. Now we have a story about why - belief that $p$ doesn't cohere
sufficiently well with what she should believe, namely that it would be wrong to take the bet. If all that is correct, just one question
remains: does this coherence-based defeater also defeat Coraline's claim to have a rational belief that $p$? I say it does not, for three reasons.

First, her attitude towards $p$ tracks the evidence perfectly. She is making no mistakes with respect to $p$. She is making a mistake with respect to $q$, but not with respect to $p$. So her attitude towards
$p$, i.e. belief, is rational.

Second, talking about beliefs and talking about credences are simply two ways of modelling the very same things, namely minds. If the agent both has a credence 0.99 in $p$, and believes that $p$, these are not two different states. Rather, there is one state of the agent, and two different ways of modelling it. So it is implausible to apply different valuations to the state depending on which modelling tools we choose to use. That is, it's implausible to say that while we're modelling the
agent with credences, the state is rational, but when we change tools, and start using beliefs, the state is irrational. Given this outlook on
beliefs and credences, it is natural to say that her belief is irrational. 

It's natural but not (as Jeremy Fantl pointed out to me), compulsory. Natural, but not compulsory, for reasons Jeremy Fantl pointed
out to me.^[The following isn't Fantl's example, but I think it makes much the same point as the examples he suggested.] We don't want a metaphysics on which persons and philosophers are separate entities. Yet we can say that someone is a good person but a bad philosopher. Normative statuses can differ depending on which property of a thing we are considering. That suggests it is at least coherent to say that one and the same state is a good credence but a bad belief. But while this may be coherent, I don't see any particular motivation for it, and it is natural to have the evaluations go together.

Third, we don't *need* to say that Coraline's belief in $p$ is irrational in order to preserve other nice theories, in the way that we do need to say that she doesn't know $p$ in order to preserve a nice
account of how we understand decision tables. It's this last point that I think Fantl and McGrath, who say that the belief is irrational (and unjustified), would reject. So I'll conclude this chapter with a look at their arguments.
