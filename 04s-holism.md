## Atomism about Rational Belief

In the previous chapter I argued for two individually necessary and jointly sufficient conditions for belief. They are

1. In some possible decision problem, $p$ is taken for granted.
2. For every question the agent is interested in, the agent answers the question the same way (i.e., giving the same answer for the same reasons) whether the question is asked unconditionally or conditional on $p$.

At this point one might think that offering a theory of rational belief would be easy. It is rational to believe $p$ just in case it is rational to satisfy these conditions. Unfortunately, this nice thought can't be right. It certainly fails in one direction: it can be irrational to satisfy these conditions while rationally believing $p$. And it can arguably fail in the other direction: it can be rational to satisfy these conditions while being irrational to believe that $p$. The latter turns on some tricky questions about the nature of rationality that are independent of our interest in interests, so I'll put it off for a bit. The first case is more important.

Coraline is like Anisa and Chamari, in that she has read a reliable book saying that the Battle of Agincourt was in 1415. And she now believes that the Battle of Agincourt was indeed in 1415, for the very good reason that she read it in a reliable book. 

In front of her is a sealed envelope, and inside the envelope a number is written on a slip of paper. Let $X$ denote that number, non-rigidly. (So when I say Coraline believes $X = x$, it means she believes that the number written on the slip of paper is $x$, where $x$ rigidly denotes some number.) Coraline is offered the following bet:

* If she declines the bet, nothing happens.
* If she accepts the bet, and the Battle of Agincourt was in 1415, she wins $1.
* If she accepts the bet, and the Battle of Agincourt was not in 1415, she loses $X$ dollars.

For some reason, Coraline is convinced that $X = 10$. This is very strange, since she was shown the slip of paper just a few minutes ago, and it clearly showed that $X = 1,000,000,000$. Coraline wouldn't bet on when the Battle of Agincourt was at odds of a billion to one. But she would take that bet at 10 to 1, which is what she is faced with. Indeed, she doesn't even conceptualise it as a bet; it's a free dollar she thinks. Right now, she is disposed to treat the date of the battle as a given. She is disposed to lose this disposition should a very long odds bet depend on it. But she doesn't believe she is facing such a bet.

So Coraline accepts the bet; she thinks it is a free dollar. And that's when the battle took place, so she wins the dollar. All's well that end's well. But it was a really wildly irrational bet to take. You shouldn't bet at those odds on something you remember from a history book. Neither memory nor history books are that reliable. Coraline was not rational to treat the questions _Should I take this bet?_, and _Conditional on the Battle of Agincourt being in 1415, should I take this bet?_ the same way. Her treating them the same way was fortunate - she won a dollar - but irrational.

Yet it seems odd to say that Coraline's belief about the Battle of Agincourt was irrational. What was irrational was her belief about the envelope, not her belief about the battle. To say that a particular disposition was irrational is to make a holistic assessment of the person with the disposition. But whether a belief is rational or not is, relatively speaking, atomistic.

That suggests the following condition on rational belief.

> S's belief that $p$ is irrational if
> 
> 1. S irrationally has one of the dispositions that is characteristic of belief that $p$; and
> 2. What explains S having a disposition that is irrational in that way is her attitudes towards $p$, not (solely) her attitudes towards other propositions, or her skills in practical reasoning.

In "Knowledge, Bets and Interests I gave a similar theory about these cases - I said that S's belief that $p$ was irrational if the irrational dispositions were caused by an irrationally high credence in $p$. I mean this account to be ever so slightly more general. I'll come back to that below, because first I want to spell out the second clause.

Intuitively, Coraline's irrational acceptance of the belief is explained by her (irrational) belief about $X$, not her (rational) belief about the Battle of Agincourt. We can take this notion of explanation as a primitive if we like; it's in no worse philosophical shape than other notions we take as a primitive. But it is possible to spell it out a little more.

Coraline has a pattern of irrational dispositions related to the envelope. If you offer her $50 or $X$ dollars, she'll take the $50. If you change the bet so it isn't about Agincourt, but is instead about any other thing she has excellent but not quite conclusive evidence for, she'll still take the bet.

On the other hand, she does not have a pattern of irrational dispositions related to the Battle of Agincourt. She has this one, but if you change the payouts so they are not related to this particular envelope, then for all we have said so far, she won't do anything irrational.

That difference in patterns matters. We know that it's the beliefs about the envelope, and not the beliefs about the battle, that are explanatory because of this pattern. We could try and create a reductive analysis of explanation in clause 2 using facts about patterns, like the way Lewis tries to create a reductive analysis of causation using similar facts about patterns in _Causation as Influence_ (@Lewis2004a). But doing so would invariably run up against edge cases that would be more trouble to resolve than they are worth.

That's because there are ever so many ways in which someone could have an irrational disposition about any particular case. We can imagine Coraline having a rational belief about the envelope, but still taking the bet because of any of the following reasons:

- It has been her life goal to lose a billion dollars in a day, so taking the bet strictly dominates not taking it.
- She believes (irrationally) that anyone who loses a billion dollars in a day goes to heaven, and she (rationally) values heaven above any monetary amount.
- She consistently makes reasoning errors about billions, so the prospect of losing a billion dollars rarely triggers an awareness that she should reconsider things she normally takes for granted.

The last one of these is especially interesting. The picture of rational agency I have in the background here owes a lot to the notion of epistemic vigilance, as developed by Dan Sperber and co-authors [@SperberEtAl2010]. The rational agent will have all these beliefs in their head that they will drop when the costs of being wrong about them are too high, or the costs of re-opening inquiry into them are too low. They can't reason, at least in any conscious way, about whether to drop these beliefs, because to do that is, in some sense, to call the belief into doubt. And what's at issue is whether they should call the belief into doubt. So what they need is some kind of disposition to replace a belief that $p$ with an attitude that $p$ is highly probable, and this disposition should correlate with the cases where taking $p$ for granted will not maximise expected utility. This disposition will be a kind of vigilance. As Sperber et al show, we need some notion of vigilance to explain a lot of different aspects of epistemic evaluation, and I think it can be usefully pressed into service here.^[Kenneth @Boyd2015 suggests a somewhat similar role for vigilance in the course of defending an interest-invariant epistemic theory. Obviously I don't agree with his conclusions, but my use of Sperber's work does echo his.]

But if you need something like vigilance, then you have to allow that vigilance might fail. And maybe some irrational dispositions can be traced to that failure, and not to any propositional attitude the decider has. For example, if Coraline systematically fails to be vigilant when exactly one billion dollars is at stake, then we might want to say that her belief in $p$ is still rational, and she is practically, rather than theoretically, irrational. (Why could this happen? Perhaps she thinks of Dr Evil every time she hears the phrase "One billion dollars", and this distractor prevents her normally reliable skill of being vigilant from kicking in.)

If one tries to turn the vague talk of patterns of bets involving one proposition or another into a reductive analysis of when one particular belief is irrational, one will inevitably run into hard cases where a decider has multiple failures. We can't say that what makes Coraline's belief about the envelope, and not her belief about the battle, irrational is that if you replaced the envelope, she would invariably have a rational disposition. After all, she might have some other irrational belief about whatever we replace the envelope with. Or she might have some failure of practical reasoning, like a vigilance failure. Any kind of universal claim, like that it is only bets about the envelope that she gets wrong, won't do the job we need.

The best we can say is that is that there will be a pattern...

Talk of credences was not quite sensitive to this...

One way to get credences - assume away a certain kind of irrationality, use representation theorems...

Another way to get credences - assume away a different kind of irrationality (i.e., weird cross valuations), build up by parts...

None of these will quite work

But they are close, and they show us what we're basically looking for in limit cases

