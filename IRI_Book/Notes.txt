We know a lot about the world. And this is good, because knowledge is useful. But when we try to say how useful knowledge is, we immediately run into problems. For a natural way of spelling out how useful knowledge is seems to imply that in fact we know very little. We can see this with a short example.

F01 has won a small prize. The reward is that she gets to play a game with a chance of a monetary reward. When she starts the game on her computer, she will be shown two sentences. She has to pick one of the sentences, and say whether it is true or false. If she is right, she wins $50, and if she is wrong she wins nothing. She knows everything I've just described about the game, and nothing else relevant. So F01 starts the game, and the computer shows her these two sentences.

A. Two plus two equals four.
B. The Magna Carta was signed in 1215.

F01 has four choices: choose sentence A or sentence B, and then choose True or False. We'll represent her choices as AT, AF, BT, BF, where AT means choose A and say it is True, BF means choose B and say it is False, and so on. Now she has recently read a book on the Magna Carta, and is very confident that book said the Magna Carta was signed in 1215. But she is also very confident that two plus two is, indeed, four. In this situation, I think the following argument is sound.

1. It is irrational for F01 to choose BT; the uniquely rational choice is AT.
2. If F01 knows that the Magna Carta was signed in 1215, then it is rationally permissible for her to choose BT.
3. So, F01 does not know that the Magna Carta was signed in 1215.

And this kind of argument generalises. For any truth that could have appeared as sentence B, and for which the choice BT is irrational, F01 does not know that truth. This looks like a powerful style of sceptical argument. To be sure, it is not as sweeping as some kinds of sceptical argument. It does not threaten F01's knowledge that she exists, that three plus three is six, or that she weighs less than a double decker bus. But it does threaten her knowledge of all sorts of details about the world, including knowledge of many things that she would, in normal circumstances, unhesitatingly assent to.

There are two obvious ways to respond to the argument I just offered. One is to deny one or other premise. The other is to argue that the conclusion is not as bad as it seems. The purpose of this book is to present a version of the second option. F01 does not, at the time she considers whether to play AT or BT, know that the Magna Carta was signed in 1215. But this does not mean she knows very little. It means, instead, that what she knows is dependent on which questions she is considering. When she was not considering whether to play AT or BT, she did know that the Magna Carta was signed in 1215. But this knowledge could not survive the change of focus that the choice brings on.

I will be defending a version of an **interest-relative** epistemology. Such epistemologies have previously been presented by [#FantlMcGrath2002, FantlMcGrath2009;], [#Hawthorne2004;] and [#Stanley2005;], though my version will differ a fair bit in the details. I will be defending at some length the soundness of the argument that presented above, though I am sure that defense will be far from convincing. And I will defend the viability of interest-relative theories, and the particular way that I'm developing the interest-relative view. But I'm not really going to defend the view that adopting interest-relativity is the only way, or even the best way, to respond to the argument. That is not because I doubt there are such arguments, but rather because I have very little to add to existing arguments. So at various points I'll note ways in which we could avoid interest-relativity, and point to the relevant literature on why those ways are not promising. But I won't have much to add to them.

At first glance one might think that interest-relative epistemology has very little useful to add about F01's case. The bumper sticker version of interest-relative epistemology is that we know less in high stakes cases. And F01 is not in a high stakes case. Fifty dollars isn't nothing, but if we said that having fifty dollars on the line makes something high stakes, then we'd say many people are in high stakes cases most of the time. And that would somewhat undermine a key motivation for interest-relative epistemology, which is to say that we typically know a lot, even in light of the pragmatic argument for scepticism.

The version of interest-relative epistemology defended in this book does not say that stakes are directly relevant to what an agent knows. What matters is not the stakes in any gamble the agent must make, but the odds. Roughly, knowing that _p_ requires that the evidential probability that _p_ be high enough that any bet on _p_ has a positive expected value. And whether that's true depends on the odds on offer, not the stakes.

To be sure, in practice high stakes bets are often long odds bets. In practice, we can easily lose a lot, and it is hard to gain very much. A world where it was as easy to win $30,000 as it is to destroy a $30,000 car would be very unlike this one. And for most of us, the welfare loss involved in losing $30,000 would be much greater than the welfare gain from winning $30,000. So losses are more probable, and of greater significance, than gains. Those are both ways in which high stakes choices tend to be, in practice, long odds choices. So I'll end up saying the same thing about cases involving high stakes as traditional interest-relative theorists do. But I do it without giving any special role to stakes. And that's important, because the stakes-based version of interest-relative epistemology cannot explain what's going on in F01's case.

We can imagine the example of F01. F01 herself could imagine such a case too, even if she were not in it. Perhaps she knows that she'll get to play the game, but is not yet playing it, and is casually thinking about what to do in various version of the game. While idly pondering this, she think about what to do if A is that two plus two equals four, and B is that the Magna Carta was signed in 1215. In this little reverie, she should still think that what to do is AT, not BT. But this is a puzzle for interest-relative theories. Now the stakes have fallen from fifty dollars to zero; it's just an imagining. 

The solution is to say that interests include both practical and theoretical interests. F01 is, in this example, interested in the question of what to do when given the choice between AT and BT. And someone with her background, faced with that question, does not know when the Magna Carta was signed. It does not matter that the question has no practical significance for her. What one knows is sensitive to theoretical interests, like her theoretical interest in what to do when faced with this hypothetical choice, as well as practical interests.

In earlier work I had defended a part-time version of interest-relativity. In this book I defend a full-time version. There was, I thought, a rather nice motivation for the part-time version. But it also has trouble, and I now think the trouble is so severe that it overrides the motivation. Let's start with that motivation.

A compulsory question for any interest-relative epistemology is when does a change in interest lead to a change in knowledge. An only slightly less compulsory question is why it is that those changes in interests produce those changes in knowledge. A part-time interest-relative theory can offer a rather nice answer to those questions.

Assume that what evidence any agent has is independent of what their interests are. And assume that this makes it the case that the evidential probability of any proposition for any agent, of any proposition is independent of that agent's interests. Then we can use that evidential probability to craft constraints on knowledge. An agent only knows that _p_ if the correct answer to any question they are interested in is the same unconditionally as it is conditional on _p_. One question an agent might be interested in is whether to choose X or Y, where the agent should choose X if the evidential expected utility of X is higher than the evidential expected utility of Y. This restriction on knowledge must be interest-relative on pain of scepticism. For almost any proposition, there are cases where what the agent should do is different to what they should do conditional on that proposition. (The choice situation F01 faces can be used to generate these cases with ease.) And perhaps this is the only kind of interest-relativity we need. This restriction can explain all the puzzle cases that are used to motivate interest-relativity, so it seems plausible to posit that it is the only way in which knowledge is relative to interests.

If that explanation works, then we have answered both of the compulsory questions. We say when knowledge is lost because of a change of interests, and we say that when knowledge is lost, it is lost because the evidential probability is no longer sufficiently close to 1. So far, so good. That's why we might like a part-time interest-relative theory; it offers a plausible sounding answer to two compulsory questions facing the interest-relative theory.

But there is a bug. The explanation requires that there be an interest-neutral account of what the agent's evidence is. And there is a strong argument that evidence too must be interest-relative. If _E_ is part of the agent's evidence, then the evidential probability of _E_ is 1. So the right answer to any question will be the same conditional on _E_ as unconditionally. So interests can't stop the agent knowing _E_; if she knows it given some interests, she knows it given any interests.

Now consider a variant of F01's case. M01 is playing the same game. A few minutes ago he saw his old school friend M02, who he hadn't seen in decades. He was shocked; he thought M02 lived in another country. But there he was, clear as day. Now he plays the game, and gets the following two sentences come up.

A. Two plus two is four.
B. M02 is nearby.

It would be irrational to choose BT. It wouldn't hurt; M02 is nearby, so he'd get the fifty dollars. But it would be an unnecessary risk. He might have misremembered what M02 looks like, or confused him for a different school friend. It could just be someone who looks really a lot like M02. It's much safer to simply choose AT, and take the sure fifty dollars.

And now we have a problem. If M01 wasn't playing this game, he would clearly know that M02 was nearby. Indeed, that would be part of his evidence, since he literally just saw M02. But it would be irrational to choose BT in the game. That's because (we interest-relative theorists say) he doesn't know that M02 is nearby in the context of choosing what to play in the game. But if evidence is interest-neutral, and it would be part of his evidence in some contexts that M02 is nearby, it must be part of his evidence in all contexts that M02 is nearby. And that's inconsistent with him not knowing that M02 is nearby given just these interests.

We need a way out, and I doubt there is an available way out that keeps the idea of interest-neutral evidence. The two most obvious ways to do that lead to serious difficulties.

We could try saying that M01 really does know that M02 is nearby. The problem with that is that the arguments for F01 not knowing when the Magna Carta was signed seem to generalise pretty smoothly to M01's case. We'll see this in more detail when we look at the details of those arguments, but it seems that any principled reason we can give for F01 not having knowledge generalises to M01. So that route is unavailable, at least to the interest-relative theorist.

We could try saying that although in normal cases M01 knows that M02 is nearby, that isn't part of his evidence. The evidence is something much more fundamental. The problem is that we need to say what that more fundamental evidence could be. And when we say that, we have to meet two constraints. First, the evidence must consist entirely of propositions that are so certain that it would be rationally permissible to bet on them instead of betting that two plus two is four. Second, the evidence must collectively be strong enough to ground the kind of ordinary knowledge we take ourselves to have. I'm rather sceptical that there is any characterisation of evidence that can meet both constraints.

So I conclude that we need an interest-relative theory of evidence. In chapter 4 of this book, I'll offer such a theory. The basic idea is that  it is a constraint on a mechanism generating evidence that that mechanism be reliable. How reliable? That, I think, is interest-relative. In regular contexts, a fair degree of fallibility is consistent with the mechanism generating evidence in cases where it is (safely) correct. In distinctive long odds contexts, the needed degree of reliability shoots up. There is a lot more to say about just how this works, but the basic idea is to locate the interest-relativity inside the reliability constraint.

This complicates our answers to the compulsory questions. When does a change of interests produce a change of knowledge? The answer now is disjunctive. Sometimes it happens because a change of interest changes the degree of reliability that is associated with sources of evidence, so the agent has less evidence. And sometimes it happens because the evidential probability is not high enough given the choices the agent faces. That's not as neat as the answers offered by the part-time interest-relative theorist. But we get more cases right this way, and we avoid dividing epistemology into interest-neutral and interest-relative parts.

A lot of the argument in this book turns on the following conditional:

* If it is irrational for F01 to choose BT, then F01 does not know when the Magna Carta was signed.

We could write this, equivalently, as

* If F01 knows when the Magna Carta was signed, then it is not irrational for her to choose BT.

We'll primarily use the first version, but we'll primarily argue for the second version. There is a weaker and a stronger version of the argument for the conditional. The stronger version goes via this principle.

Knowledge is Given
:    If S is thinking about whether to do X, and she knows that _p_, and it is rational for her to do X given _p_, then it is rational for S to do X.

This is my preferred version of a principle linking knowledge and action. Other interest-relative theorists have similar principles, and the differences between them don't matter for present purposes.[^The most striking thing about **Knowledge is Given**, relative to the existing literature, is that it does not use the ideology of reasons. Its peers are deeply embedded within that ideology. Saying precisely what the differences are between **Knowledge is Given** and those peers would require saying something substantial about the relationship between reasons and rationality. And that would take us a long way from present concerns. So I'm just going to use **Knowledge is Given**, without placing it precisely within existing frameworks.] With that principle in hand, we can reason as follows.

1. Given that the Magna Carta was signed in 1215, playing BT gets F01 as high a return as she can get, and F01 knows that.
2. Any option that F01 knows gets as high a return as she can get is rational for F01 to play.
3. So, if F01 knows that the Magna Carta was signed in 1215, then by **Knowledge is Given**, it is rational for her to play BT.

And that argument seems sound, at least with the principle **Knowledge is Given** in hand. The strong argument that the conditional, then, is that the principle **Knowledge is Given** is true, and it quickly implies the conditional is also true. That's what I think is right. I think **Knowledge is Given** is true, and this argument is sound.

But I don't need anything quite that strong. There is also a weaker argument for the conditional. The weaker argument says, in effect, that if there are exceptions to **Knowledge is Given**, then they do not apply to F01. There are two types of exceptions that are sometimes suggested to principles like **Knowledge is Given**. I'll discuss these in much greater detail in later chapters, so for now a sketch will have to suffice.

One kind of exception says that **Knowledge is Given** does not hold in particularly high stakes situations. If the stakes are high enough, then even if X is rational given _p_, knowing _p_ might not be enough to make X rational. Maybe one has to be certain that _p_, or know that one knows that _p_, or something else. Since F01 isn't in a high stakes situation, such an exception is irrelevant. As long as **Knowledge is Given** holds in low stakes contexts, the argument goes through.

The other kind of exception is contrastive, not absolute. It starts from the following thought. Let's say an agent knows, but is not certain, that X will produce a return Z. And she is completely, maximally, certain that Y will produce that same return Z. She is also completely, maximally, certain that X will not produce any return greater than Z. Then maybe we can get an intuition that Y is preferably to X. After all, if you're not going to do better than Z, then it would be good to make it the case that one has the highest epistemic standing possible towards the proposition that one will get Z. Again, this isn't my view; I endorse **Knowledge is Given**. But there is the possibility of an intuitive counterexample here.

Again though, note that it wouldn't apply to F01's case. Let's say, to give the objector everything they could ask for, that F01 knows without being certain that the Magna Carta was signed in 1215. And let's say they are completely, maximally, certain that two plus two is four. It doesn't follow that they are completely, maximally, certain that playing AT will get them $50. After all, the only stipulation we made about the game setup was that F01 knows that if they play AT, and two plus two is four, then they will get $50. We didn't say this was certain. And we didn't seem to need to say that in order to get the intuition that only AT could be rationally chosen.

I'm relying here on an implicit closure assumption about knowledge. If F01 knows that the Magna Carta was signed in 1215, and she knows that if it was, then playing BT will get her $50, and if on that basis she believes that playing BT will get her $50, then she knows that playing BT will get her $50. Closure assumptions are not without controversy. They are brought into question by, for example, the preface paradox. (We'll come back to that paradox in chapter XXX.) A critic of closure could reason as follows:

* Certainty that _p_, plus knowledge of $p \rightarrow q$, produces knowledge of _q_.
* But knowledge that _p_, plus knowledge of $p \rightarrow q$, does not always produce knowledge of _q_.

And if you squint, you can sort of see a motivation for this. Assume knowledge is a less strong state than certainty. (This assumption I think is right.) Then in the second bullet point, the inputs, two pieces of knowledge, are weaker than they are in the first bullet point. But the output in the first bullet point could not be plausibly strengthened. So the output in the second bullet point should not be as strong as the output in the first bullet point.

We'll come back to this when we discuss the preface paradox, but for now I'll just note that I think this reasoning is flawed. It is central to how we think about knowledge that it is closed under competent deduction. So if F01 knows when the Magna Carta was signed, and what two plus two is, then she knows that she'll get $50 whether she chooses AT or BT. And we aren't told enough to say that she is certain that she will get $50 in either case. So there is no epistemic asymmetry between the two choices. Or, at least, if she knows when the Magna Carta was signed, there is no such asymmetry. So the case doesn't fall into an exception to **Knowledge is Given** for that reason either.

As I said, I am going to defend the unqualified principle **Knowledge is Given** in this book. But it is important to the overall dialectic to note that I don't need it to motivate interest-relativity. Even an exception-ridden version of **Knowledge is Given** implies a lot of interest-relativity. That should matter to people who think that it is plausible that something like **Knowledge is Given** is true, and implausible that knowledge is interest-relative. They might think that the right move next is to qualify **Knowledge is Given** so as to save interest-neutrality. That won't work; the needed qualifications will have to be too great.

The epistemology I'm defending is interest-relative. But the interests don't come in at the places you might expect. They are buried fairly deep in the weeds. They always make an indirect impact. And they are always answers to questions about how we related gradable to binary notions. I think epistemology is full of places where that kind of relation needs to be understood.

1. An agent only believes _p_ if they are confident enough that it is true.
2. An agent only knows _p_ if they believe it on the basis of strong enough evidence.
3. An information source only provides evidence if it is reliable enough.

In all three of those claims, a gradable notion is related to a binary one. And in every case, the claim is that the binary notion only obtains if the value of some variable is high enough. [^I think that these three claims are not only true, but explanatory; what comes after the 'only if' explains what is at the start of the sentence. We will return to this point in chapter 5.] That term 'enough' is not exactly transparent. What is enough reliability, enough confidence, enough support? I think the best way to understand 'enough' here is as interest-relative. Indeed, I think knowledge is interest-relative because, and only because, questions about how much is enough in these cases get interest-relative answers. Other answers, I'll argue as this book proceeds, lead to either arbitrariness or scepticism. 

At one point [add citation] Jason Stanley says that interest-relativity implies that we need to add a condition to the familiar accounts of knowledge. I don't think that's right. Indeed, I don't think that's desirable. What we really need to do is to interpret the familiar conditions in interest-relative ways. Since the familiar conditions involved using gradable notions to determine binary conditions, there was always some flexibility in how they would be interpreted. I propose that we systematically resolve that flexibility in interest-relative ways.

So I think interest-relativity comes into epistemology at three points. The next three chapters of this book will defend the interest-relative interpretation of 'enough' in the three claims above. First we'll look at belief, then at knowledge, then finally at evidence. We're going to focus on these three questions.

1. What is the relationship between belief and credence?
2. What makes something a good representation of a choice situation?
3. How reliable must a source be to count as providing evidence?

I'll defend interest-relative answers to the first and third questions. Someone believes something only if their credence is close enough to 1 that it doesn't matter to any question which is relevant given their interests. A source is reliable enough if the gain in knowledge one gets from adding it is worth the risk of believing false things on the basis of it. And whether that trade off is worthwhile depends on the agent's interests. The fuller answer to the second question will have **Knowledge is Given** fall out as a consequence, which I've argued above implies interest-relativity. So interest-relativity will be involved directly or indirectly in all three answers.
