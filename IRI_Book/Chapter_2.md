## Belief and Credence

In epistemology we all used to talk about beliefs and their descriptive and normative features. Now we often talk about degrees of belief, or credences, and their descriptive and normative properties. What is the relationship between the traditional and the new ways of speaking. Do we have two subject matters here - credences and beliefs - or two descriptions of the one subject matter? And if it is just one subject matter, what relationship is there between the two descriptions of this subject matter?

The first question is straightforward. The mind does not have two representation systems, one consisting of absolute beliefs, and the other consisting of probabilities, running in tandem. The mind probably does contain a vast plurality of representational systems, but there are not the kind of distinct systems that a metaphysical separation between belief and credence would require. If there were two distinct systems, then we should imagine that they could vary independently, at least as much as is allowed by constitutive rationality. But such variation is hard to fathom. So I'll infer that the one representational system accounts for our credences and our categorical beliefs.[^It follows from this that the question [Bovens1999;] ask, namely what beliefs should an agent have given her degrees of belief, doesn't have a non-trivial answer. If fixing the degrees of belief in an environment fixes all her doxastic attitudes, as I think it does, then there is no further question of what she should believe given these are her degrees of belief.)]

The second question is much harder. A number of authors have defended what has become known as the Lockean thesis. This says that $S$ believes that $p$ iff S's credence in $p$ is greater than some salient number $r$, where $r$ is made salient either by the context of belief ascription, or the context that $S$ is in. There are two well-known problems with the Lockean view, both of which seem fatal to me. I'll go over them, then introduce a new problem.

As Robert [91][#Stalnaker1984;] emphasised, any number $r$ is bound to seem arbitrary. Unless these numbers are made salient by the environment, there is no special difference between believing $p$ to degree 0.9786 and believing it to degree 0.9875. But if $r$ is 0.98755, this will be the difference between believing $p$ and not believing it. And that's a very important difference. The usual response to this, as found in [Ch. 4][#Foley1993;] and [#Hunter1996;] is to say that the boundary is vague. But it's not clear how this helps. On an epistemic theory of vagueness, there is still a number such that degrees of belief above that count, and degrees below that do not, and any such number is bound to seem unimportant. On supervaluational theories, the same is true. There won't be a determinate number, to be sure, but there will a number, and that seems false. My preferred theory of vagueness, as set out in [#Weatherson2005-WEATTT;] has the same consequence. Hunter defends a version of the threshold view combined with a theory of vagueness based around fuzzy logic, which seems to be the only theory that could avoid the arbitrariness objection. But as Timothy [#Williamson1994-WILV] showed, there are deep and probably insurmountable difficulties with that position. So I think the vagueness response to the arbitrariness objection is (a) the only prima facie plausible response and (b) unsuccessful. 

## Closure and Lockeans

The second problem for the Lockean View concerns conjunction. It is also set out clearly by Stalnaker.

> Reasoning in this way from accepted premises to their deductive consequences ($P$, also $Q$, therefore $R$) does seem perfectly straightforward. Someone may object to one of the premises, or to the validity of the argument, but one could not intelligibly agree that the premises are each acceptable and the argument valid, while objecting to the acceptability of the conclusion. [92][#Stalnaker1984]

If categorical belief is having a credence above the threshold, then one can coherently do exactly this. Let $x$ be a number between $r$ and than $r^{\nicefrac{1}{2}}$, such that  an atom of type U has probability $x$ of decaying within a time $t$, for some $t$ and U. Assume our agent knows this fact, and is faced with two (isolated) atoms of U. Let $p$ be that the first decays within $t$, and $q$ be that the second decays within $t$. She should, given her evidence, believe $p$ to degree $x, q$ to degree $x$, and $p \wedge q$ to degree $x^2$. If she believed $p \wedge q$ to a degree greater than $r$, she'd have to either have credences that were not supported by her evidence, or credences that were incoherent. (Or, most likely, both.) So this theory violates the platitude. 

The best response to this argument, in my view, attacks the platitude directly. It says that the preface paradox provides a reason for doubting that beliefs must be closed under entailment, or even must be consistent. David [#Christensen2005;] provides a particularly vivid description of the paradox.

> We are to suppose that an apparently rational person has written a long non-fiction book---say, on history. The body of the book, as is typical, contains a large number of assertions. The author is highly confident in each of these assertions; moreover, she has no hesitation in making them unqualifiedly, and would describe herself (and be described by others) as believing each of the book's many claims. But she knows enough about the difficulties of historical scholarship to realize that it is almost inevitable that at least a few of the claims she makes in the book are mistaken. She modestly acknowledges this in her preface, by saying that she believes the book will be found to contain some errors, and she graciously invites those who discover the errors to set her straight. [33-4][#Christensen2005]

Christensen thinks such an author might be rational in every one of her beliefs, even though these are all inconsistent. Although he does not say this, nothing in his discussion suggests that he is using the irrelevance of some of the propositions in the author's defence. So here is an argument that we should abandon closure amongst relevant beliefs.

Christensen's discussion, like other discussions of the preface paradox, makes frequent use of the fact that examples like these are quite common. We don't have to go to fake barn country to find a counterexample to closure. But it seems to me that we need two quite strong idealisations in order to get a real counterexample here.

The first of these is discussed Ishani [#MaitraANG;], and is briefly mentioned by Christensen in setting out the problem. We only have a counterexample to closure if the author believes every thing she writes in her book. (Indeed, we only have a counterexample if she reasonably believes every one of them. But we'll assume a rational author who only believes what she ought to believe.) This seems unlikely to be true to me. An author of a historical book is like a detective who, when asked to put forward her best guess about what explains the evidence, says ''If I had to guess, I'd say ...'' and then launches into spelling out her hypothesis. It seems clear that she need not believe the truth of her hypothesis. If she did that, she could not later learn it was true, because you can't learn the truth of something you already believe. And she wouldn't put any effort into investigating alternative suspects. But she can come to learn her hypothesis was true, and it would be rational to investigate other suspects. It seems to me (following here Maitra's discussion) that we should understand scholarly assertions as being governed by the same kind of rules that govern detectives making the kind of speech being contemplated here. And those rules don't require that the speaker believe the things they say without qualification. The picture is that the little prelude the detective explicitly says is implicit in all scholarly work.

There are three objections I know to this picture, none of them particularly conclusive. First, Christensen says that the author doesn't qualify their assertions. But neither does our detective qualify most individual sentences. Second, Christensen says that most people would describe our author as believing her assertions. But it is also natural to describe our detective as believing the things she says in her speech. It's natural to say things like ''She thinks it was the butler, with the lead pipe,'' in reporting her hypothesis. Third, [#Williamson2000-WILKAI;] has argued that if speakers don't believe what they say, we won't have an explanation of why Moore's paradoxical sentences, like ''The butler did it, but I don't believe the butler did it,'' are always defective. Whatever the explanation of the paradoxicality of these sentences might be, the alleged requirement that speakers believe what they say can't be it. For our detective cannot properly say ''The butler did it, but I don't believe the butler did it'' in setting out her hypothesis, even though believing the butler did it is not necessary for her to say ''The butler did it'' in setting out just that hypothesis.

It is plausible that for some kinds of books, the author should only say things they believe. This is probably true for travel guides, for example. Interestingly, casual observation suggests that authors of such books are much less likely to write modest prefaces. This makes some sense if those books can only include statements their authors believe, and the authors believe the conjunctions of what they believe.

The second idealisation is stressed by Simon [#Evnine1999;]. The following situation does not involve me believing anything inconsistent. 

1. I believe that what Manny just said, whatever it was, is false. 
2. Manny just said that the stands at Fenway Park are green. 
3. I believe that the stands at Fenway Park are green. 

If we read the first claim de dicto, that I believe that Manny just said something false, then there is no inconsistency. (Unless I also believe that what Manny just said was that the stands in Fenway Park are green.) But if we read it de re, that the thing Manny just said is one of the things I believe to be false, then the situation does involve me being inconsistent. The same is true when the author believes that one of the things she says in her book is mistaken. If we understand what she says de dicto, there is no contradiction in her beliefs. It has to be understood de re before we get a logical problem. And the fact is that most authors do not have de re attitudes towards the claims made in their book. Most authors don't even remember everything that's in their books. (I certainly don't.) Some may argue that authors don't even have the capacity to consider a proposition as long and complicated as the conjunction of all the claims in their book. Christensen considers this objection, but says it isn't a serious problem.

> It is undoubtedly true that ordinary humans cannot entertain book-length conjunctions. But surely, agents who do not share this fairly superficial limitation are easily conceived. And it seems just as wrong to say of such agents that they are rationally required to believe in the inerrancy of the books they write. [38][#Christensen2005]

I'm not sure this is undoubtedly true; it isn't clear that propositions (as opposed to their representations) have lengths. And humans can believe propositions that can be represented by sentences as long as books. But even without that point, Christensen is right that there is an idealisation here, since ordinary humans do not know exactly what is in a given book, and hence don't have de re attitudes towards the propositions expressed in the book.

I'm actually rather suspicious of the intuition that Christensen is pushing here, that idealising in this way doesn't change intuitions about the case. The preface paradox gets a lot of its (apparent) force from intuitions about what attitude we should have towards real books. Once we make it clear that the real life cases are not relevant to the paradox, the intuitions about it become rather murky. 

This point matters because it gets at just what the intuitions about closure under known entailment are. Consider again Stalnaker's careful statement of the point.

> Reasoning in this way from accepted premises to their deductive consequences ($P$, also $Q$, therefore $R$) does seem perfectly straightforward. Someone may object to one of the premises, or to the validity of the argument, but one could not intelligibly agree that the premises are each acceptable and the argument valid, while objecting to the acceptability of the conclusion. [92][#Stalnaker1984]

And note how active the verbs in Stalnaker's description are. When faced with a valid argument we have to _object_ to one of the premises, or to the validity of the argument. What we can't do is _agree_ to the premises and the validity of the argument, while _objecting_ to the conclusion. I agree. If we are really agreeing to some propositions, and objecting to others, in those cases deductive coherence feels mandatory. What is less obvious is whether deductive coherence between propositions in the background is mandatory. Indeed, I'm going to develop a theory where rational agents can accept each member of an inconsistent set of propositions. And they can use each member of that set, at different times, as parts of a scaffolding in their reasoning. What I insist they cannot do is accept each member while each member is salient and relevant to their current purposes. The preface paradox does not give us any reason to reject that principle; it is literally never true that every proposition endorsed in a long book is relevant and reasonably accepted.

I'm not sure whether it is ever true that every proposition in a long book is salient and relevant to one's current purposes. If one is in such an odd situation, I think one should not believe each of the propositions unless one is willing to endorse their conjunction. If one is not willing to accept the conjunction, one should merely say that some of the individual propositions are probably true.

Proponents of the preface paradox know that this is a possible response, and tend to argue that it is impractical. Here is Christensen on this point.

> It is clear that our everyday binary way of talking about beliefs has immense practical advantages over a system which insisted on some more fine-grained reporting of degrees of confidence ... At a minimum, talking about people as believing, disbelieving, or withholding belief has at least as much point as do many of the imprecise ways we have of talking about things that can be described more precisely. [96][#Christensen2005]

Richard Foley makes a similar point.

> There are deep reasons for wanting an epistemology of beliefs, reasons that epistemologies of degrees of belief by their very nature cannot possibly accommodate. [170][#Foley1993]

It's easy to make too much of this point. It's a lot easier to triage propositions into TRUE, FALSE and NOT SURE and work with those categories than it is to work assign precise numerical probabilities to each proposition. But these are not the only options. Foley's discussion subsequent to the above quote sometimes suggests they are, especially when he contrasts the triage with ''indicat[ing] as accurately as I can my degree of confidence in each assertion that I defend.'' (171) But really it isn't much harder to add two more categories, PROBABLY TRUE and PROBABLY FALSE to those three, and work with that five-way division rather than a three-way division.[^Consider those surveys that present a bunch of propositions and ask whether you strongly agree, somewhat agree, neither agree nor disagree, somewhat disagree, or strongly disagree. They are asking you to sort propositions into five bundles. And while this is harder than a three-way sort, it's not that much harder. And it's much easier than stating a precise probability for each.] It's not even clear that humans as they are actually constructed have a strong preference for the three-way over the five-way division. Even if they do, I'm not sure in what sense this is a 'deep' fact about them.

Once we have the five-way division, it is clear what authors should do if they want to respect closure. For any conjunction that they don't believe (i.e. classify as true), they should not believe one of the conjuncts. But of course they can classify every conjunct as probably true, even if they think the conjunction is false, or even certainly false. It is an idealisation to say rational authors should make this five-way distinction amongst propositions they consider. But it's no more of an idealisation than we need to set up the preface paradox in the first place. To use the preface paradox to find an example of someone who reasonably violates closure, we need to insist on the following three constraints.

1. They are part of a research community where only asserting propositions you believe is compatible with active scholarship;
2. They know exactly what is in their book, so they are able to believe that one of the propositions in the book is mistaken, where this is understood de re; but
3. They are unable to effectively function if they have to effect a five-way, rather than a three-way, division amongst the propositions they consider.

Put more graphically, to motivate the preface paradox we have to think that our inability to have de re thoughts about the contents of books is a ''superficial constraint'', but our preference for working with a three-way rather than a five-way division is a ''deep'' fact about our cognitive system. Maybe each of these attitudes could be plausible taken on its own (though I'm sceptical of that) but the conjunction is not plausible. Such an agent is so strange that norms about them have no relevance to norms for humans.

That said, there is something important about Foley's point that we need beliefs and not just degrees of belief. I think the best way to see why we have that need goes via thinking about games and decisions. And we'll start that investigation by looking at a different problem for the Lockean view.

## Playing Games with a Lockean

The Lockean theory is that to believe something is to have a credence in it above some threshold. That implies that having a credence in some proposition of one implies believing it. That's because it implies that for any two propositions that the agent has the  same credence in, she believes both or neither. And there are some propositions, such as that two plus two is four, that the agent has credence one in and believes. I'm going to argue that in a particular quite simple game, rational agents have credence one in some propositions that they do not believe.

The game itself is a nice illustration of how a number of distinct solution concepts in game theory come apart. Indeed, the use I'll make of it isn't a million miles from the use that [#KohlbergMertens1986;] make of it. 

To set the problem up, I need to say a few words about how I think of game theory. This won't be at all original - most of what I say is taken from important works by Robert [#Stalnaker1994, Stalnaker1996, Stalnaker1998, Stalnaker1999;]. But it is different to what I used to think, and perhaps to what some other people think too, so I'll set it out slowly.[^I'm grateful to the participants in a game theory seminar at Arché in 2011, especially Josh Dever and Levi Spectre, for very helpful discussions that helped me see through my previous confusions.]

Start with a simple decision problem, where the agent has a choice between two acts $A_1$ and $A_2$, and there are two possible states of the world, $S_1$ and $S_2$, and the agent knows the payouts for each act-state pair are given by the following able.

|      | $S_1$  | $S_2 |  
| ---: | :----:	| :----:	|  
| $A_1$ | 4 | 0 |  
| $A_2$ | 1 | 1 |  

What to do? I hope you share the intuition that it is radically underdetermined by the information I've given you so far. If $S_2$ is much more probable than $S_1$, then $A_2$ should be chosen; otherwise $A_1$ should be chosen. But I haven't said anything about the relative probability of those two states. Now compare that to a simple game. Row has two choices, which I'll call $A_1$ and $A_2$. Column also has two choices, which I'll call $S_1$ and $S_2$. It is common knowledge that each player is rational, and that the payouts for the pairs of choices are given in the following table. (As always, Row's payouts are given first.)

|      | $S_1$  | $S_2 |  
| ---: | :----:	| :----:	|  
| $A_1$ | 4,0 | 0,1 |  
| $A_2$ | 1,0 | 1,1 |  

What should Row do? This one is easy. Column gets 1 for sure if she plays $S_2$, and 0 for sure if she plays $S_1$. So she'll play $S_2$. And given that she's playing $S_2$, it is best for Row to play $A_2$.

You probably noticed that the game is just a version of the decision problem that we discussed a couple of paragraphs ago. The relevant states of the world are choices of Column. But that's fine; we didn't say in setting out the decision problem what constituted the states $S_1$ and $S_2$. And note that we solved the problem without explicitly saying anything about probabilities. What we added was some information about Column's payouts, and the fact that Column is rational. From there we deduced something about Column's play, namely that she would play $S_2$. And from that we concluded what Row should do.

There's something quite general about this example. What's distinctive about game theory isn't that it involves any special kinds of decision making. Once we get the probabilities of each move by the other player, what's left is (mostly) expected utility maximisation. (We'll come back to whether the 'mostly' qualification is needed below.) The distinctive thing about game theory is that the probabilities aren't specified in the setup of the game; rather, they are solved for. Apart from special cases, such as where one option strictly dominates another, we can't say much about a decision problem with unspecified probabilities. But we can and do say a lot about games where the setup of the game doesn't specify the probabilities, because we can solve for them given the other information we have.

This way of thinking about games makes the description of game theory as 'interactive epistemology' [#Aumann1999] rather apt. The theorist's work is to solve for what a rational agent should think other rational agents in the game should do. From this perspective, it isn't surprising that game theory will make heavy use of equilibrium concepts. In solving a game, we must deploy a theory of rationality, and attribute that theory to rational actors in the game itself. In effect, we are treating rationality as something of an unknown, but one that occurs in every equation we have to work with. Not surprisingly, there are going to be multiple solutions to the puzzles we face.

This way of thinking lends itself to an epistemological interpretation of one of the most puzzling concepts in game theory, the mixed strategy. Arguably the core solution concept in game theory is the Nash equilibrium. A set of moves is a Nash equilibrium if no player can improve their outcome by deviating from the equilibrium, conditional on no other player deviating. In many simple games, the only Nash equilibria involve mixed strategies. Here's one simple example.

|      | $S_1$  | $S_2 |  
| ---: | :----:	| :----:	|  
| $A_1$ | 0,1 | 10,0 |  
| $A_2$ | 9,0 | -1,1 |  

This game is reminiscent of some puzzles that have been much discussed in the decision theory literature, namely asymmetric Death in Damascus puzzles. Here Column wants herself and Row to make the 'same' choice, i.e., $A_1$ and $S_1$ or $A_2$ and $S_2$. She gets 1 if they do, 0 otherwise. And Row wants them to make different choices, and gets 10 if they do. Row also dislikes playing $A_2$, and this costs her 1 whatever else happens. It isn't too hard to prove that the only Nash equilibrium for this game is that Row plays a mixed strategy playing both $A_1$ and $A_2$ with probability $\nicefrac{1}{2}$, while Column plays the mixed strategy that gives $S_1$ probability $\nicefrac{11}{20}$, and $S_2$ with probability $\nicefrac{9}{20}$.

Now what is a mixed strategy? It is easy enough to take away form the standard game theory textbooks a metaphysical interpretation of what a mixed strategy is. Here, for instance, is the paragraph introducing mixed strategies in Dixit and Skeath's \textit{Games of Strategy}.

> When players choose to act unsystematically, they pick from among their pure strategies in some random way ... We call a random mixture between these two pure strategies a mixed strategy. [186][#DixitSkeath2004]

Dixit and Skeath are saying that it is definitive of a mixed strategy that players use some kind of randomisation device to pick their plays on any particular run of a game. That is, the probabilities in a mixed strategy must be in the world; they must go into the players' choice of play. That's one way, the paradigm way really, that we can think of mixed strategies metaphysically.

But the understanding of game theory as interactive epistemology naturally suggests an epistemological interpretation of mixed strategies. 

> One could easily ... [model players] ... turning the choice over to a randomizing device, but while it might be harmless to permit this, players satisfying the cognitive idealizations that game theory and decision theory make could have no motive for playing a mixed strategy. So how are we to understand Nash equilibrium in model theoretic terms as a solution concept? We should follow the suggestion of Bayesian game theorists, interpreting mixed strategy profiles as representations, not of players' choices, but of their beliefs. [57-8][#Stalnaker1994]

One nice advantage of the epistemological interpretation, as noted by [185][#Binmore2007;] is that we don't require players to have $n$-sided dice in their satchels, for every $n$, every time they play a game.[^Actually, I guess it is worse than if some games have the only equilibria involving mixed strategies with irrational probabilities. And it might be noted that Binmore's introduction of mixed strategies, on page 44 of [#Binmore2007], sounds much more like the metaphysical interpretation. But I think the later discussion is meant to indicate that this is just a heuristic introduction; the epistemological interpretation is the correct one.] But another advantage is that it lets us make sense of the difference between playing a pure strategy and playing a mixed strategy where one of the 'parts' of the mixture is played with probability one. 

With that in mind, consider the below game, which I'll call GRG.[^Note that I'm here disagreeing with my treatment of this game in [#Weatherson2012-WEAGAT].] Informally, in this game $A$ and $B$ must each play either a green or red card. I will capitalise $A$'s moves, i.e., $A$ can play GREEN or RED, and italicise $B$'s moves, i.e., $B$ can play _green_ or _red_. If two green cards, or one green card and one red card are played, each player gets \$1. If two red cards are played, each gets nothing. Each cares just about their own wealth, so getting \$1 is worth 1 util. All of this is common knowledge. More formally, here is the game table, with $A$ on the row and $B$ on the column.

|         | _green_ | _red_ |  
|  -----:	| ------	| ------	|  
| GREEN | 1, 1 | 1, 1 |  
| RED | 1, 1 | 0, 0 |  

Assume that the players know these are the payouts, and they know each other to be rational, and these pieces of knowledge are common knowledge to at least as many iterations as needed to solve the game. (I'll argue in chapter 3 that's how game tables should by default be interpreted.) With that in mind, let's think about how the agents should approach this game.

I'm going to make one big simplifying assumption at first. We'll relax this later, but it will help the discussion to start with this assumption. This assumption is that the doctrine of **Uniqueness** applies here; there is precisely one rational credence to have in any salient proposition about how the game will play. Some philosophers think that Uniqueness always holds [#White2005-WHIEP]. I join with those such as [#North2010;] and [#Schoenfield2013;] who don't. But all I'm doing is assuming that it does hold here.

The first thing to note about the game is that it is symmetric. So the probability of $A$ playing GREEN should be the same as the probability of $B$ playing _green_, since $A$ and $B$ face exactly the same problem. Call this common probability $x$. If $x < 1$, we get a quick contradiction. The expected value, to Row, of GREEN, is 1. Indeed, the known value of GREEN is 1. If the probability of _green_ is $x$, then the expected value of RED is $x$. So if $x < 1$, and Row is rational, she'll definitely play GREEN. But that's inconsistent with the claim that $x < 1$, since that means that it isn't definite that Row will play GREEN.

So we can conclude that $x = 1$. Does that mean we can know that Row will play GREEN? No. Assume we could conclude that. Whatever reason we would have for concluding that would be a reason for any rational person to conclude that Column will play _green_. Since any rational person can conclude this, Row can conclude it. So Row knows that she'll get 1 whether she plays GREEN or RED. But then she should be indifferent between playing GREEN and RED. And if we know she's indifferent between playing GREEN and RED, and our only evidence for what she'll play is that she's a rational player who'll maximise her returns, then we can't be in a position to know she'll play GREEN.

I think the arguments of the last two paragraphs are sound. We'll turn to an objection presently, but let's note how bizarre is the conclusion we've reached. One argument has shown that it could not be more probable that Row will play GREEN. A second argument has shown that we can't know that Row will play GREEN. It reminds me of examples involving blindspots \citep{Sorensen1988}. Consider this case:

> (B): Brian does not know (B)

That's true, as a quick argument shows. Assume it's false, so I do know (B). Knowledge is factive, so (B) is true. But that contradicts the assumption that it's false. So it's true. But I obviously don't know that it's true; that's what this very true proposition says.[^It's received wisdom in philosophy that one can never properly say something of the form _p, but I don't know that p_. This is used as a data point in views as far removed from each other as those defended in [#Heal1994;] and [#Williamson1996-WILKAA;]. But I don't feel the force of this alleged datum at all, and (B) is just one reason. For a different kind of case that makes the same point, see [#MaitraWeatherson;].]

Now I'm not going to rest anything on this case, because there are so many tricky things one can say about blindspots, and about the paradoxes generally. It does suggest that there are other cases where one can properly have maximal credence in a true proposition without knowledge.[^We will come back to (B) a little when we are talking about 'Gettier cases'. Presumably (B) is a Gettier case, since I have a justified true belief that it is true, but don't know it is true. Yet it doesn't look much like traditional Gettier cases. This is one reason, among many, to suspect that Gettier cases do not form a natural kind.] [#Williamson201x;] has argued that in certain infintary cases it is possible to maximally have rational credence without the possibility of knowledge. Now every one of these cases, my game theoretic case, the blindspot case, or the infinitary case, is controversial. But it is striking how different the cases are. There are a lot of rather different reasons to think that maximal credence does not entail even the possibility of knowledge.

Let's return to an objection to my argument about GRG. The objection is that I'm wrong to assume that agents will only maximise expected utility. They may have tie-breaker rules, and those rules might undermine the arguments I gave above. The assumption is that there's a uniquely rational credence to have in any given situation.

I argued that if we knew that $A$ would play GREEN, we could show that $A$ had no reason to play GREEN. But actually what we showed was that the expected utility of playing GREEN would be the same as playing RED. Perhaps $A$ has a reason to play GREEN, namely that GREEN weakly dominates RED. After all, there's one possibility on the table where GREEN does better than RED, and none where RED does better. And perhaps that's a reason, even if it isn't a reason that expected utility considerations are sensitive to.

Now I don't want to insist on expected utility maximisation as the only rule for rational decision making. Sometimes, I think some kind of tie-breaker procedure is part of rationality. In the papers by Stalnaker I mentioned above, he often appeals to this kind of weak dominance reasoning to resolve various hard cases. But I don't think weak dominance provides a reason to play GREEN in this particular case. When Stalnaker says that agents should use weak dominance reasoning, it is always in the context of games where the agents' attitude towards the game matrix is different to their attitude towards each other. One case that Stalnaker discusses in detail is where the game table is common knowledge, but there is merely common (justified, true) belief in common rationality. Given such a difference in attitudes, it does seem there's a good sense in which the most salient departure from equilibrium will be one in which the players end up somewhere else on the table. And given that, weak dominance reasoning seems appropriate.

But that's not what we've got here. Assuming that rationality requires playing GREEN/_green_, the players know we'll end up in the top left corner of the table. There's no chance that we'll end up elsewhere. Or, perhaps better, there is just as much chance we'll end up 'off the table', as that we'll end up in a non-equilibrium point on the table. To make this more vivid, consider the 'possibility' that $B$ will play _blue_, and if $B$ plays _blue_, $A$ will receive 2 if she plays RED, and -1 if she plays GREEN. Well hold on, you might think, didn't I say that _green_ and _red_ were the only options, and this was common knowledge? Well, yes, I did, but if the exercise is to consider what would happen if something the agent knows to be true doesn't obtain, then the possibility that one agent will play blue certainly seems like one worth considering. It is, after all, a metaphysical possibility. And if we take it seriously, then it isn't true that under any possible play of the game, GREEN does better than RED.

We can put this as a dilemma. Assume, for reductio, that GREEN/_green_ is the only rational play. Then if we restrict our attention to possibilities that are epistemically open to $A$, then GREEN does just as well as RED; they both get 1 in every possibility. If we allow possibilities that are epistemically closed to $A$, then the possibility where $B$ plays _blue_ is just as relevant as the possibility that $B$ is irrational. After all, we stipulated that this is a case where rationality is common knowledge. In neither case does the weak dominance reasoning get any purchase.

With that in mind, we can see why we don't need the assumption of Uniqueness. Let's play through how a failure of Uniqueness could undermine the argument. Assume, again for reductio, that we have credence $\varepsilon > 0$ that $A$ will play RED. Since $A$ maximises expected utility, that means $A$ must have credence 1 that $B$ will play _green_. But this is already odd. Even if you think people can have different reactions to the same evidence, it is odd to think that one rational agent could regard a possibility as infinitely less likely than another, given isomorphic evidence. And that's not all of the problems. Even if $A$ has credence 1 that $B$ will play _green_, it isn't obvious that playing RED is rational. After all, relative to the space of epistemic possibilities, GREEN weakly dominates RED. Remember that we're no longer assuming that it can be known what $A$ or $B$ will play. So even without Uniqueness, there are two reasons to think that it is wrong to have credence $\varepsilon > 0$ that $A$ will play RED. So we've still shown that credence 1 doesn't imply knowledge, and since the proof is known to us, and full belief is incompatible with knowing that you can't know, this is a case where credence 1 doesn't imply full belief. So whether $A$ plays GREEN, like whether the coin will ever land tails, is a case the Lockean cannot get right. No matter where they set the threshold for belief our credence is above that threshold, but we don't believe. 

So I think this case is a real problem for a Lockean view about the relationship between credence and belief. If $A$ is rational, she can have credence 1 that $B$ will play _green_, but won't believe that $B$ will play _green_. Now I should acknowledge that this case is hard; when I originally developed an interest-relative theory of belief I got it wrong. But the full interest-relative theory gets it right. Let's build up to that theory in stages.

## Belief and Update

To believe something is to treat it as given. There are a number of ways of cashing out that intuition, but the following two are the most important to us.

1. If someone believes something, then learning that thing is true (and nothing else) won't change their attitudes.
2. If someone believes something, then they will be prepared to treat it as a fixed point in reasoning.

Those two principles are going to be the basis of the theory of belief I'll develop. I'll see how far we can get relying on the first principle, then use the second to deal with some cases that hadn't been correctly classified the first time around.

We'll start with the functionalist idea that to believe that $p$ is to treat $p$ as true for the purposes of practical reasoning. To believe $p$ is to have preferences that make sense, by one's own lights, in a world where $p$ is true. So, if one prefers A to B and believes that $p$, one prefers A to B given $p$. To say one prefers A to B given $p$ is not to say that if one were to learn $p$, she would prefer A to B. It's rather to say that one prefers the state of the world where A is performed and $p$ is true to the state of the world where B is performed and $q$ is true. This distinction matters in cases where learning $p$ changes the agent's preferences, and will become important in a bit.

This idea, that learning what you already believe makes no difference, can also be applied entirely within the realm of conditional preferences. It implies that someone who prefers $A$ to $B$ given $q$, and believes $p$ already prefers $A$ to $B$ given $p \wedge q$. If preference just is preference given a tautology, then this principle has the principle of the previous paragraph fall out as a special case. So we'll work with it for a while.

Let's start with the hypothesis that this is all there is to belief. To believe just is for one's preferences to be unchanged when conditionalising on the proposition. This hypothesis is false, but it's going to be useful to see just what's right and wrong about it. So our initial suggestion is that the following is true for all $A, B, q$. Here _Bel_($q$) means that the agent believes $q$, and $A \geq _q B$ means that the agent prefers $A$ to $B$ given $q$.

* _Bel_$(p) \leftrightarrow \forall A, B, q (A \geq _q B \leftrightarrow A  \geq _{p \wedge q} B)$

In words, an agent believes that $p$ iff conditionalising on $p$ doesn't change any conditional preferences.

As it stands, this is subject to simple counterexamples. Go back to F01, choosing which proposition to say is true. The principle says that she does not believe that the Magna Carta was signed in 1215. Conditional on that being true, she is indifferent between playing BT and AT, but actually she prefers BT to AT. That's not a problem; I think she doesn't actually believe AT. But consider her position yesterday when she was not facing this choice. The principle still implies that she doesn't believe that the Magna Carta was signed in 1215. Let $q$ be the description of the game setup, including what AT and BT are. And let $p$ be that the Magna Carta was signed in 1215. Conditional on $q$ she strictly prefers playing AT to BT. But conditional on $p \wedge q$ she is indifferent between the two. And that's too strong; yesterday she believed that the Magna Carta was signed in 1215.

We can fix this problem by making the account interest-relative. Restrict the quantifiers to those actions, and those propositions that the agent is thinking about. I mean 'thinking about' to be understood in a fairly broad sense here. An agent is thinking about an action if she is wondering how good it would be to do, whether it would be better or worse than some other option, whether it would be better or worse given some condition, and so on. And she is thinking about a proposition if she is wondering whether it is true, what the probability is that it is true, whether it is more or less probable than some other proposition, which actions would be good or bad, or better or worse, conditional on it being true, and so on. These criteria are broad, but they do not encompass everything. Most people, most of the time, are not thinking about just the choice between AT and BT that F01 faces. So the rationality or irrationality of one of those actions conditional on the Magna Carta being signed in 1215 is irrelevant to what she believes.

So we have an idea for a simple analysis of belief. An agent believes that _p_ if her conditional preferences over actions are unchanged when she conditionalises on _p_, as long as we restrict attention to actions and conditions that she is thinking about. That will get a lot of cases right, but it gets three kinds of cases wrong. First, it focuses too much on practical as opposed to theoretical interests. Second, it doesn't look at ways of updating other than conditionalising. And third, it doesn't have anything useful to say about propositions that the agent is not thinking about, and which are irrelevant to anything the agent is thinking about. Let's deal with those three things in order.

Imagine that F03 is thinking about _p_, and she says to herself, ''_p_ is almost certainly true, but there is a chance it is false. In fact, I think the probability of _p_ is about 0.983; it is probably true, but there is a 1.7% chance it is false. So _p_ is true, though it might be false.''. There is something very strange about that last step. Once F03 has acknowledged that there is a positive probability that _p_ is false, she can't simply go on to say that _p_ is true. At most she can say that _p_ is probably true.

But the analysis presented so far does not get this right. After all, if $\Pr(p) = 0.983$, then it is very likely that updating on $p$ will not change her conditional preferences over salient options. So we need to change the analysis a little.

In the above little speech, one question that F03 clearly is interested in is the probability of $p$. At least, she is interested in the probability of $p$ to three decimal places. And conditionalising on $p$ changes the answer to that question. Unconditionally, the probability of $p$ is 0.983; conditional on $p$ the probability of $p$ is 1. We can use that fact to solve the problem.

Say that a necessary condition for belief in $p$ is that conditionalising in $p$ does not change the answer to any question that is relevant to the agent. And say that the relevant questions include all questions the agent is thinking about, and conditional versions of those questions, conditional on all propositions she is thinking about. These questions might be practical questions about whether to do $A$ or $B$. But they might be purely theoretical questions about well the evidence supports $p$, or about how likely it is. One can be interested in a question about an historical event even if nothing at all practical turns on the answer. And what it takes to believe a particular answer will depend on how detailed one's care about the probability of the answer is.

So F03 does not believe that $p$. Indeed, reasoning of the form "The probability of $p$ is $x$, so $p$ is true (though it might be false)", will always be bad reasoning if $x < 1$. That's actually a striking result, since it is perfectly possible to believe things whose probability is less than 1. Indeed, it is possible to rationally believe them. But it seems wrong to reason from a non-maximal probability to the truth of something. We can explain this if we take belief to be relative to what one is interested in, and we take the very existence of this reasoning to make it the case that the agent is interested in the probability of $p$.

The second problem concerned updating by means other than conditionalisation. Conditionalising on a probability 1 proposition doesn't change anything. So you'd think that that the picture of belief I've offered here would imply that probability 1 implies belief. Indeed, in earlier work I summarised this picture of belief by saying that the agent believed $p$ iff their credence in $p$ was close enough to 1 for the purposes they had. But the discussion of GRG showed that we had to allow for the possibility of probability 1 without belief. It turns out we can do that provided we remember that not all updating is conditionalisation.

Here's an example from [#Gillies2010;] that makes the point well.[^A similar example is in [94][#Kratzer2012;]]

> I have lost my marbles. I know that just one of them -- Red or Yellow -- is in the box. But I don't know which. I find myself saying things like ... ''If Yellow isn't in the box, the Red must be.'' (4:13)

As Gillies goes on to point out, this isn't really a problem for the Ramsey test view of conditionals.

> The Ramsey test -- the schoolyard version, anyway -- is a test for when an indicative conditional is acceptable given your beliefs. It says that (if $p$)($q$) is acceptable in belief state $B$ iff $q$ is acceptable in the derived or subordinate state $B$-plus-the-information-that-$p$. (4:27)

And he notes that this can explain what goes on with the marbles conditional. Add the information that Yellow isn't in the box, and it isn't just true, but must be true, that Red is in the box.

Note though that while we can explain this conditional using the Ramsey test, we can't explain it using any version of the idea that probabilities of conditionals are conditional probabilities. The probability that Red must be in the box is 0. The probability that Yellow isn't in the box is not 0. So conditional on Yellow not being in the box, the probability that Red must be in the box is still 0. Yet the conditional is perfectly assertable.

There is, and this is Gillies's key point, something about the behaviour of modals in the consequents of conditionals that we can't capture using conditional probabilities, or indeed many other standard tools. And what goes for consequents of conditionals goes for updated beliefs too. Learn that Yellow isn't in the box, and you'll conclude that Red must be. But that learning can't go via conditionalisation; just conditionalise on the new information and the probability that Red must be in the box goes from 0 to 0.

Now it's a hard problem to say exactly how this alternative to updating by conditionalisation should work. But very roughly, the idea is that at least some of the time, we update by eliminating worlds from the space of possibilities. This affects dramatically the probability of propositions whose truth is sensitive to which worlds are in the space of possibiilties.

For example, in the game GRG, we should believe that rational $B$ might play _red_. And whether or not $B$ might play red is highly salient; it matters to the probability of whether $A$ will play GREEN or RED. Conditionalising on something that has probability 1, such as that $B$ will play _green_, can hardly change that probability. But updating on the proposition that $B$ will play _green_ can make a difference. We can see that by simply noting that the conditional _If B plays green, she might play red_ is incoherent.

So I conclude that a theory of belief like mine can handle the puzzle this game poses, as long as it distinguishes between conditionalising and updating, in just the way Gillies suggests. To believe that $p$, it is necessary to be disposed to not change any attitude towards a salient question on updating that $p$. Updating often goes by conditionalisation, so we can often say that belief means having attitudes that match unconditionally and conditionally on $p$. But not all updating works that way, and the theory of belief needs to acknowledge this.

Finally, let's look at propositions that are not relevant to any question that the the agent is interested in.[^What follows is a version of the example about Dalí's moustache from [#RossSchroeder201x;], purged of any worries about vagueness, and without the focus on consistency. For reasons I'll get to below, I'm not worried about the argument that my theory will allow agents to have inconsistent beliefs. But the case they raise is still a serious puzzle.] Let _D_ be the proposition that the number of games the Detroit Tigers won in 1976 (in the MLB regular season) is not a multiple of 3. And let M04 be a person for whom _D_, and $\neg D$  are completely irrelevant to anything they care about, either practically or theoretically. His attitudes towards any relevant question are the same unconditionally as they would be after updating on either _D_ or $\neg D$. So it looks like he will end up counting as believing _D_, and believing $\neg D$, by default.

In earlier work, I added a clause meant to help with cases like this. I said that for determining whether an agent believes that $p$, we should treat the question of whether $p$'s probability is above or below 0.5 as salient, even if the agent doesn't care about it. Obviously this won't help with this particular case. The probability of _D_ is around $\nicefrac{2}{3}$, and is certainly above 0.5. This 'fix' avoids the consequence that M04 implausibly counts as believing $\neg D$. But he still counts, almost as implausibly, as believing _D_. This needs to be fixed.

Here's how I now propose to solve this problem. For an agent to count as believing $p$, it must be possible for $p$ to do some work for them in reasoning. Here's what I mean by work. Consider a very abstract set up of a decision problem, as follows.

| | $p$ | $q$ |  
| ----: | ------	| ------	|  
| X | 4 | 1 |  
| Y | 3 | 2 |  

That table encodes a lot of information. It encodes that $p \vee q$ is true; otherwise there are some columns missing. It encodes that the only live choices are X or Y; otherwise there are rows missing. It encodes that doing X is better than doing Y if $p$, and worse if $q$. 

For any agent, and any decision problem, there is a table like this that they would be disposed to use to resolve that problem. Or, perhaps, there are a series of tables and there is no fact about which of them they would be most disposed to use.

Given all that terminology, here's my extra constraint on belief. To believe that $p$, there must be some decision problem such that some table the agent would be disposed to use to solve it encodes that $p$. If there is no such problem, the agent does not believe that $p$. For anything that one intuitively believes, this is an easy condition to satisfy. Let the problem be whether to take a bet that pays 1 if $p$, and loses 1 otherwise. Here's the natural table to use to solve the problem.

|             | *p* |  
|  --------:  | :-: |  
| Take bet    |  1  |  
| Decline bet |  0  |

This table encodes that $p$, so it is sufficient to count as believing that $p$. And it doesn't matter that this bet isn't on the table. I'm disposed to use this table, so that's all that matters.

But might there be problems in the other direction. What about an agent who, if offered such a bet on _D_, would use such a simple table? I simply say that they believe that _D_. I would not use any such table. I'd use this table.

|             | *D* | \\(\neg D\\) |
|  --------:  | :-: | :----------: |
| Take bet    |  1  |       -1     |
| Decline bet |  0  |       0      |  

Using that table would still mean taking the bet. I'd generally take this bet.^[Well, actually I'd probably decline the bet because being offered the bet would change the probability of _D_ for reasons made clear in [14-15][#RunyonGuysDolls]. But that hardly undermines the point I'm making here.] But this isn't some analytic fact about me, or even I think some respect in which I'm obeying the dictates of rationality. It's simply a fact that I wouldn't take _D_ for granted in any inquiry. And that's what my non-belief that _D_ consists in.

And while I haven't specified enough about M04 to say whether he would make decisions the same way, it seems plausible enough to think that he would. Unless he is prepared to simply write _D_ into a decision table for some problem, even some currently irrelevant problem, he doesn't believe _D_. 

And that, in short, is the theory of belief I want to defend. An agent believes $p$ just in case they will take $p$ as given in all problems that are currently relevant, and at least one problem that may or may not be relevant. That, I say, is how belief is relative to interests.

## Belief and Maximal Credence

There is a straightforward functionalist argument that belief requires credence one. Since much of the picture I'm presenting goes through most smoothly on a functionalist picture of the mind, that argument is one that I take seriously.

A key functional role of credences is that if an agent has credence $x$ in $p$ she should be prepared to buy a bet that returns 1 util if $p$, and 0 utils otherwise, iff the price is no greater than p utils. A key functional role of belief is that if an agent believes p, and recognises that $\varphi$ is the best thing to do given $p$, then she'll do $\varphi$. Given $p$, it's worth paying any price up to 1 util for a bet that pays 1 util if $p$. So believing $p$ seems to mean being in a functional state that is like having credence 1 in $p$.

But this argument isn't quite right. If we spell out more carefully what the functional roles of credence and belief are, a loophole emerges in the argument that belief implies credence 1. The interest-relative theory of belief I'm endorsing exploits that loophole. What's the difference, in functional terms, between having credence $x$ in $p$, and having credence $x + \varepsilon$ in $p$? Well, think again about the bet that pays 1 util if $p$, and 0 utils otherwise. And imagine that bet is offered for $x + \nicefrac{\varepsilon}{2}$ utils. The person whose credence is $x$ will decline the offer; the person whose credence is $x + \varepsilon$ will accept it. Now it will usually be that no such bet is on offer. No matter; as long as one agent is disposed to accept the offer, and the other agent is not, that suffices for a difference in credence.

The upshot of that is that differences in credences might be, indeed usually will be, constituted by differences in dispositions concerning how to act in choice situations far removed from actuality. I'm not usually in a position of having to accept or decline a chance to buy a bet for 0.9932 utils that the local coffee shop is currently open. Yet whether I would accept or decline such a bet matters to whether my credence that the coffee shop is open is 0.9931 or 0.9933. This isn't a problem with the standard picture of how credences work. It's just an observation that the high level of detail embedded in the picture relies on taking the constituents of mental states to involve many dispositions.

One of the crucial features of the theory of belief I'm defending is that what an agent believes is in general insensitive to such abtruse dispositions, although it is very sensitive to dispositions about practical matters. It's true that if I believe that $p$, and I'm rational enough, I'll act as if $p$ is true. Is it also true that if I believe $p$, I'm disposed to act as if $p$ is true no matter what choices are placed in front of me? The theory being defended here says no, and that seems plausible. F01 can believe that the Magna Carta was signed in 1215, but be disposed to lose that belief rather than act on it if odd choices, like that presented by the genie, emerge.

This suggests the key difference between belief and credence 1. For a rational agent, a credence of 1 in $p$ means that the agent is disposed to answer a wide range of questions the same way she would answer that question conditional on $p$. If her credence in $p$ is 1, then she'll be disposed to give the same answer unconditionally as she gives conditionally on $p$ to questions about the probability of any proposition, and the utility of any act. This won't necessarily be true if she merely believes that $p$. After all, belief that $p$ is consistent with updating on $p$ making a difference to some irrelevant question. That turns out to be the difference between belief and credence 1. Updating on belief makes no difference to any relevant question; updating on a proposition in which has credence 1 makes no difference to any (non-modal) question. Since the difference between belief and credence 1 is interest-relative, belief is interest-relative.

Roger [#Clarke2013;] has argued that if credence is context-sensitive, it is possible to equate belief with credence one. I don't think credence one is sufficient for belief because of GRG, and because of blindspot cases. But I also don't think it is necessary. Most things I believe I wouldn't bet on at infinite odds. And Clarke doesn't say that belief requires this strong betting disposition. What he says instead is that just what ones credences are turns out to be context-sensitive. 

To understand this proposal, we need a theory of what credences are, and Clarke doesn't really offer one. Instead he says we should ''take _degree of belief__ as a primitive concept and get at its meaning via the ''platitudes'' we take to be true about it.'' (11n12) But if the platitudes about it are platitudes about betting dispositions, this won't work. After all, those platitudes say that credences govern betting behavior!

Maybe that's not really a platitude; maybe credences aren't connected that closely to betting behavior. In fact Frank [#Ramsey1929;] suggested a nice alternative way to think about credences. To have credence $\nicefrac{2}{3}$ in $p$ is to be exactly as confident in $p$ as in $q \vee r$, where $q, r$ and $s$ certainly exclusive and exhaustive, and one has equal confidence in all three. We'll come back to this definition later; it turns out to be useful to have a definition of numerical credences in terms of comparative confidence. It's easy to see how to extend that to a definition of credence $\nicefrac{m}{n}$ for any integer $m, n$. It's a little trickier to say precisely what, say, credence $\nicefrac{1}{\pi}$ is, but not impossible. However we do this, we get a plausible story about how numerical and comparative confidences interact, and that's a plausible way of filling in what the platitudes about degrees of belief are.

But it doesn't help Clarke. To have credence 1 in $p$ is to have a credence in it that is higher than 0.999999. And to have credence 0.999999 in something is to think it is just as likely to be true as one ticket in a million ticket fair lottery is to lose. But I can believe something without thinking it is more likely to be true than that one particular ticket in a million ticket fair lottery is to lose.

Whether we think of degrees of belief as betting dispositions or as encoding comparative confidence, there is no reason to think credence 1 is necessary for belief. Clarke primarily argues against Lockean views about the credence/belief relationship; he doesn't discuss pragmatic theories like my earlier theory. But some of the arguments do carry across. In particular, he notes that the Lockean view has serious problems with closure principles. We might wonder whether my theory does better. The point of the next section is to argue that it does.

## Closure and Its Discontents

