{\rtf1\ansi\uc1\deff0\deflang1024
{\fonttbl{\f0\fnil\fcharset0 Times;}
{\f1\fnil\fcharset0 Helvetica;}
{\f2\fnil\fcharset0 Helvetica;}
{\f3\fnil\fcharset0 Courier;}
{\f4\fnil\fcharset0 Zapf Chancery;}
{\f5\fnil\fcharset0 Times;}
{\f6\fnil\fcharset0 Helvetica;}
{\f7\fnil\fcharset0 Helvetica;}
{\f8\fnil\fcharset0 Courier;}
{\f9\fnil\fcharset0 Zapf Chancery;}
{\f10\fnil\fcharset0 Times;}
{\f11\fnil\fcharset0 Helvetica;}
{\f12\fnil\fcharset0 Helvetica;}
{\f13\fnil\fcharset0 Courier;}
{\f14\fnil\fcharset0 Zapf Chancery;}
{\f15\fnil\fcharset0 Symbol;}
{\f16\fnil\fcharset0 MT Extra;}
{\f17\fnil\fcharset0 STIXGeneral;}
}
{\colortbl;
\red0\green0\blue0;
\red0\green0\blue255;
\red0\green255\blue255;
\red0\green255\blue0;
\red255\green0\blue255;
\red255\green0\blue0;
\red255\green255\blue0;
\red255\green255\blue255;
}
{\stylesheet
{\s0\qj\widctlpar\f0\fs20 \snext0 Normal ;}
{\cs10 \additive\ssemihidden Default Paragraph Font ;}
{\s1\qc\sb240\sa120\keepn\f0\b\fs40 \sbasedon0\snext0 Part ;}
{\s2\ql\sb240\sa120\keepn\f0\b\fs40 \sbasedon0\snext0 heading 1 ;}
{\s3\ql\sb240\sa120\keepn\f0\b\fs32 \sbasedon0\snext0 heading 2 ;}
{\s4\ql\sb240\sa120\keepn\f0\b\fs32 \sbasedon0\snext0 heading 3 ;}
{\s5\ql\sb240\sa120\keepn\f0\b\fs24 \sbasedon0\snext0 heading 4 ;}
{\s6\ql\sb240\sa120\keepn\f0\b\fs24 \sbasedon0\snext0 heading 5 ;}
{\s7\ql\sb240\sa120\keepn\f0\b\fs24 \sbasedon0\snext0 heading 6 ;}
{\s8\qr\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext8 rightpar ;}
{\s9\qc\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext9 centerpar ;}
{\s10\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext10 leftpar ;}
{\s11\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equation ;}
{\s12\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equationNum ;}
{\s13\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equationAlign ;}
{\s14\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equationAlignNum ;}
{\s15\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equationArray ;}
{\s16\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equationArrayNum ;}
{\s17\ql\sb120\sa120\keep\widctlpar\f0\fs20 \sbasedon0\snext0 theorem ;}
{\s18\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 bitmapCenter ;}
{\s20\qc\sb240\sa240\b\f0\fs36 \sbasedon0\snext21 Title ;}
{\s21\qc\sa120\f0\fs20 \sbasedon0\snext0 author ;}
{\s22\ql\tqc\tx4536\tqr\tx9072\f0\fs20 \sbasedon0\snext22 footer ;}
{\s23\ql\tqc\tx4536\tqr\tx9072\f0\fs20 \sbasedon0\snext23 header ;}
{\s30\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 caption ;}
{\s31\qc\sb120\sa0\keep\widctlpar\f0\fs20 \sbasedon0\snext0 Figure ;}
{\s32\qc\sb120\sa0\keep\widctlpar\f0\fs20 \sbasedon0\snext32 Table ;}
{\s33\qc\sb120\sa0\keep\widctlpar\f0\fs20 \sbasedon0\snext33 Tabular ;}
{\s34\qc\sb120\sa0\keep\widctlpar\f0\fs20 \sbasedon0\snext34 Tabbing ;}
{\s35\qj\li1024\ri1024\fi340\widctlpar\f0\fs20 \sbasedon0\snext35 Quote ;}
{\s38\ql\widctlpar\f3\fs20 \snext38 HTML Preformatted ;}
{\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20 \sbasedon0\snext46 List ;}
{\s47\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20 \sbasedon0\snext47 List 1 ;}
{\s50\qc\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 latex picture ;}
{\s51\qc\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 subfigure ;}
{\s61\ql\sb240\sa120\keepn\f0\b\fs32 \sbasedon0\snext62 bibheading ;}
{\s62\ql\fi-567\li567\sb0\sa0\f0\fs20 \sbasedon0\snext62 bibitem ;}
{\s64\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20 \sbasedon0\snext64 endnotes ;}
{\s65\ql\fi-113\li397\lin397\f0\fs20 \sbasedon0\snext65 footnote text ;}
{\s66\qj\fi-170\li454\lin454\f0\fs20 \sbasedon0\snext66 endnote text ;}
{\cs62\super \additive\sbasedon10 footnote reference ;}
{\cs63\super \additive\sbasedon10 endnote reference ;}
{\s70\qc\sa120\b\f0\fs20 \sbasedon0\snext71 abstract title ;}
{\s71\qj\li1024\ri1024\fi340\widctlpar\f0\fs20 \sbasedon0\snext0 abstract ;}
{\s80\ql\sb240\sa120\keepn\f0\b\fs20 \sbasedon0\snext0 contents_heading ;}
{\s81\ql\li425\tqr\tldot\tx8222\sb240\sa60\keepn\f0\fs20\b \sbasedon0\snext82 toc 1 ;}
{\s82\ql\li512\tqr\tldot\tx8222\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext83 toc 2 ;}
{\s83\ql\li1024\tqr\tldot\tx8222\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext84 toc 3 ;}
{\s84\ql\li1536\tqr\tldot\tx8222\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext85 toc 4 ;}
{\s85\ql\li2048\tqr\tldot\tx8222\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext86 toc 5 ;}
{\s86\ql\li2560\tqr\tldot\tx8222\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext86 toc 6 ;}
{\s91\qc\sb240\sa120\keepn\f0\b\fs32 \sbasedon0\snext0 heading 2 ;}
{\s92\ql\sb240\sa120\keepn\f0\i\fs32 \sbasedon0\snext0 heading 3 ;}
{\s93\ql\fi425\keepn\f0\fs24 \sbasedon0\snext0 heading 4 ;}
}
{\info
{\title Original file was KBI-Weatherson-Revised.tex}
{\doccomm Created using latex2rtf 2.1.0 (released Mar 5 2010) on Fri May 13 10:37:39 2011
}
}
{\footer\pard\plain\f0\fs20\qc\chpgn\par}
\paperw12280\paperh15900\margl2680\margr2700\margt2540\margb1760\pgnstart0\widowctrl\qj\ftnbj\f0\aftnnar
{\pard\plain\ql\sb240\sa120\keepn\f0\b\fs40\sl240\slmult1 \fi0 Chapter 1\par
\pard\plain\s2\ql\sb240\sa120\keepn\f0\b\fs40\sl240\slmult1 \sb240 \fi0 [\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi0 Knowledge, Bets and Interests]{Knowledge, Bets and Interests auth=Brian Weatherson}\par
\pard\plain\s3\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb240 \fi0 1.1  Knowledge in Decision Making\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 When you pick up a volume like this one, which describes itself as being about \lquote knowledge ascriptions\rquote , you probably expect to find it full of papers on epistemology, broadly construed. And you\rquote d probably expect many of those papers to concern themselves with cases where the interests of various parties (ascribers, subjects of the ascriptions, etc.) change radically, and this affects the truth values of various ascriptions. And, at least in this paper, your expectations will be clearly met.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 But here\rquote s an interesting contrast. If you\rquote d picked up a volume of papers on \lquote belief ascriptions\rquote , you\rquote d expect to find a radically different menu of writers and subjects. You\rquote d expect to find a lot of concern about names and demonstratives, and about how they can be used by people not entirely certain about their denotation. More generally, you\rquote d expect to find less epistemology, and much more mind and language. I haven\rquote t read all the companion papers to mine in this volume, but I bet you won\rquote t find much of that here.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 This is perhaps unfortunate, since belief ascriptions and knowledge ascriptions raise at least some similar issues. Consider a kind of contextualism about belief ascriptions, which holds that (L) can be truly uttered in some contexts, but not in others, depending on just what aspects of Lois Lane\rquote s psychology are relevant in the conversation.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} The reflections in the next few paragraphs are inspired by some comments in by Stalnaker {Stalnaker2008}, though I don\rquote t want to suggest the theory I\rquote ll discuss is actually Stalnaker\rquote s.}
\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 1.\tab
Lois Lane believes that Clark Kent is vulnerable to kryptonite. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 We could imagine a theorist who says that whether (L) can be uttered truly depends on whether it matters to the conversation that Lois Lane might not recognise Clark Kent when he\rquote s wearing his Superman uniform. And, this theorist might continue, this isn\rquote t because \lquote Clark Kent\rquote  is a context-sensitive expression; it is rather because \lquote believes\rquote  is context-sensitive. Such a theorist will also, presumably, say that whether (K) can be uttered truly is context-sensitive.\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 1.\tab
Lois Lane knows that Clark Kent is vulnerable to kryptonite. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 And so, our theorist is a kind of contextualist about knowledge ascriptions. But they might agree with approximately none of the motivations for contextualism about knowledge ascriptions put forward by 
[{\field{\*\fldinst{\lang1024 REF BIB_Cohen1988 \\* MERGEFORMAT }}{\fldrslt{{4}{1988}{{Cohen}}{{}}}}}
], 
[{\field{\*\fldinst{\lang1024 REF BIB_DeRose1995 \\* MERGEFORMAT }}{\fldrslt{{5}{1995}{{DeRose}}{{}}}}}
] or 
[{\field{\*\fldinst{\lang1024 REF BIB_Lewis1996b \\* MERGEFORMAT }}{\fldrslt{{17}{1996}{{Lewis}}{{}}}}}
]. Rather, they are a contextualist about knowledge ascriptions solely because they are contextualist about belief ascriptions like (L).\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Call the position I\rquote ve just described {\b doxastic contextualism} about knowledge ascriptions. It\rquote s a kind of contextualism all right; it says that whether (K) can be truly uttered is context sensitive, and not because of the context-sensitivity of any term in the \lquote that\rquote -clause. But it explains the contextualism solely in terms of the contextualism of belief ascriptions. The more familiar kind of contextualism about knowledge ascriptions we\rquote ll call {\b non-doxastic contextualism}.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 We can make the same kind of division among interest-relative invariantist, or IRI, theories of knowledge ascriptions. Any kind of IRI will say that there are sentences of the form {\i S knows that p} whose truth depends on the interests, in some sense, of {{\i S}}. But we can divide IRI theories up the same way that we divide up contextualist theories.\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b Doxastic IRI} Knowledge ascriptions are interest-relative, but their interest-relativity traces solely to the interest-relativity of the corresponding belief ascriptions. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b Non-Doxastic IRI} Knowledge ascriptions are interest-relative, and their interest-relativity goes beyond the interest-relativity of the corresponding belief ascriptions. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 In my {Weatherson2005-WEACWD}, I tried to motivate Doxastic IRI. More precisely, I argued for Doxastic IRI about ascriptions of {\i justified} belief, and hinted that the same arguments would generalise to knowledge ascriptions. I now think those hints were mistaken, and want to defend Non-Doxastic IRI about knowledge ascriptions.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} Whether Doxastic or Non-Doxastic IRI is true about justified belief ascriptions turns on some tricky questions about what to say when a subject\rquote s credences are nearly, but not exactly appropriate given her evidence. Space considerations prevent a full discussion of those cases here.}
 My change of heart has been prompted by cases like those Jason 
[{\field{\*\fldinst{\lang1024 REF BIB_Stanley2005_STAKAP \\* MERGEFORMAT }}{\fldrslt{{22}{2005}{{Stanley}}{{}}}}}
] calls \lquote Ignorant High Stakes\rquote  cases.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} I mean here the case of Coraline, to be discussed in section 3 below. Several people have remarked in conversation that Coraline doesn\rquote t look to them like a case of Ignorant High Stakes. This isn\rquote t surprising; Coraline is better described as being {\i mistaken} than {\i ignorant}, and she\rquote s mistaken about odds not stakes. If they\rquote re right, that probably means my argument for Non-Doxastic IRI is less like Stanley\rquote s, and hence more original, than I think it is. So I don\rquote t feel like pressing the point!  But I do want to note that {\i I} thought the Coraline example was a variation on a theme Stanley originated.}
 But to see why these cases matter, it will help to start with why I think some kind of IRI must be true. And that story starts with some reflections on the way we teach decision theory.\par
\pard\plain\s4\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb120 \fi0 1.1.1  The Struction of Decision Problems\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 Professor Dec is teaching introductory decision theory to her undergraduate class. She is trying to introduce the notion of a dominant choice. So she introduces the following problem, with two states, {{\field{\*\fldinst{ EQ {\i S}\\s\\do5({\fs16 1})}}{\fldrslt }}
} and {{\field{\*\fldinst{ EQ {\i S}\\s\\do5({\fs16 2})}}{\fldrslt }}
}, and two choices, {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 1})}}{\fldrslt }}
} and {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
} as is normal for introductory problems.\par
{{\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 \par
{\trowd\cellx2299\cellx4598\cellx6897
{\pard\intbl\qr {}\cell}
{\pard\intbl\qc {{{\field{\*\fldinst{ EQ {\i S}\\s\\do5({\fs16 1})}}{\fldrslt }}
}}\cell}
{\pard\intbl\qc {{{\field{\*\fldinst{ EQ {\i S}\\s\\do5({\fs16 2})}}{\fldrslt }}
}}\cell}
\row}
{\trowd\cellx2299\cellx4598\cellx6897
{\pard\intbl\qr {{{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 1})}}{\fldrslt }}
}}\cell}
{\pard\intbl\qc {-$200}\cell}
{\pard\intbl\qc {$1000}\cell}
\row}
{\trowd\cellx2299\cellx4598\cellx6897
{\pard\intbl\qr {{{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
}}\cell}
{\pard\intbl\qc {-$100}\cell}
{\pard\intbl\qc {$1500}\cell}
\row}
} \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 She\rquote s hoping that the students will see that {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 1})}}{\fldrslt }}
} and {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
} are bets, but {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
} is clearly the better bet. If {{\field{\*\fldinst{ EQ {\i S}\\s\\do5({\fs16 1})}}{\fldrslt }}
} is actual, then both bets lose, but {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
} loses less money. If {{\field{\*\fldinst{ EQ {\i S}\\s\\do5({\fs16 2})}}{\fldrslt }}
} is actual, then both bets win, but {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
} wins more. So {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
} is better. That analysis is clearly wrong if the state is causally dependent on the choice, and controversial if the states are evidentially dependent on the choices. But Professor Dec has not given any reason for the students to think that the states are dependent on the choices in either way, and in fact the students don\rquote t worry about that kind of dependence.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 That doesn\rquote t mean, however, that the students all adopt the analysis that Professor Dec wants them to. One student, Stu, is particularly unwilling to accept that {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
} is better than {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 1})}}{\fldrslt }}
}. He thinks, on the basis of his experience, that when more than $1000 is on the line, people aren\rquote t as reliable about paying out on bets. So while {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 1})}}{\fldrslt }}
} is guaranteed to deliver $1000 if {{\field{\*\fldinst{ EQ {\i S}\\s\\do5({\fs16 2})}}{\fldrslt }}
}, if the agent bets on {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
}, she might face some difficulty in collecting on her money.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Given the context, i.e., that they are in an undergraduate decision theory class, it seems that Stu has misunderstood the question that Professor Dec intended to ask. But it is a little harder than it first seems to specify just exactly what Stu\rquote s mistake is. It isn\rquote t that he thinks Professor Dec has {\i misdescribed} the situation. It isn\rquote t that he thinks it is false that the agent will collect $1500 if she chooses {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
} and is in {{\field{\*\fldinst{ EQ {\i S}\\s\\do5({\fs16 2})}}{\fldrslt }}
}. He just thinks that she {\i might} not be able to collect it, so the expected payout might really be a little less than $1500.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Before we try to say just what the misunderstanding between Professor Dec and Stu consists in, let\rquote s focus on a simpler problem. Alice is out of town on a holiday, and she faces the following decision choice concerning what to do with a token in her hand.\par
{{\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 \par
{\trowd\cellx3737\cellx6899
{\pard\intbl\qr {{\b Choice}}\cell}
{\pard\intbl\qc {{\b Outcome}}\cell}
\row}
{\trowd\cellx3737\cellx6899
{\pard\intbl\qr {Put token on table}\cell}
{\pard\intbl\qc {Win $1000}\cell}
\row}
{\trowd\cellx3737\cellx6899
{\pard\intbl\qr {Put token in pocket}\cell}
{\pard\intbl\qc {Win nothing}\cell}
\row}
} \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 This looks easy, especially if we\rquote ve taken Professor Dec\rquote s class. Putting the token on the table dominates putting the token in her pocket. It returns $1000, versus no gain. So she should put the token on the table.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 I\rquote ve left Alice\rquote s story fairly schematic; let\rquote s fill in some of the details. Alice is on holiday at a casino. It\rquote s a fair casino; the probabilities of the outcomes of each of the games is just what you\rquote d expect. And Alice knows this. The table she\rquote s standing at is a roulette table. The token is a chip from the casino worth $1000. Putting the token on the table means placing a bet. As it turns out, it means placing a bet on the roulette wheel landing on 28. If that bet wins she gets her token back and another token of the same value. There are many other bets she could make, but Alice has decided not to make all but one of them. Since her birthday is the 28{{\field{\*\fldinst{ EQ \\s\\up5({\fs16 {{\i t}{\i h}}})}}{\fldrslt }}
}, she is tempted to put a bet on 28; that\rquote s the only bet she is considering. If she makes this bet, the objective chance of her winning is {{1}{38}}, and she knows this. As a matter of fact she will win, but she doesn\rquote t know this. (This is why the description in the table I presented above is truthful, though frightfully misleading.) As you can see, the odds on this bet are terrible. She should have a chance of winning around {{1}{2}} to justify placing this bet.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} Assuming Alice\rquote s utility curve for money curves downwards, she should be looking for a slightly higher chance of winning than {{1}{2}} to place the bet, but that level of detail isn\rquote t relevant to the story we\rquote re telling here.}
 So the above table, which makes it look like placing the bet is the dominant, and hence rational, option, is misleading.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Just how is the table misleading though?  It isn\rquote t because what is says is false. If Alice puts the token on the table she wins $1000; and if she doesn\rquote t, she stays where she is. It isn\rquote t, or isn\rquote t just, that Alice doesn\rquote t believe the table reflects what will happen if she places the bet. As it turns out, Alice is smart, so she doesn\rquote t form beliefs about chance events like roulette wheels. But even if she did, that wouldn\rquote t change how misleading the table is. The table suggests that it is rational for Alice to put the token on the table. In fact, that is irrational. And it would still be irrational if Alice believes, {\i irrationally}, that the wheel will land on 28.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 A better suggestion is that the table is misleading because Alice doesn\rquote t {\i know} that it accurately depicts the choice she faced. If she did know that these were the outcomes to putting the token on the table versus in her pocket, it seems it would be rationally compelling for her to put it on the table. If we take it as tacit in a presentation of a decision problem that the agent knows that the table accurately depicts the outcomes of various choices in different states, then we can tell a plausible story about what the miscommunication between Professor Dec and Stu was. Stu was assuming that if the agent wins $1500, she might not be able to easily collect. That is, he was assuming that the agent does not know that she\rquote ll get $1500 if she chooses {{\field{\*\fldinst{ EQ {\i C}\\s\\do5({\fs16 2})}}{\fldrslt }}
} and is in state {{\field{\*\fldinst{ EQ {\i S}\\s\\do5({\fs16 2})}}{\fldrslt }}
}. Professor Dec, if she\rquote s anything like other decision theory professors, will have assumed that the agent did know exactly that.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 As we\rquote ve seen, the standard presentation of a decision problem presupposes not just that the table states what will happen, but the agent stands in some special doxastic relationship to that information. Could that relationship be weaker than knowledge?  It\rquote s true that it is hard to come up with clear counterexamples to the suggestion that the relationship is merely justified true belief. But I think it is somewhat implausible to hold that the standard presentation of an example merely presupposes that the agent has a justified true belief that the table is correct, and does not in addition know that the table is correct.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 My reasons for thinking this are similar to one of the reasons Timothy Williamson [Ch. 9]{Williamson2000-WILKAI} gives for doubting that one\rquote s evidence is all that one justifiably truly believes. To put the point in Lewisian terms, it seems that knowledge is a much more {\i natural} relation than justified true belief. And when ascribing contents, especially contents of tacitly held beliefs, we should strongly prefer to ascribe more rather than less natural contents.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 I\rquote m here retracting some things I said a few years ago in a paper on philosophical methodology {Weatherson2003-WEAWGA}. There I argued that identifying knowledge with justified true belief would give us a theory on which knowledge was more natural than a theory on which we didn\rquote t identify knowledge with any other epistemic property. I now think that is wrong for a couple of reasons. First, although it\rquote s true (as I say in the earlier paper) that knowledge can\rquote t be primitive or perfectly natural, this doesn\rquote t make it less natural than justification, which is also far from a fundamental feature of reality. Indeed, given how usual it is for languages to have a simple representation of knowledge, we have some evidence that it is very natural for a term from a special science. Second, I think in the earlier paper I didn\rquote t fully appreciate the point (there attributed to Peter Klein) that the Gettier cases show that the property of being a justified true belief is not particularly natural. In general, when {{\i F}} and {{\i G}} are somewhat natural properties, then so is the property of being {{\i F}{\u8743*}{\i G}}. But there are exceptions, especially in cases where these are properties that a whole can have in virtue of a part having the property. In those cases, a whole that has an {{\i F}} part and a {{\i G}} part will be {{\i F}{\u8743*}{\i G}}, but this won\rquote t reflect any distinctive property of the whole. And one of the things the Gettier cases show is that the properties of {\i being justified} and {\i being true}, as applied to belief, fit this pattern.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} Note that even if you think that philosophers are generally too quick to move from instinctive reactions to the Gettier case to abandoning the justified true belief theory of knowledge, this point holds up. What is important here is that on sufficient reflection, the Gettier cases show that some justified true beliefs are not knowledge, and that the cases in question also show that being a justified true belief is not a particularly natural or unified property. So the point I\rquote ve been making in the last few paragraphs is independent of the point I wanted to stress in \ldblquote What Good are Counterexamples? \rdblquote , namely, that philosophers in some areas (especially epistemology) are insufficiently reformist in their attitude towards our intuitive reactions to cases.}
\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 So the \lquote special doxastic relationship\rquote  is not weaker than knowledge. Could it be stronger?  Could it be, for example, that the relationship is certainty, or some kind of iterated knowledge?  Plausibly in some game-theoretic settings it is stronger \endash  it involves not just knowing that the table is accurate, but knowing that the other player knows the table is accurate. In some cases, the standard treatment of games will require positing even more iterations of knowledge. For convenience, it is sometimes explicitly stated that iterations continue indefinitely, so each party knows the table is correct, and knows each party knows this, and knows each party knows that, and knows each party knows {\i that}, and so on. An early example of this in philosophy is in the work by David 
[{\field{\*\fldinst{\lang1024 REF BIB_Lewis1969a \\* MERGEFORMAT }}{\fldrslt{{15}{1969}{{Lewis}}{{}}}}}
] on convention. But it is usually acknowledged (again in a tradition extending back at least to Lewis) that only the first few iterations are actually needed in any problem, and it seems a mistake to attribute more iterations than are actually used in deriving solutions to any particular game. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The reason that would be a mistake is that we want game theory, and decision theory, to be applicable to real-life situations. There is very little that we know, and know that we know, and know we know we know, and so on indefinitely [Ch. 4]{Williamson2000-WILKAI}. There is, perhaps, even less that we are certain of. If we only could say that a person is playing a particular game when they stand in these very strong relationships to the parameters of the game, then people will almost never be playing any games of interest. Since game theory, and decision theory, are not meant to be that impractical, I conclude that the \lquote special doxastic relationship\rquote  cannot be that strong. It could be that in some games, the special relationship will involve a few iterations of knowledge, but in decision problems, where the epistemic states of others are irrelevant, even that is unnecessary, and simple knowledge seems sufficient.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 It might be argued here that we shouldn\rquote t expect to apply decision theory directly to real-life problems, but only to idealised versions of them, so it would be acceptable to, for instance, require that the things we put in the table are, say, things that have probability exactly 1. In real life, virtually nothing has probability 1. In an idealisation, many things do. But to argue this way seems to involve using \lquote idealisation\rquote  in an unnatural sense. There is a sense in which, whenever we treat something with non-maximal probability as simply given in a decision problem that we\rquote re ignoring, or abstracting away from, some complication. But we aren\rquote t {\i idealising}. On the contrary, we\rquote re modelling the agent as if they were irrationally certain in some things which are merely very very probable. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 So it\rquote s better to say that any application of decision theory to a real-life problem will involve ignoring certain (counterfactual) logical or metaphysical possibilities in which the decision table is not actually true. But not any old abstraction will do. We can\rquote t ignore just anything, at least not if we want a good model. Which abstractions are acceptable?  I have an answer to this: we can abstract away from any possibility in which something the agent actually knows is false. I don\rquote t have a knock-down argument that this is the best of all possible abstractions, but nor do I know of any alternative answer to the question which abstractions are acceptable which is nearly as plausible.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 In part that is because it is plausible that the \lquote special doxastic relationship\rquote  should be a fairly simple, natural relationship. And it seems that any simple, natural relationship weaker than knowledge will be so weak that when we plug it into our decision theory, it will say that Alice should do clearly irrational things in one or other of the cases we described above. And it seems that any simple, natural relationship stronger than knowledge will be so strong that it makes decision theory or game theory impractical.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 I also cheated a little in making this argument. When I described Alice in the casino, I made a few explicit comments about her information states. And every time, I said that she {\i knew} various propositions. It seemed plausible at the time that this is enough to think those propositions should be added to the table. That\rquote s some evidence against the idea that more than knowledge, perhaps iterated knowledge or certainty, is needed before we add propositions to the decision table.\par
\pard\plain\s4\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb120 \fi0 1.1.2  From Decision Theory to Interest-Relativity\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 This way of thinking about decision problems offers a new perspective on the issue of whether we should always be prepared to bet on what we know.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} This issue is of course central to the plotline in 
[{\field{\*\fldinst{\lang1024 REF BIB_Hawthorne2004 \\* MERGEFORMAT }}{\fldrslt{{10}{2004}{{Hawthorne}}{{}}}}}
].}
 To focus intuitions, let\rquote s take a concrete case. Barry is sitting in his apartment one evening when he hears a musician performing in the park outside. The musician, call her Beth, is one of Barry\rquote s favourite musicians, so the music is familiar to Barry. Barry is excited that Beth is performing in his neighbourhood, and he decides to hurry out to see the show. As he prepares to leave, a genie appears an offers him a bet. If he takes the bet, and the musician is Beth, then the genie give Barry ten dollars. On the other hand, if the musician is not Beth, he will be tortured in the fires of hell for a millenium. Let\rquote s put Barry\rquote s options in table form.\par
{{\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 \par
{\trowd\cellx1883\cellx4212\cellx6898
{\pard\intbl\qr {}\cell}
{\pard\intbl\qc {{\b Musician is Beth}}\cell}
{\pard\intbl\qc {{\b Musician is not Beth}}\cell}
\row}
{\trowd\cellx1883\cellx4212\cellx6898
{\pard\intbl\qr {{\b Take Bet}}\cell}
{\pard\intbl\qc {Win $10}\cell}
{\pard\intbl\qc {1000 years of torture}\cell}
\row}
{\trowd\cellx1883\cellx4212\cellx6898
{\pard\intbl\qr {{\b Decline Bet}}\cell}
{\pard\intbl\qc {Status quo}\cell}
{\pard\intbl\qc {Status quo}\cell}
\row}
} \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 Intuitively, it is extremely irrational for Barry to take the bet. People do make mistakes about identifying musicians, even very familiar musicians, by the strains of music that drift up from a park. It\rquote s not worth risking a millenium of torture for $10.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 But it also seems that we\rquote ve misstated the table. Before the genie showed up, it seemed clear that Barry knew that the musician was Beth. That was why he went out to see her perform. (If you don\rquote t think this is true, make the sounds from the park clearer, or make it that Barry had some prior evidence that Beth was performing which the sounds from the park remind him of. It shouldn\rquote t be too hard to come up with an evidential base such that (a) in normal circumstances we\rquote d say Barry knew who was performing, but (b) he shouldn\rquote t take this genie\rquote s bet.) Now our decision tables should reflect the knowledge of the agent making the decision. If Barry knows that the musician is Beth, then the second column is one he knows will not obtain. Including it is like including a column for what will happen if the genie is lying about the consequences of taking or declining the bet. So let\rquote s write the table in the standard form.\par
{{\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 \par
{\trowd\cellx2969\cellx6655\cellx6898
{\pard\intbl\qr {}\cell}
{\pard\intbl\qc {{\b Musician is Beth}}\cell}
{\pard\intbl\qc {}\cell}
\row}
{\trowd\cellx2969\cellx6655
{\pard\intbl\qr {{\b Take Bet}}\cell}
{\pard\intbl\qc {Win $10}\cell}
\row}
{\trowd\cellx2969\cellx6655\cellx6898
{\pard\intbl\qr {{\b Decline Bet}}\cell}
{\pard\intbl\qc {Status quo}\cell}
{\pard\intbl\qc {}\cell}
\row}
} \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 And it is clear what Barry\rquote s decision should be in this situation. Taking the bet dominates declining it, and Barry should take dominating options.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 What has happened?  It is incredibly clear that Barry should decline the bet, yet here we have an argument that he should take the bet. If you accept that the bet should be declined, then there are three options available it seems to me.\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 1.\tab
Barry never knew that the musician was Beth. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 2.\tab
Barry did know that the musician was Beth, but this knowledge was destroyed by the genie\rquote s offer of the bet. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 3.\tab
States of the world that are known not to obtain should still be represented in decision problems, so taking the bet is not a dominating option. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 The first option is basically a form of scepticism. If the take-away message from the above discussion is that Barry doesn\rquote t know the musician is Beth, we can mount a similar argument to show that he knows next to nothing.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} The idea that interest-relativity is a way of fending off scepticism is a very prominent theme in 
[{\field{\*\fldinst{\lang1024 REF BIB_FantlMcGrath2009 \\* MERGEFORMAT }}{\fldrslt{{6}{2009}{{Fantl and McGrath}}{{}}}}}
].}
 And the third option would send us back into the problems about interpreting and applying decision theory that we spend the first few pages trying to get out of.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 So it seems that the best solution here, or perhaps the least bad solution, is to accept that knowledge is interest-relative. Barry did know that the musician was Beth, but the genie\rquote s offer destroyed that knowledge. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The argument here bears more than a passing resemblance to the arguments in favour of interest-relativity that are made by Hawthorne, Stanley and Fantl and McGrath. But I think the focus on decision theory shows how we getting to interest-relativity with slightly weaker premises than they are using.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} As they make clear in 
[{\field{\*\fldinst{\lang1024 REF BIB_Hawthorne2008_HAWKAA \\* MERGEFORMAT }}{\fldrslt{{11}{2008}{{Hawthorne and Stanley}}{{}}}}}
], Hawthorne and Stanley are interested in defending relatively strong premises linking knowledge and action independently of the argument for the interest-relativity of knowledge. What I\rquote m doing here is showing how that conclusion does not rest on anything nearly as strong as the principles they believe, and so there is plenty of space to disagree with their general principles, but accept interest-relativity.}
 In particular, the only premises I\rquote ve used to derive an interest-relative conclusion are:\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 1.\tab
Before the genie showed up, Barry knew the musician was Beth. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 2.\tab
It\rquote s rationally permissible, {\i in cases like Barry\rquote s}, to take dominating options. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 3.\tab
It\rquote s always right to model decision problems by including what the agent knows in the \lquote framework\rquote , i.e., the specification of the options and payouts. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 4.\tab
It is rationally impermissible for Barry to take the genie\rquote s offered bet. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 The second premise there is {\i much} weaker than the principles linking knowledge and action defended in previous arguments for interest-relativity. It isn\rquote t the claim that one can always act on what one knows, or that one can only act on what one knows, or that knowledge always (or only) provides reason to act. It\rquote s just the claim that in one very specific type of situation, in particular when one has to make a relatively simple bet, which affects nobody but the person making the bet, it\rquote s rationally permissible to take a dominating option. In conjunction with the third premise, it entails that {\i in those kind of cases}, the fact that one knows taking the bet will lead to a better outcome suffices for making acceptance of the bet rationally permissible. It doesn\rquote t say anything about what else might or might not make acceptance rationally permissible. It doesn\rquote t say anything about what suffices for rationally permissibility in other kinds of cases, such as cases where someone else\rquote s interests are at stake, or where taking the bet might violate a deontological constraint, or any other way in which real-life choices differ from the simplest decision problems.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} I have more to say about those cases in section 2.2.}
 It doesn\rquote t say anything about any other kind of permissibility, e.g., moral permissibility. But it doesn\rquote t need to, because we\rquote re only in the business of proving that there is {\i some} interest-relativity to knowledge, and an assumption about practical rationality in some range of cases suffices to prove that.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The case of Barry and Beth also bears some relationship to one of the kinds of case that have motivated contextualism about knowledge. Indeed, it has been widely noted in the literature on interest-relativity that interest-relativity can explain away many of the puzzles that motivate contextualism. And there are difficulties that face any contextualist theory {Weatherson2006-WEAQC}. So I prefer an {\i invariantist} form of interest-relativity about knowledge. That is, my view is a form of interest-relative-invariantism, or IRI.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} This is obviously not a full argument against contextualism; that would require a much longer paper than this.}
\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Now everything I\rquote ve said here leaves it open whether the interest-relativity of knowledge is a natural and intuitive theory, or whether it is a somewhat unhappy concession to difficulties that the case of Barry and Beth raise. I think the former is correct, and interest-relativity is fairly plausible on its own merits, but it would be consistent with my broader conclusions to say that in fact the interest-relative theory of knowledge is very implausible and counterintuitive. If we said that, we could still justify the interest-relative theory by noting that we have on our hands here a paradoxical situation, and any option will be somewhat implausible. This consideration has a bearing on how we should think about the role of intuitions about cases, or principles, in arguments that knowledge is interest-relative. Several critics of the view have argued that the view is counter-intuitive, or that it doesn\rquote t accord with the reactions of non-expert judges.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} See, for instance, 
[{\field{\*\fldinst{\lang1024 REF BIB_MBT2009 \\* MERGEFORMAT }}{\fldrslt{{1}{2009}{{Blome-Tillmann}}{{}}}}}
], or 
[{\field{\*\fldinst{\lang1024 REF BIB_FeltzZarpentine2010 \\* MERGEFORMAT }}{\fldrslt{{7}{forthcoming}{{Feltz and Zarpentine}}{{}}}}}
].}
 In a companion paper, \ldblquote Defending Interest-Relative Invariantism\rdblquote , I note that those arguments usually misconstrue what the consequences of interest-relative theories of knowledge are. But even if they don\rquote t, I don\rquote t think there\rquote s any quick argument that if interest-relativity is counter-intuitive, it is false. After all, the only alternatives that seem to be open here are very counter-intuitive.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Finally, it\rquote s worth noting that if Barry is rational, he\rquote ll stop (fully) believing that the musician is Beth once the genie makes the offer. Assuming the genie allows this, it would be very natural for Barry to try to acquire more information about the singer. He might walk over to the window to see if he can see who is performing in the park. So this case leaves it open whether the interest-relativity of knowledge can be explained fully by the interest-relativity of belief. I used to think it could be; I no longer think that. To see why this is so, it\rquote s worth rehearsing how the interest-relative theory of belief runs.\par
\pard\plain\s3\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb240 \fi0 1.2  The Interest-Relativity of Belief\par
\pard\plain\s4\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb180 \fi0 1.2.1  Interests and Functional Roles\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 The previous section was largely devoted to proving an existential claim: there is {\i some} interest-relativity to knowledge. Or, if you prefer, it proved a negative claim: the best theory of knowledge is {\i not} interest-neutral. But this negative conclusion invites a philosophical challenge: what is the best explanation of the interest-relativity of knowledge?  My answer is in two parts. Part of the interest-relativity of knowledge comes from the interest-relativity of belief, and part of it comes from the fact that interests generate certain kinds of doxastic defeaters.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 We can see that belief is interest-relative by seeing that the best functionalist theory of belief is interest-relative. Since the best functionalist theory of belief is the true theory of belief, that means belief is interest-relative. That last sentence assumed a non-trivial premise, namely that functionalism about belief is true. I\rquote m not going to argue for that, since the argument would take at least a book. (The book in question might look a lot like 
[{\field{\*\fldinst{\lang1024 REF BIB_DBMJackson2007 \\* MERGEFORMAT }}{\fldrslt{{2}{2007}{{Braddon-Mitchell and Jackson}}{{}}}}}
].) But I am going to, in this section, argue for what I said in the first sentence of this paragraph, namely that the best functionalist account of belief is interest-relative.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 In my {Weatherson2005-WEACWD}, I suggested that was all of the explanation of the interest-relativity of knowledge. I was wrong, and section 3 of this paper will show why I was wrong. Interests also generate certain kinds of doxastic defeaters, and they enter independently into the explanation of why knowledge is interest-relative. Very roughly, the idea is that {{\i S}} doesn\rquote t know {{\i p}} if {{\i S}}\rquote s belief that {{\i p}} does not sufficiently cohere with the rest of what she should believe and does believe. If this coherence constraint fails, there is a doxastic defeater. When is some incoherence too much incoherence for knowledge?  That turns out to be interest-relative, in cases like the case of Coraline described in the next section. But we\rquote re getting ahead of ourselves, the first task is to link functionalism about belief and the interest-relativity of belief.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 We start not with the functional characterisaton of belief, but with something closeby, the functional characterisation of credence. Frank 
[{\field{\*\fldinst{\lang1024 REF BIB_RamseyTruthProb \\* MERGEFORMAT }}{\fldrslt{{20}{1926}{{Ramsey}}{{}}}}}
] provides a clear statement of one of the key functional roles of credences; their connection to action. Of course, Ramsey did not take himself to be providing one component of the functional theory of credence. He took himself to be providing a behaviourist/operationalist reduction of credences to dispositions. But we do not have to share Ramsey\rquote s metaphysics to use his key ideas. Those ideas include that it\rquote s distinctively {\i betting} dispositions that are crucial to the account of credence, and that all sorts of actions in everyday life constitute bets.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The connection to betting behaviour lives on today most prominently in the work on \lquote representation theorems\rquote .{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} See 
[{\field{\*\fldinst{\lang1024 REF BIB_Maher1993 \\* MERGEFORMAT }}{\fldrslt{{19}{1993}{{Maher}}{{}}}}}
] for the most developed account in recent times.}
 What a representation theorem shows is that for any agent whose pairwise preferences satisfy some structural constraints, there is a probability function and a utility function such that the agent prefers bet {{\i X}} to bet {{\i Y}} just in case the expected utility of {{\i X}} (given that probability and utility function) is greater than that of {{\i Y}}. Moreover, the probability function is unique (and the utility function is unique up to positive affine transformations). Given that, it might seem plausible to identify the agent\rquote s credence with this probability function, and the agent\rquote s (relative) values with this utility function.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Contemporary functionalism goes along with much, but not quite all, of this picture. The betting preferences are an important part of the functional role of a credence; indeed, they just are the output conditions. But there are two other parts to a functional role: an input condition and a set of internal connections. So the functionalist thinks that the betting dispositions are not quite sufficient for having credences. A pre-programmed automaton might have dispositions to accept (or at least move as if accepting) various bets, but this will not be enough for credences {DBMJackson2007}. So when we\rquote re considering whether someone is in a particular credal state, we have to consider not just their actions, and their disposition to act, but the connections between the alleged credal state and other states.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The same will be true of belief. Part of what it is to believe {{\i p}} is to act as if {{\i p}} is true. Another part is to have other mental states that make sense in light of {{\i p}}. This isn\rquote t meant to rule out the possibility of inconsistent beliefs. As David 
[{\field{\*\fldinst{\lang1024 REF BIB_Lewis1982c \\* MERGEFORMAT }}{\fldrslt{{16}{1982}{{Lewis}}{{}}}}}
] points out, it is easy to have inconsistent beliefs if we don\rquote t integrate our beliefs fully. What it is meant to rule out is the possibility of a belief that is not at all integrated into the agent\rquote s cognitive system. If the agent believes {{\i q}}, but refuses to infer {{\i p}{\u8743*}{\i q}}, and believes {{\field{\*\fldinst{ EQ {\i p}{\u8594*}{\i r}}}{\fldrslt }}
}, but refuses to infer {{\i r}}, and so on for enough other beliefs, she doesn\rquote t really believe {{\i p}}. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 When writers start to think about the connection between belief and credence, they run into the following problem fairly quickly. The alleged possibility where {{\i S}} believes {{\i p}}, and doesn\rquote t believe {{\i q}}, but her credence in {{\i q}} is higher than her credence in {{\i p}} strikes many theorists as excessively odd.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} I\rquote m going to argue below that cases like that of Barry and Beth suggest that in practice this isn\rquote t nearly as odd as it first seems.}
 That suggests the so-called \lquote threshold view\rquote  of belief, that belief is simply credence above a threshold. It is also odd to say that a rational, reflective agent could believe {{\i p}}, believe {{\i q}}, yet take it as an open question whether {{\i p}{\u8743*}{\i q}} is true, refusing to believe or disbelieve it. Finally, it seems we can believe things to which we don\rquote t give credence 1. In the case of Barry and Beth from section 1, for example, before the genie comes in, it seems Barry does believe the musician is Beth. But he doesn\rquote t have credence 1 in this, since having credence 1 means being disposed to make a bet at any odds. And we can\rquote t have all three of these ideas. That is, we can\rquote t accept the \lquote threshold view\rquote , the closure of belief under conjunction, and the possibility that propositions with credence less than one are believed.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 We can raise the same kind of problem by looking directly at functional roles. A key functional role of credences is that if an agent has credence {{\i x}} in {{\i p}} she should be prepared to buy a bet that returns 1 util if {{\i p}}, and 0 utils otherwise, iff the price is no greater than {{\i x}} utils. A key functional role of belief is that if an agent believes {{\i p}}, and recognises that {{\u966*}} is the best thing to do given {{\i p}}, then she\rquote ll do {{\u966*}}. Given {{\i p}}, it\rquote s worth paying any price up to 1 util for a bet that pays 1 util if {{\i p}}. So believing {{\i p}} seems to mean being in a functional state that is like having credence 1 in {{\i p}}. But as we argued in the previous paragraph, it is wrong to identify belief with credence 1.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 If we spell out more carefully what the functional states of credence and belief are, a loophole emerges in the argument that belief implies credence 1. The interest-relative theory of belief exploits that loophole. What\rquote s the difference, in functional terms, between having credence {{\i x}} in {{\i p}}, and having credence {{\i x}+{\u949*}} in {{\i p}}?  Well, think again about the bet that pays 1 util if {{\i p}}, and 0 utils otherwise. And imagine that bet is offered for {{\i x}+{{\u949*}}{2}} utils. The person whose credence is {{\i x}} will decline the offer; the person whose credence is {{\i x}+{\u949*}} will accept it. Now it will usually be that no such bet is on offer.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} There are exceptions, especially in cases where {{\i p}} concerns something significant to financial markets, and the agent trades financial products. If you work through the theory that I\rquote m about to lay out, one consequence is that such agents should have very few unconditional beliefs about financially-sensitive information, just higher and lower credences. I think that\rquote s actually quite a nice outcome, but I\rquote m not going to rely on that in the argument for the view.}
 No matter; as long as one agent is {\i disposed} to accept the offer, and the other agent is not, that suffices for a difference in credence.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The upshot of that is that differences in credences might be, indeed usually will be, constituted by differences in dispositions concerning how to act in choice situations far removed from actuality. I\rquote m not usually in a position of having to accept or decline a chance to buy a bet for 0.9932 utils that the local coffee shop is currently open. Yet whether I would accept or decline such a bet matters to whether my credence that the coffee shop is open is 0.9931 or 0.9933. This isn\rquote t a problem with the standard picture of how credences work. It\rquote s just an observation that the high level of detail embedded in the picture relies on taking the constituents of mental states to involve many dispositions.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 It isn\rquote t clear that belief should be defined in terms of the same kind of dispositions involving better behaviour in remote possibilities. It\rquote s true that if I believe that {{\i p}}, and I\rquote m rational enough, I\rquote ll act as if {{\i p}} is true. Is it also true that if I believe {{\i p}}, I\rquote m disposed to act as if {{\i p}} is true no matter what choices are placed in front of me?  I don\rquote t see any reason to say yes, and there are a few reasons to say no. As we say in the case of Barry and Beth, Barry can believe that {{\i p}}, but be disposed to {\i lose that belief} rather than act on it if odd choices, like that presented by the genie, emerge.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 This suggests the key difference between belief and credence 1. For a rational agent, a credence of 1 in {{\i p}} means that the agent is disposed to answer a wide range of questions the same way she would answer that question conditional on {{\i p}}. That follows from the fact that these four principles are trivial theorems of the orthodox theory of expected utility.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} The presentation in this section, as in 
[{\field{\*\fldinst{\lang1024 REF BIB_Weatherson2005_WEACWD \\* MERGEFORMAT }}{\fldrslt{{26}{2005}{{Weatherson}}{{}}}}}
], assumes at least a weak form of consequentialism. This was arguably a weakness of the earlier paper. We\rquote ll return to the issue of what happens in cases where the agent doesn\rquote t, and perhaps shouldn\rquote t, maximise expected utility, at the end of the section.}
 \par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b C1AP} For all {{\i q},{\i x}}, if {{\plain Pr}({\i p})=1} then {{\plain Pr}({\i q})={\i x}} iff {{\plain Pr}({\i q}|{\i p})={\i x}}. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b C1CP} For all {{\i q},{\i r}}, if {{\plain Pr}({\i p})=1} then {{\plain Pr}({\i q}){\u8805*}{\plain Pr}({\i r})} iff {{\plain Pr}({\i q}|{\i p}){\u8805*}{\plain Pr}({\i r}|{\i p})}. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b C1AU} For all {{\u966*},{\i x}}, if {{\plain Pr}({\i p})=1} then {{\i U}({\u966*})={\i x}} iff {{\i U}({\u966*}|{\i p})={\i x}}. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b C1CP} For all {{\u966*},{\u968*}}, if {{\plain Pr}({\i p})=1} then {{\i U}({\u966*}){\u8805*}{\i U}({\u968*})} iff {{\i U}({\u966*}|{\i p}){\u8805*}{\i U}({\u968*}|{\i p})}. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 In the last two lines, I use {{\i U}({\u966*})} to denote the expected utility of {{\u966*}}, and {{\i U}({\u966*}|{\i p})} to denote the expected utility of {{\u966*}} conditional on {{\i p}}. It\rquote s often easier to write this as simply {{\i U}({\u966*}{\u8743*}{\i p})}, since the utility of {{\u966*}} conditional on {{\i p}} just is the utility of doing {{\u966*}} in a world where {{\i p}} is true. That is, it is the utility of {{\u966*}{\u8743*}{\i p}} being realised. But we get a nicer symmetry between the probabilistic principles and the utility principles if we use the explictly conditional notation for each.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 If we make the standard kinds of assumptions in orthodox decision theory, i.e., assume at least some form of probabilism and consequentialism{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} I mean here consequentialism in roughly the sense used by 
[{\field{\*\fldinst{\lang1024 REF BIB_Hammond1988 \\* MERGEFORMAT }}{\fldrslt{{9}{1988}{{Hammond}}{{}}}}}
].}
, then the agent will answer each of these questions the same way simpliciter and conditional on {{\i p}}.\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
How probable is {{\i q}}?  \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
Is {{\i q}} or {{\i r}} more probable?  \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
How good an idea is it to do {{\u966*}}?  \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
Is it better to do {{\u966*}} or {{\u968*}}?  \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 Each of those questions is schematic. As in the more technical versions given above, they quantify over propositions and actions, albeit tacitly in the case of these versions. And these quantifiers have a very large domain. The standard theory is that an agent whose credence in {{\i p}} is 1 will have the same credence in {{\i q}} as in {{\i q}} given {{\i p}} for any {{\i q}} whatsoever.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Now in one sense, exactly the same things are true if the agent believes {{\i p}}. If one is wondering whether {{\i q}} or {{\i r}} is more probable, and one believes {{\i p}}, then the fact that {{\i p}} will be taken as a given in structring this inquiry. So conditionalising on {{\i p}} should not change the answer to the question. And the same goes for any actions {{\u966*}} and {{\u968*}} that the agent is choosing between. But this isn\rquote t true unrestrictedly. That\rquote s what we saw in the case of Barry and Beth. Which choices are on the table will change which things the agent will take as given, will use to structure inquiry, will believe. If we return to the technical version of our four questions, we might put all this the following way.\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b BAP} For all relevant {{\i q},{\i x}}, if {{\i p}} is believed then {{\plain Pr}({\i q})={\i x}} iff {{\plain Pr}({\i q}|{\i p})={\i x}}. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b BCP} For all relevant {{\i q},{\i r}}, if {{\i p}} is believed then {{\plain Pr}({\i q}){\u8805*}{\plain Pr}({\i r})} iff {{\plain Pr}({\i q}|{\i p}){\u8805*}{\plain Pr}({\i r}|{\i p})}. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b BAU} For all relevant {{\u966*},{\i x}}, if {{\i p}} is believed then {{\i U}({\u966*})={\i x}} iff {{\i U}({\u966*}|{\i p})={\i x}}. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b BCP} For all relevant {{\u966*},{\u968*}}, if {{\i p}} is believed then {{\i U}({\u966*}){\u8805*}{\i U}({\u968*})} iff {{\i U}({\u966*}|{\i p}){\u8805*}{\i U}({\u968*}|{\i p})}. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 This is where interests, theoretical or practical, matter. The functional definition of belief looks a lot like the functional definition of credence 1. But there\rquote s one key difference. Both definitions involve quantifiers: quantifying over propositions, actions and values. In the definition of credence 1, those quantifiers are (largely) unrestricted. In the definition of belief, those quantifiers are tightly restricted, and the restrictions are in terms of the interests, practical and theoretical, of the agent.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 In the earlier paper I went into a lot of detail about just what \lquote relevant\rquote  means in this context, and I won\rquote t repeat that here. But I will say a little about one point I didn\rquote t sufficiently stress in that paper: the importance of the restriction of {\b BAP} and {\b BAU} to {\i relevant} values of {{\i x}}. This lets us have the following nice consequence.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Charlie is trying to figure out exactly what the probability of {{\i p}} is. That is, for any {{\i x}{\u8712*}[0,1]}, whether {{\plain Pr}({\i p})={\i x}} is a relevant question. Now Charlie is well aware that {{\plain Pr}({\i p}|{\i p})=1}. So unless {{\plain Pr}({\i p})=1}, Charlie will give a different answer to the questions {\i How probable is p? } and {\i Given p, how probable is p? }. So unless Charlie holds that {{\plain Pr}({\i p})} is 1, she won\rquote t count as believing that {{\i p}}. One consequence of this is that Charlie can\rquote t reason, \ldblquote The probability of {{\i p}} is exactly 0.978, so {{\i p}}.\rdblquote  That\rquote s all to the good, since that looks like bad reasoning. And it looks like bad reasoning even though in some circumstances Charlie can rationally believe propositions that she (rationally) gives credence 0.978 to.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 But note that the reasoning in the previous paragraph assumes that every question of the form {\i Is the probability of p equal to x? } is relevant. In practice, fewer questions than that will be relevant. Let\rquote s say that the only questions relevant to Charlie are of the form {\i What is the probability of {{\i p}} to one decimal place? }. And assume that no other questions become relevant in the course of her inquiry into this question.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} This is probably somewhat unrealistic. It\rquote s hard to think about whether {{\plain Pr}({\i p})} is closer to 0.7 or 0.8 without raising to salience questions about, for example, what the second decimal place in {{\plain Pr}({\i p})} is. This is worth bearing in mind when coming up with intuitions about the cases in this paragraph.}
 Charlie decides that to the first decimal place, {{\plain Pr}({\i p})=1.0}, i.e., {{\plain Pr}({\i p})>0.95}. That is compatible with simply believing that {{\i p}}. And that seems right; if for practical purposes, the probability of {{\i p}} is indistinguishable from 1, then the agent is confident enough in {{\i p}} to believe it.\par
\pard\plain\s4\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb120 \fi0 1.2.2  Two Caveats\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 The theory sketched so far seems to me right in the vast majority of cases. It fits in well with a broadly functionalist view of the mind, and it handles difficult cases, like that of Kate, nicely. But it needs to be supplemented and clarified a little to handle some other difficult cases. In this section I\rquote m going to supplement the theory a little to handle what I call \lquote impractical propositions\rquote , and say a little about morally loaded action.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Jones has a false geographic belief: he believes that Los Angeles is west of Reno, Nevada.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} I\rquote m borrowing this example from Fred Dretske, who uses it to make some interesting points about dispositional belief.}
 This isn\rquote t because he\rquote s ever thought about the question. Rather, he\rquote s just disposed to say \ldblquote Of course\rdblquote  if someone asks, \ldblquote Is Los Angeles west of Reno? \rdblquote  That disposition has never been triggered, because no one\rquote s ever bothered to ask him this. Call the proposition that Los Angeles is west of Reno {{\i p}}. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The theory given so far will get the right result here: Jones does believe that {{\i p}}. But it gets the right answer for an odd reason. Jones, it turns out, has very little interest in American geography right now. He\rquote s a schoolboy in St Andrews, Scotland, getting ready for school and worried about missing his schoolbus. There\rquote s no inquiry he\rquote s currently engaged in for which {{\i p}} is even close to relevant. So conditionalising on {{\i p}} doesn\rquote t change the answer to any inquiry he\rquote s engaged in, but that would be true no matter what his credence in {{\i p}} is.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 There\rquote s an immediate problem here. Jones believes {{\i p}}, since conditionalising on {{\i p}} doesn\rquote t change the answer to any relevant inquiry. But for the very same reason, conditionalising on {{\u8800*}{\i p}} doesn\rquote t change the answer to any relevant inquiry. It seems our theory has the bizarre result that Jones believes {{\u8800*}{\i p}} as well. That is both wrong and unfair. We end up attributing inconsistent beliefs to Jones simply because he\rquote s a harried schoolboy who isn\rquote t currently concerned with the finer points of the geography of the American southwest.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Here\rquote s a way out of this problem in four relatively easy steps. First, we say that which questions are relevant questions is not just relative to the agent\rquote s interests, but also relevant to the proposition being considered. A question may be relevant relative to {{\i p}}, but not relative to {{\i q}}. Second, we say that relative to {{\i p}}, the question of whether to believe {{\i p}} is a relevant question. Third, we say that an agent only prefers believing {{\i p}} to not believing it if their credence in {{\i p}} is greater than their credence in {{\u8800*}{\i p}}, i.e., if their credence in {{\i p}} is greater than {{1}{2}}. Finally, we say that when the issue is whether the subject believes that {{\i p}}, the question of whether to believe {{\i p}} is not just a relvant question on its own, but it stays being a relevant question conditional on any {{\i q}} that is relevant to the subject. In the earlier paper {Weatherson2005-WEACWD} I argue that this solves the problem raised by impractical propositions in a smooth and principled way.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 That\rquote s the first caveat. The second is one that isn\rquote t discussed in the earlier paper. If the agent is merely trying to get the best outcome for themselves, then it makes sense to represent them as a utility maximiser. And within orthodox decision theory, it is easy enough to talk about, and reason about, conditional utilities. That\rquote s important, because conditional utilities play an important role in the theory of belief offered here. But if the agent faces moral constraints on her decision, it isn\rquote t always so easy to think about conditional utilities.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 When agents have to make decisions that might involve them causing harm to others if certain propositions turn out to be true, then I think it is best to supplement orthodox decision theory with an extra assumption. The assumption is, roughly, that for choices that may harm others, expected value is absolute value. It\rquote s easiest to see what this means using a simple case of three-way choice. The kind of example I\rquote m considering here has been used for (slightly) different purposes by Frank 
[{\field{\*\fldinst{\lang1024 REF BIB_Jackson1991 \\* MERGEFORMAT }}{\fldrslt{{12}{1991}{{Jackson}}{{}}}}}
]. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The agent has to do {{\u981*}} or {{\u968*}}. Failure to do either of these will lead to disaster, and is clearly unacceptable. Either {{\u981*}} or {{\u968*}} will avert the disaster, but one of them will be moderately harmful and the other one will not. The agent has time before the disaster to find out which of {{\u981*}} and {{\u968*}} is harmful and which is not for a nominal cost. Right now, her credence that {{\u981*}} is the harmful one is, quite reasonably, {{1}{2}}. So the agent has three choices:\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
Do {{\u981*}}; \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
Do {{\u968*}}; or \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
Wait and find out which one is not harmful, and do it. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 We\rquote ll assume that other choices, like letting the disaster happen, or finding out which one is harmful and doing it, are simply out of consideration. In any case, they are clearly dominated options, so the agent shouldn\rquote t do them. Let {{\i p}} be the propostion that {{\u981*}} is the harmful one. Then if we assume the harm in question has a disutility of 10, and the disutility of waiting to act until we know which is the harmful one is 1, the values of the possible outcomes are as follows:\par
{{\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 \par
{\trowd\cellx4679\cellx5452\cellx6898
{\pard\intbl\qr {}\cell}
{\pard\intbl\qc {{{\i p}}}\cell}
{\pard\intbl\qc {{{\u8800*}{\i p}}}\cell}
\row}
{\trowd\cellx4679\cellx5452\cellx6898
{\pard\intbl\qr {{\b Do {{\u981*}}}}\cell}
{\pard\intbl\qc {-10}\cell}
{\pard\intbl\qc {0}\cell}
\row}
{\trowd\cellx4679\cellx5452\cellx6898
{\pard\intbl\qr {{\b Do {{\u968*}}}}\cell}
{\pard\intbl\qc {0}\cell}
{\pard\intbl\qc {-10}\cell}
\row}
{\trowd\cellx4679\cellx5452\cellx6898
{\pard\intbl\qr {{\b Find out which is harmful}}\cell}
{\pard\intbl\qc {-1}\cell}
{\pard\intbl\qc {-1}\cell}
\row}
} \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 Given that {{\i P}{\i r}({\i p})={1}{2}}, it\rquote s easy to compute that the expected value of doing either {{\u981*}} or {{\u968*}} is -5, while the expected value of finding out which is harmful is -1, so the agent should find out which thing is to be done before acting. So far most consequentialists would agree, and so probably would most non-consequentialists for most ways of fleshing out the abstract example I\rquote ve described.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} Some consequentialists say that what the agent should do depends on whether {{\i p}} is true. If {{\i p}} is true, she should do {{\u968*}}, and if {{\i p}} is false she should do {{\u981*}}. As we\rquote ll see, I have reasons for thinking this is rather radically wrong.}
\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 But most consequentialists would also say something else about the example that I think is not exactly true. Just focus on the column in the table above where {{\i p}} is true. In that column, the highest value, 0, is alongside the action {\i Do} {{\u968*}}. So you might think that conditional on {{\i p}}, the agent should do {{\u968*}}. That is, you might think the conditional expected value of doing {{\u968*}}, conditional on {{\i p}} being true, is 0, and that\rquote s higher than the conditional expected value of any other act, conditional on {{\i p}}. If you thought that, you\rquote d certainly be in agreement with the orthodox decision-theoretic treatment of this problem.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 In the abstract statement of the situation above, I said that one of the options would be {\i harmful}, but I didn\rquote t say who it would be harmful to. I think this matters. I think what I called the orthodox treatment of the situation is correct when the harm accrues to the person making the decision. But when the harm accrues to another person, particularly when it accrues to a person that the agent has a duty of care towards, then I think the orthodox treatment isn\rquote t quite right.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 My reasons for this go back to Jackson\rquote s original discussion of the puzzle. Let the agent be a doctor, the actions {{\u981*}} and {{\u968*}} be her prescribing different medication to a patient, and the harm a severe allergic reaction that the patient will have to one of the medications. Assume that she can run a test that will tell her which medication the patient is allergic to, but the test will take a day. Assume that the patient will die in a month without either medication; that\rquote s the disaster that must be averted. And assume that the patient is is some discomfort that either medication would relieve; that\rquote s the small cost of finding out which medication is risk. Assume finally that there is no chance the patient will die in the day it takes to run the test, so the cost of running the test is really nominal.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 A good doctor in that situation will find out which medication the patient is allergic to before ascribing either medicine. It would be {\i reckless} to ascribe a medicine that is unnecessary and that the patient might be allergic to. It is worse than reckless if the patient is actually allergic to the medicine prescribed, and the doctor harms the patient. But even if she\rquote s lucky and prescribes the \lquote right\rquote  medication, the recklessness remains. It was still, it seems, the wrong thing for her to do.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 All of that is in Jackson\rquote s discussion of the case, though I\rquote m not sure he\rquote d agree with the way I\rquote m about the incorporate these ideas into the formal decision theory. Even under the assumption that {{\i p}}, prescribing {{\u968*}} is still wrong, because it is reckless. That should be incorporated into the values we ascribe to different actions in different circumstances. The way I do it is to associate the value of each action, in each circumstance, with its actual expected value. So the decision table for the doctor\rquote s decision looks something like this.\par
{{\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 \par
{\trowd\cellx4679\cellx5452\cellx6898
{\pard\intbl\qr {}\cell}
{\pard\intbl\qc {{{\i p}}}\cell}
{\pard\intbl\qc {{{\u8800*}{\i p}}}\cell}
\row}
{\trowd\cellx4679\cellx5452\cellx6898
{\pard\intbl\qr {{\b Do {{\u981*}}}}\cell}
{\pard\intbl\qc {-5}\cell}
{\pard\intbl\qc {-5}\cell}
\row}
{\trowd\cellx4679\cellx5452\cellx6898
{\pard\intbl\qr {{\b Do {{\u968*}}}}\cell}
{\pard\intbl\qc {-5}\cell}
{\pard\intbl\qc {-5}\cell}
\row}
{\trowd\cellx4679\cellx5452\cellx6898
{\pard\intbl\qr {{\b Find out which is harmful}}\cell}
{\pard\intbl\qc {-1}\cell}
{\pard\intbl\qc {-1}\cell}
\row}
} \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 In fact, the doctor is making a decision under certainty. She knows that the value of prescribing either medicine is -5, and the value of running the tests is -1, so she should run the tests.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 In general, when an agent has a duty to maximise the expected value of some quantity {{\i Q}}, then the value that goes into the agent\rquote s decision table in a cell is {\i not} the value of {{\i Q}} in the world-action pair the agent represents. Rather, it\rquote s the expected value of {{\i Q}} given that world-action pair. In situations like this one where the relevant facts (e.g., which medicine the patient is allergic to) don\rquote t affect the evidence the agent has, the decision is a decision under {\i certainty}. This is all as things should be. When you have obligations that are drawn in terms of the expected value of a variable, the actual values of that variable cease to be directly relevant to the decision problem.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 One upshot of these considerations is that when moral and epistemic considerations get entangled, for example when agents have a moral duty not to take certain kinds of risks, it can get tricky to apply the theory of belief developed here. In a separate paper (\ldblquote Defending Interest-Relative Invariantism\rdblquote ) I\rquote ve shown how this idea can help respond to some criticisms of similar views raised by Jessica 
[{\field{\*\fldinst{\lang1024 REF BIB_Brown2008_BROKAP \\* MERGEFORMAT }}{\fldrslt{{3}{2008}{{Brown}}{{}}}}}
].\par
\pard\plain\s3\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb240 \fi0 1.3  Fantl and McGrath on Interest-Relativity\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 Jeremy Fantl and Matthew McGrath {FantlMcGrath2009} have argued that my interest-relative theory of belief cannot explain all of the interest-relativity in epistemology. I\rquote m going to agree with their conclusion, but not with their premises. At this point you might suspect, dear reader, that you\rquote re about to be drawn into a micro-battle between two similar but not quite identical explanations of the same (alleged) phenomena. And you wouldn\rquote t be entirely mistaken. But I think seeing why Fantl and McGrath\rquote s objections to my theory fail will show us something interesting about the relationship between interests and knowledge. In particular, it will show us that interests can generate certain kinds of {\i defeaters} of claims to knowledge.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Fantl and McGrath\rquote s primary complaint against the interest-relative theory of belief I developed in my 
[{\field{\*\fldinst{\lang1024 REF BIB_Weatherson2005_WEACWD \\* MERGEFORMAT }}{\fldrslt{{26}{2005}{{Weatherson}}{{}}}}}
] and in the previous section is that it is not strong enough to entail principles such as (JJ).\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 {\b (JJ)} If you are justified in believing that {{\i p}}, then {{\i p}} is warranted enough to justify you in {{\u981*}}-ing, for any {{\u981*}}. 
[{\field{\*\fldinst{\lang1024 REF BIB_FantlMcGrath2009 \\* MERGEFORMAT }}{\fldrslt{{6}{2009}{{Fantl and McGrath}}{{}}}}}, 99
] \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 In practice, what this means is that there can\rquote t be a salient {{\i p},{\u981*}} such that:\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
The agent is justified in believing {{\i p}}; \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
The agent is not warranted in doing {{\u981*}}; but \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
If the agent had more evidence for {{\i p}}, and nothing else, the agent would be be warranted in doing {{\u981*}}. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 That is, once you\rquote ve got enough evidence, or warrant, for justified belief in {{\i p}}, then you\rquote ve got enough evidence for {{\i p}} as matters for any decision you face. This seems intuitive, and Fantl and McGrath back up its intuitiveness with some nicely drawn examples.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Now it\rquote s true that the interest-relative theory of belief cannot be used to derive (JJ), at least on the reading of it I just provided. But that\rquote s because on the intended reading, it is false, and the interest-relative theory is true. So the fact that (JJ) can\rquote t be derived is a feature, not a bug. The problem arises because of cases like that of Coraline. Here\rquote s what we\rquote re going to stipulate about Coraline.\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
She knows that {{\i p}} and {{\i q}} are independent, so her credence in any conjunction where one conjunct is a member of {\{{\i p},{\u8800*}{\i p}\}} and the other is a member of {\{{\i q},{\u8800*}{\i q}\}} will be the product of her credences in the conjuncts. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
Her credence in {{\i p}} is 0.99, just as the evidence supports. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
Her credence in {{\i q}} is also 0.99. This is unfortunate, since the rational credence in {{\i q}} given her evidence is 0.01. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
She has a choice between taking and declining a bet with the following payoff structure.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} I\rquote m more interested in the abstract structure of the case than in whether any real-life situation is modelled by just this structure. But it might be worth noting the rough kind of situation where this kind of situation can arise. So let\rquote s say Coraline has a particular bank account that is uninsured, but which currently paying 10% interest, and she is deciding whether to deposit another $1000 in it. Then {{\i p}} is the proposition that the bank will not collapse, and she\rquote ll get her money back, and {{\i q}} is the proposition that the interest will stay at 10%. To make the model exact, we have to also assume that if the interest rate on her account doesn\rquote t stay at 10%, it falls to 0.1%. And we have to assume that the interest rate and the bank\rquote s collapse are probabilistically independent. Neither of these are at all realistic, but a realistic case would simply be more complicated, and the complications would obscure the philosophically interesting point.}
 (Assume that the marginal utility of money is close enough to constant that expected dollar returns correlate more or less precisely with expected utility returns.) \par
}{{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb60 \fi0 \par
{\trowd\cellx1544\cellx3305\cellx5427\cellx6899
{\pard\intbl\qr {}\cell}
{\pard\intbl\qc {{\b {p{\u8743*}q}}}\cell}
{\pard\intbl\qc {{\b {p{\u8743*}{\u8800*}q}}}\cell}
{\pard\intbl\qc {{\b {{\u8800*}p}}}\cell}
\row}
{\trowd\cellx1544\cellx3305\cellx5427\cellx6899
{\pard\intbl\qr {{\b Take bet}}\cell}
{\pard\intbl\qc {$100}\cell}
{\pard\intbl\qc {$1}\cell}
{\pard\intbl\qc {{\u8722?$1000}}\cell}
\row}
{\trowd\cellx1544\cellx3305\cellx5427\cellx6899
{\pard\intbl\qr {{\b Decline bet}}\cell}
{\pard\intbl\qc {0}\cell}
{\pard\intbl\qc {0}\cell}
{\pard\intbl\qc {0}\cell}
\row}
} \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 As can be easily computed, the expected utility of taking the bet given her credences is positive, it is just over $89. And Coraline takes the bet. She doesn\rquote t compute the expected utility, but she is sensitive to it.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} If she did compute the expected utility, then one of the things that would be salient for her is the expected utility of the bet. And the expected utility of the bet is different to its expected utility given {{\i p}}. So if that expected utility is salient, she doesn\rquote t believe {{\i p}}. And it\rquote s going to be important to what follows that she {\i does} believe {{\i p}}.}
 That is, had the expected utility given her credences been close to 0, she would have not acted until she made a computation. But from her perspective this looks like basically a free $100, so she takes it. Happily, this all turns out well enough, since {{\i p}} is true. But it was a dumb thing to do. The expected utility of taking the bet given her evidence is negative, it is a little under -$8. So she isn\rquote t warranted, given her evidence, in taking the bet.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 I also claim the following three things are true of her.\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 1.\tab
{{\i p}} is not justified enough to warrant her in taking the bet. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 2.\tab
She believes {{\i p}}.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} In terms of the example discussed in the previous footnote, she believes that the bank will survive, i.e., that she\rquote ll get her money back if she deposits it.}
 \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 3.\tab
This belief is rational. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 The argument for 1 is straightforward. She isn\rquote t warranted in taking the bet, so {{\i p}} isn\rquote t sufficiently warranted to justify it. This is despite the fact that {{\i p}} is obviously relevant. Indeed, given {{\i p}}, taking the bet strictly dominates declining it. But still, {{\i p}} doesn\rquote t warrant taking this bet, because nothing warrants taking a bet with negative expected utility. Had the rational credence in {{\i p}} been higher, then the bet would have been reasonable. Had the reasonable credence in {{\i p}} been, say, 0.9999, then she would have been reasonable in taking the bet, and using {{\i p}} as a reason to do so. So there\rquote s a good sense in which {{\i p}} simply isn\rquote t warranted enough to justify taking the bet.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} I\rquote m assuming here that the interpretation of (JJ) that I gave above is correct, though actually I\rquote m not entirely sure about this. For present purposes, I plan to simply interpret (JJ) that way, and not return to exegetical issues.}
\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The argument for 2 is that she has a very high credence in {{\i p}}, this credence is grounded in the evidence in the right way, and it leads her to act as if {{\i p}} is true, e.g. by taking the bet. It\rquote s true that her credence in {{\i p}} is not 1, and if you think credence 1 is needed for belief, then you won\rquote t like this example. But if you think that, you won\rquote t think there\rquote s much connection between (JJ) and pragmatic conditions in epistemology either. So that\rquote s hardly a position a defender of Fantl and McGrath\rquote s position can hold.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} We do have to assume that {{\u8800*}{\i q}} is not so salient that attitudes conditional on {{\u8800*}{\i q}} are relevant to determining whether she believes {{\i p}}. That\rquote s because conditional on {{\u8800*}{\i q}}, she prefers to not take the bet, but conditional on {{\u8800*}{\i q}{\u8743*}{\i p}}, she prefers to take the bet. But if she is simply looking at this as a free $100, then it\rquote s plausible that {{\u8800*}{\i q}} is not salient.}
\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The argument for 3 is that her attitude towards {{\i p}} tracks the evidence perfectly. She is making no mistakes with respect to {{\i p}}. She is making a mistake with respect to {{\i q}}, but not with respect to {{\i p}}. So her attitude towards {{\i p}}, i.e. belief, is rational.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 I don\rquote t think the argument here strictly needs the assumption I\rquote m about to make, but I think it\rquote s helpful to see one very clear way to support the argument of the last paragraph. The working assumption of my project on interest-relativity has been that talking about beliefs and talking about credences are simply two ways of modelling the very same things, namely minds. If the agent both has a credence 0.99 in {{\i p}}, and believes that {{\i p}}, these are not two different states. Rather, there is one state of the agent, and two different ways of modelling it. So it is implausible, if not incoherent, to apply different valuations to the state depending on which modelling tools we choose to use. That is, it\rquote s implausible to say that while we\rquote re modelling the agent with credences, the state is rational, but when we change tools, and start using beliefs, the state is irrational. Given this outlook on beliefs and credences, premise 3 seems to follow immediately from the setup of the example.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 So that\rquote s the argument that (JJ) is false. And if it\rquote s false, the fact that the interest-relative theory doesn\rquote t entail it is a feature, not a bug. But there are a number of possible objections to that position. I\rquote ll spend the rest of this section, and this paper, going over them.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} Thanks here to a long blog comments thread with Jeremy Fantl and Matthew McGrath for making me formulate these points much more carefully. The original thread is at {\f3 http://tar.weatherson.org/2010/03/31/do-justified-beliefs-justify-action/}.}
\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 { The following argument shows that Coraline is not in fact justified in believing that {{\i p}}.\par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 1.\tab
{{\i p}} entails that Coraline should take the bet, and Coraline knows this. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 2.\tab
If {{\i p}} entails something, and Coraline knows this, and she justifiably believes {{\i p}}, she is in a position to justifiably believe the thing entailed. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 3.\tab
Coraline is not in a position to justifiably believe that she should take the bet. \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 4.\tab
So, Coraline does not justifiably believe that {{\i p}} \par
}}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 {The problem here is that premise 1 is false. What\rquote s true is that {{\i p}} entails that Coraline will be better off taking the bet than declining it. But it doesn\rquote t follow that she should take the bet. Indeed, it isn\rquote t actually true that she should take the bet, even though {{\i p}} is actually true. Not just is the entailment claim false, the world of the example is a counterinstance to it.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 It might be controversial to use this very case to reject premise 1. But the falsity of premise 1 should be clear on independent grounds. What {{\i p}} entails is that Coraline will be best off by taking the bet. But there are lots of things that will make me better off that I shouldn\rquote t do. Imagine I\rquote m standing by a roulette wheel, and the thing that will make me best off is betting heavily on the number than will actually come up. It doesn\rquote t follow that I should do that. Indeed, I should not do it. I shouldn\rquote t place any bets at all, since all the bets have a highly negative expected return. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 In short, all {{\i p}} entails is that taking the bet will have the best consequences. Only a very crude kind of consequentialism would identify what I should do with what will have the best returns, and that crude consequentialism isn\rquote t true. So {{\i p}} doesn\rquote t entail that Coraline should take the bet. So premise 1 is false.}\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 { Even though {{\i p}} doesn\rquote t {\i entail} that Coraline should take the bet, it does provide inductive support for her taking the bet. So if she could justifiably believe {{\i p}}, she could justifiably (but non-deductively) infer that she should take the bet. Since she can\rquote t justifiably infer that, she isn\rquote t justified in taking the bet. } {The inductive inference here looks weak. One way to make the inductive inference work would be to deduce from {{\i p}} that taking the bet will have the best outcomes, and infer from that that the bet should be taken. But the last step doesn\rquote t even look like a reliable ampliative inference. The usual situation is that the best outcome comes from taking an {\i ex ante} unjustifiable risk.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 It may seem better to use {{\i p}} combined with the fact that conditional on {{\i p}}, taking the bet has the highest {\i expected} utility. But actually that\rquote s still not much of a reason to take the bet. Think again about cases, completely normal cases, where the action with the best outcome is an {\i ex ante} unjustifiable risk. Call that action {{\u981*}}, and let {{\i B}{\u981*}} be the proposition that {{\u981*}} has the best outcome. Then {{\i B}{\u981*}} is true, and conditional on {{\i B}{\u981*}}, {{\u981*}} has an excellent expected return. But doing {{\u981*}} is still running a dumb risk. Since these kinds of cases are normal, it seems it will very often be the case that this form of inference leads from truth to falsity. So it\rquote s not a reliable inductive inference.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 More generally, we should worry quite a lot about Coraline\rquote s ability to draw inductive inferences about the propriety of the bet here. Unlike deductive inferences, inductive inferences can be defeated by a whole host of factors. If I\rquote ve seen a lot of swans, in a lot of circumstances, and they\rquote ve all been blue, that\rquote s a good reason to think the next swan I see will be blue. But it ceases to be a reason if I am told by a clearly reliable testifier that there are green swans in the river outside my apartment. And that\rquote s true even if I dismiss the testifier because I think he has a funny name, and I don\rquote t trust people with funny names. Now although Coraline has evidence for {{\i p}}, she also has a lot of evidence against {{\i q}}, evidence that she is presumably ignoring since her credence in {{\i q}} is so high. Any story about how Coraline can reason from {{\i p}} to the claim that she should have to take the bet will have to explain how her irrational attraction to {{\i q}} doesn\rquote t serve as a defeater, and I don\rquote t see how that could be done.}\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 {In the example, Coraline isn\rquote t just in a position to justifiably believe {{\i p}}, she is in a position to {\i know} that she justifiably believes it. And from the fact that she justifiably believes {{\i p}}, and the fact that if {{\i p}}, then taking the bet has the best option, she can infer that she should take the bet.} {It\rquote s possible at this point that we get to a dialectical impasse. I think this inference is non-deductive, because I think the example we\rquote re discussing here is one where the premises are true and the conclusion false. Presumably someone who doesn\rquote t like the example will think that it is a good deductive inference.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 What makes the objection useful is that, unlike the inductive inference mentioned in the previous objection, this at least has the {\i form} of a good inductive inference. Whenever you justifiably believe {{\i p}}, and the best outcome given {{\i p}} is gained by doing {{\u981*}}, then {\i usually} you should {{\u981*}}. Since Coraline knows the premises are true, ceteris paribus that gives her a reason to believe the premise is probably true.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 But other things aren\rquote t at all equal. In particular, this is a case where Coraline has a highly irrational credence concerning a proposition whose probability is highly relevant to the expected utility of possible actions. Or, to put things another way, an inference from something to something else it is correlated with can be defeated by related irrational beliefs. (That\rquote s what the swan example above shows.) So if Coraline tried to infer this way that she should take the bet, her irrational confidence in {{\i q}} would defeat the inference.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The objector might think I am being uncharitable here. The objection doesn\rquote t say that Coraline\rquote s knowledge provides an {\i inductive} reason to take the bet. Rather, they say, it provides a {\i conclusive} reason to take the bet. And conclusive reasons cannot be defeated by irrational beliefs elsewhere in the web. Here we reach an impasse. I say that knowledge that you justifiably believe {{\i p}} cannot provide a conclusive reason to bet on {{\i p}} because I think Coraline knows she justifiably believes {{\i p}}, but does not have a conclusive reason to bet on {{\i p}}. That if, I think the premise the objector uses here is false because I think (JJ) is false. The person who believes in (JJ) won\rquote t be so impressed by this move. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Having said all that, the more complicated example at the end of 
[{\field{\*\fldinst{\lang1024 REF BIB_Weatherson2005_WEACWD \\* MERGEFORMAT }}{\fldrslt{{26}{2005}{{Weatherson}}{{}}}}}
] was designed to raise the same problem without the consequence that if {{\i p}} is true, the bet is sure to return a positive amount. In that example, conditionalising on {{\i p}} means the bet has a positive expected return, but still possibly a negative return. But in that case (JJ) still failed. If accepting there are cases where an agent justifiably believes {{\i p}}, and knows this, but can\rquote t rationally bet on {{\i p}} is too much to accept, that more complicated example might be more persuasive. Otherwise, I concede that someone who believes (JJ) and thinks rational agents can use it in their reasoning will not think that a particular case is a counterexample to (JJ).}\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 {If Coraline were ideal, then she wouldn\rquote t believe {{\i p}}. That\rquote s because if she were ideal, she would have a lower credence in {{\i q}}, and if that were the case, her credence in {{\i p}} would have to be much higher (close to 0.999) in order to count as a belief. So her belief is not justified.} {The premise here, that if Coraline were ideal she would not believe that {{\i p}}, is true. The conclusion, that she is not justified in believing {{\i p}}, does not follow. It\rquote s always a mistake to {\i identify} what should be done with what is done in ideal circumstances. This is something that has long been known in economics. The {\i locus classicus} of the view that this is a mistake is 
[{\field{\*\fldinst{\lang1024 REF BIB_LipseyLancaster \\* MERGEFORMAT }}{\fldrslt{{18}{1956-1957}{{Lipsey and Lancaster}}{{}}}}}
]. A similar point has been made in ethics in papers such as 
[{\field{\*\fldinst{\lang1024 REF BIB_Watson1977 \\* MERGEFORMAT }}{\fldrslt{{24}{1977}{{Watson}}{{}}}}}
] and 
[{\field{\*\fldinst{\lang1024 REF BIB_KennettSmith1996b \\* MERGEFORMAT }}{\fldrslt{{13}{1996{a}}{{Kennett and Smith}}{{}}}}}, {\field{\*\fldinst{\lang1024 REF BIB_KennettSmith1996a \\* MERGEFORMAT }}{\fldrslt{{14}{1996{b}}{{Kennett and Smith}}{{}}}}}
]. And it has been extended to epistemology by 
[{\field{\*\fldinst{\lang1024 REF BIB_Williamson1998_WILCOK \\* MERGEFORMAT }}{\fldrslt{{28}{1998}{{Williamson}}{{}}}}}
].\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 All of these discussions have a common structure. It is first observed that the ideal is both {{\i F}} and {{\i G}}. It is then stipulated that whatever happens, the thing being created (either a social system, an action, or a cognitive state) will not be {{\i F}}. It is then argued that given the stipulation, the thing being created should not be {{\i G}}. That is not just the claim that we shouldn\rquote t {\i aim} to make the thing be {{\i G}}. It is, rather, that in many cases being {{\i G}} is not the best way to be, given that {{\i F}}-ness will not be achieved. Lipsey and Lancaster argue that (in an admittedly idealised model) that it is actually quite unusual for {{\i G}} to be best given that the system being created will not be {{\i F}}.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 It\rquote s not too hard to come up with examples that fit this structure. Following 
[{\field{\*\fldinst{\lang1024 REF BIB_Williamson2000_WILKAI \\* MERGEFORMAT }}{\fldrslt{{29}{2000}{{Williamson}}{{}}}}}, 209
], we might note that I\rquote m justified in believing that there are no ideal cognitive agents, although were I ideal I would not believe this. Or imagine a student taking a ten question mathematics exam who has no idea how to answer the last question. She knows an ideal student would correctly answer an even number of questions, but that\rquote s no reason for her to throw out her good answer to question nine. In general, once we have stipulated one departure from the ideal, there\rquote s no reason to assign any positive status to other similarities to the idea. In particular, given that Coraline has an irrational view towards {{\i q}}, she won\rquote t perfectly match up with the ideal, so there\rquote s no reason it\rquote s good to agree with the ideal in other respects, such as not believing {{\i p}}.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Stepping back a bit, there\rquote s a reason the interest-relative theory says that the ideal and justification come apart right here. On the interest-relative theory, like on any pragmatic theory of mental states, the {\i identification} of mental states is a somewhat holistic matter. Something is a belief in virtue of its position in a much broader network. But the {\i evaluation} of belief is (relatively) atomistic. That\rquote s why Coraline is justified in believing {{\i p}}, although if she were wiser she would not believe it. If she were wiser, i.e., if she had the right attitude towards {{\i q}}, the very same credence in {{\i p}} would not count as a belief. Whether her state counts as a belief, that is, depends on wide-ranging features of her cognitive system. But whether the state is justified depends on more local factors, and in local respects she is doing everything right.}\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 {Since the ideal agent in Coraline\rquote s position would not believe {{\i p}}, it follows that there is no {\i propositional} justification for {{\i p}}. Moreover, doxastic justification requires propositional justification{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} See 
[{\field{\*\fldinst{\lang1024 REF BIB_Turri2010 \\* MERGEFORMAT }}{\fldrslt{{23}{2010}{{Turri}}{{}}}}}
] for a discussion of recent views on the relationship between propositional and doxastic justification. This requirement seems to be presupposed throughout that literature.}
 So Coraline is not doxastically justified in believing {{\i p}}. That is, she isn\rquote t justified in believing {{\i p}}.} {I think there are two ways of understanding \lquote propositional justification\rquote . On one of them, the first sentence of the objection is false. On the other, the second sentence is false. Neither way does the objection go through.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The first way is to say that {{\i p}} is propositionally justified for an agent iff that agent\rquote s evidence justifies a credence in {{\i p}} that is high enough to count as a belief {\i given the agent\rquote s other credences and preferences}. On that understanding, {{\i p}} is propositionally justified by Coraline\rquote s evidence. For all that evidence has to do to make {{\i p}} justified is to support a credence a little greater than 0.9. And by hypothesis, the evidence does that.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The other way is to say that {{\i p}} is propositionally justified for an agent iff that agent\rquote s evidence justifies a credence in {{\i p}} that is high enough to count as a belief {\i given the agent\rquote s preferences and the credences supported by that evidence}. On this reading, the objection reduces to the previous objection. That is, the objection basically says that {{\i p}} is propositionally justified for an agent iff the ideal agent in her situation would believe it. And we\rquote ve already argued that that is compatible with doxastic justification. So either the objection rests on a false premise, or it has already been taken care of.}\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 {If Coraline is justified in believing {{\i p}}, then Coraline can use {{\i p}} as a premise in practical reasoning. If Coraline can use {{\i p}} as a premise in practical reasoning, and {{\i p}} is true, and her belief in {{\i p}} is not Gettiered, then she knows {{\i p}}. By hypothesis, her belief is true, and her belief is not Gettiered. So she should know {{\i p}}. But she doesn\rquote t know {{\i p}}. So by several steps of modus tollens, she isn\rquote t justified in believing {{\i p}}.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} Compare the \lquote subtraction argument\rquote  on page 99 of 
[{\field{\*\fldinst{\lang1024 REF BIB_FantlMcGrath2009 \\* MERGEFORMAT }}{\fldrslt{{6}{2009}{{Fantl and McGrath}}{{}}}}}
].}
} {Like the previous objection, this one turns on an equivocation, this time over the neologism \lquote Gettiered\rquote . Some epistemologists use this to simply mean that a belief is justified and true without constituting knowledge. By that standard, the third sentence is false. Or, at least, we haven\rquote t been given any reason to think that it is true. Given everything else that\rquote s said, the third sentence is a raw assertion that Coraline knows that {{\i p}}, and I don\rquote t think we should accept that.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 The other way epistemologists sometimes use the term is to pick out justified true beliefs that fail to be knowledge for the reasons that the beliefs in the original examples from 
[{\field{\*\fldinst{\lang1024 REF BIB_Gettier1963 \\* MERGEFORMAT }}{\fldrslt{{8}{1963}{{Gettier}}{{}}}}}
] fail to be knowledge. That is, it picks out a property that beliefs have when they are derived from a false lemma, or whatever similar property is held to be doing the work in the original Gettier examples. Now on this reading, Coraline\rquote s belief that {{\i p}} is not Gettiered. But it doesn\rquote t follow that it is known. There\rquote s no reason, once we\rquote ve given up on the JTB theory of knowledge, to think that whatever goes wrong in Gettier\rquote s examples is the {\i only} way for a justified true belief to fall short of knowledge. It could be that there\rquote s a practical defeater, as in this case. So the second sentence of the objection is false, and the objection again fails.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 But note I\rquote m conceding to the objector that Coraline does not know {{\i p}}. That\rquote s important, because it reveals another way in which knowledge is interest-relative. We can see that Coraline does not know that {{\i p}} by noting that if she did know {{\i p}}, we could represent her decision like this:\par
{{\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 \par
{\trowd\cellx2544\cellx4355\cellx6899
{\pard\intbl\qr {}\cell}
{\pard\intbl\qc {{\b {q}}}\cell}
{\pard\intbl\qc {{\b {{\u8800*}q}}}\cell}
\row}
{\trowd\cellx2544\cellx4355\cellx6899
{\pard\intbl\qr {{\b Take bet}}\cell}
{\pard\intbl\qc {$100}\cell}
{\pard\intbl\qc {$1}\cell}
\row}
{\trowd\cellx2544\cellx4355\cellx6899
{\pard\intbl\qr {{\b Decline bet}}\cell}
{\pard\intbl\qc {0}\cell}
{\pard\intbl\qc {0}\cell}
\row}
} \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 And now she should clearly take the bet, since it is a dominating option. Her irrational credence in {{\i q}} simply wouldn\rquote t matter. But she can\rquote t rationally take the bet, so this table must be wrong, so she doesn\rquote t know {{\i p}}.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 This seems odd. Why should an irrational credence in {{\i q}} defeat knowledge of {{\i p}}?  The answer is that beliefs have to sufficiently cohere with our other beliefs to be knowledge. If I have a very firm belief that {{\u8800*}{\i p}}, and also a belief that {{\i p}}, the latter belief cannot be knowledge even if it is grounded in the facts in just the right way. The contradictory belief that {{\u8800*}{\i p}} is a doxastic defeater.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Now in practice almost all of our beliefs are in some tension with some other subset of our beliefs. Unless we are perfectly coherent, which we never are, there will be lots of ways to argue that any particular belief does not sufficiently cohere with our broader doxastic state, and hence is defeated from being knowledge. There must be some restrictions on when incoherence with some part of the rest of one\rquote s beliefs defeats knowledge.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 I think an interest-relative story here is most likely to succeed. If we said Coraline knew that {{\i p}}, that would produce a tension between the fact that she does (rationally) believe that taking the bet is best given {{\i p}}, and that she should believe that declining the bet is not best {\i simpliciter}. And that matters because whether to take the bet or not is a live decision for her. That is, incoherencies or irrationalities that manifest in decision problems that are salient can defeat knowledge.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Put another way, an agent only knows {{\i p}} if we can properly model any decision she\rquote s really facing with a decision table where {{\i p}} is taken as given.{\cs62\super\chftn}
{\*\footnote\pard \s65\ql\fi-113\li397\lin397\f0\fs20{\cs62\super\chftn} Or, at least, we can model any such decision where standard decision theory applies. Perhaps we\rquote ll have to exclude cases where one of the options violates a deontological constraint. As noted in section 1, the argument for interest-relativity of knowledge is neutral on how to handle such cases.}
 Crediting Coraline with knowing {{\i p}} will mean we get the table for this decision, i.e., whether to take this very bet, wrong. And that\rquote s a live decision for her. Those two facts together entail that she doesn\rquote t know {{\i p}}. Neither alone would suffice for defeating knowledge of {{\i p}}. And that\rquote s a part of the explanation for the interest-relativity of knowledge, a part I left out of the story in my {Weatherson2005-WEACWD}. }\par
{\pard\plain\s61\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb120 \fi0 {\plain\b\fs32 Bibliography}\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \sb60 \li450\fi-450 [{\v\*\bkmkstart BIB_MBT2009}{1}{2009}{{Blome-Tillmann}}{{}}{\*\bkmkend BIB_MBT2009}]\tab
Blome-Tillmann, Michael. 2009. \ldblquote Contextualism, Subject-Sensitive Invariantism, and the Interaction of \lquote Knowledge\rquote -Ascriptions with Modal and Temporal Operators.\rdblquote  {\i Philosophy and Phenomenological Research} 79:315\endash 331, {http://dx.doi.org/10.1111/j.1933-1592.2009.00280.x}{doi:10.1111/j.1933-1592.2009.00280.x}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_DBMJackson2007}{2}{2007}{{Braddon-Mitchell and Jackson}}{{}}{\*\bkmkend BIB_DBMJackson2007}]\tab
Braddon-Mitchell, David and Jackson, Frank. 2007. {\i The Philosophy of Mind and Cognition, {\f0 second edition}}. Malden, MA: Blackwell.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Brown2008_BROKAP}{3}{2008}{{Brown}}{{}}{\*\bkmkend BIB_Brown2008_BROKAP}]\tab
Brown, Jessica. 2008. \ldblquote Knowledge and Practical Reason.\rdblquote  {\i Philosophy Compass} 3:1135\endash 1152, {http://dx.doi.org/10.1111/j.1747-9991.2008.00176.x}{doi:10.1111/j.1747-9991.2008.00176.x}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Cohen1988}{4}{1988}{{Cohen}}{{}}{\*\bkmkend BIB_Cohen1988}]\tab
Cohen, Stewart. 1988. \ldblquote How to be a Fallibilist.\rdblquote  {\i Philosophical Perspectives} 2:91\endash 123.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_DeRose1995}{5}{1995}{{DeRose}}{{}}{\*\bkmkend BIB_DeRose1995}]\tab
DeRose, Keith. 1995. \ldblquote Solving the Skeptical Problem.\rdblquote  {\i Philosophical Review} 104:1\endash 52.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_FantlMcGrath2009}{6}{2009}{{Fantl and McGrath}}{{}}{\*\bkmkend BIB_FantlMcGrath2009}]\tab
Fantl, Jeremy and McGrath, Matthew. 2009. {\i Knowledge in an Uncertain World}. Oxford: Oxford University Press.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_FeltzZarpentine2010}{7}{forthcoming}{{Feltz and Zarpentine}}{{}}{\*\bkmkend BIB_FeltzZarpentine2010}]\tab
Feltz, Adam and Zarpentine, Chris. forthcoming. \ldblquote Do You Know More When It Matters Less? \rdblquote  {\i Philosophical Psychology} Retrieved from {\f3 http://faculty.schreiner.edu/adfeltz/Papers/Know%20more.pdf}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Gettier1963}{8}{1963}{{Gettier}}{{}}{\*\bkmkend BIB_Gettier1963}]\tab
Gettier, Edmund\~L. 1963. \ldblquote Is Justified True Belief Knowledge? \rdblquote  {\i Analysis} 23:121\endash 123, {http://dx.doi.org/10.2307/3326922}{doi:10.2307/3326922}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Hammond1988}{9}{1988}{{Hammond}}{{}}{\*\bkmkend BIB_Hammond1988}]\tab
Hammond, Peter\~J. 1988. \ldblquote Consequentialist Foundations for Expected Utility.\rdblquote  {\i Theory and Decision} 25:25\endash 78, {http://dx.doi.org/10.1007/BF00129168}{doi:10.1007/BF00129168}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Hawthorne2004}{10}{2004}{{Hawthorne}}{{}}{\*\bkmkend BIB_Hawthorne2004}]\tab
Hawthorne, John. 2004. {\i Knowledge and Lotteries}. Oxford: Oxford University Press.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Hawthorne2008_HAWKAA}{11}{2008}{{Hawthorne and Stanley}}{{}}{\*\bkmkend BIB_Hawthorne2008_HAWKAA}]\tab
Hawthorne, John and Stanley, Jason. 2008. \ldblquote {Knowledge and Action}.\rdblquote  {\i Journal of Philosophy} 105:571\endash 90.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Jackson1991}{12}{1991}{{Jackson}}{{}}{\*\bkmkend BIB_Jackson1991}]\tab
Jackson, Frank. 1991. \ldblquote Decision Theoretic Consequentialism and the Nearest and Dearest Objection.\rdblquote  {\i Ethics} 101:461\endash 82.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_KennettSmith1996b}{13}{1996{a}}{{Kennett and Smith}}{{}}{\*\bkmkend BIB_KennettSmith1996b}]\tab
Kennett, Jeanette and Smith, Michael. 1996{a}. \ldblquote Frog and Toad Lose Control.\rdblquote  {\i Analysis} 56:63\endash 73, {http://dx.doi.org/10.1111/j.0003-2638.1996.00063.x}{doi:10.1111/j.0003-2638.1996.00063.x}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_KennettSmith1996a}{14}{1996{b}}{{Kennett and Smith}}{{}}{\*\bkmkend BIB_KennettSmith1996a}]\tab
\emdash . 1996{b}. \ldblquote Philosophy and Commonsense: The Case of Weakness of Will.\rdblquote  In Michaelis Michael and John O\rquote Leary-Hawthorne (eds.), {\i The Place of Philosophy in the Study of Mind}, 141\endash 157. Norwell, MA: Kluwer, {http://dx.doi.org/10.1017/CBO9780511606977.005}{doi:10.1017/CBO9780511606977.005}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Lewis1969a}{15}{1969}{{Lewis}}{{}}{\*\bkmkend BIB_Lewis1969a}]\tab
Lewis, David. 1969. {\i Convention: A Philosophical Study}. Cambridge: Harvard University Press.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Lewis1982c}{16}{1982}{{Lewis}}{{}}{\*\bkmkend BIB_Lewis1982c}]\tab
\emdash . 1982. \ldblquote Logic for Equivocators.\rdblquote  {\i No\'fb{}s} 16:431\endash 441. 97-110.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Lewis1996b}{17}{1996}{{Lewis}}{{}}{\*\bkmkend BIB_Lewis1996b}]\tab
\emdash . 1996. \ldblquote Elusive Knowledge.\rdblquote  {\i Australasian Journal of Philosophy} 74:549\endash 567, {http://dx.doi.org/10.1080/00048409612347521}{doi:10.1080/00048409612347521}. 418-446.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_LipseyLancaster}{18}{1956-1957}{{Lipsey and Lancaster}}{{}}{\*\bkmkend BIB_LipseyLancaster}]\tab
Lipsey, R.\~G. and Lancaster, Kelvin. 1956-1957. \ldblquote The General Theory of Second Best.\rdblquote  {\i Review of Economic Studies} 24:11\endash 32, {http://dx.doi.org/10.2307/2296233}{doi:10.2307/2296233}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Maher1993}{19}{1993}{{Maher}}{{}}{\*\bkmkend BIB_Maher1993}]\tab
Maher, Patrick. 1993. {\i Betting on Theories}. Cambridge: Cambridge University Press.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_RamseyTruthProb}{20}{1926}{{Ramsey}}{{}}{\*\bkmkend BIB_RamseyTruthProb}]\tab
Ramsey, Frank. 1926. \ldblquote Truth and Probability.\rdblquote  In D.\~H. Mellor (ed.), {\i Philosophical Papers}, 52\endash 94. Cambridge: Cambridge University Press.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Stalnaker2008}{21}{2008}{{Stalnaker}}{{}}{\*\bkmkend BIB_Stalnaker2008}]\tab
Stalnaker, Robert. 2008. {\i Our Knowledge of the Internal World}. Oxford: Oxford University Press.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Stanley2005_STAKAP}{22}{2005}{{Stanley}}{{}}{\*\bkmkend BIB_Stanley2005_STAKAP}]\tab
Stanley, Jason. 2005. {\i {Knowledge and Practical Interests}}. Oxford University Press.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Turri2010}{23}{2010}{{Turri}}{{}}{\*\bkmkend BIB_Turri2010}]\tab
Turri, John. 2010. \ldblquote On the Relationship between Propositional and Doxastic Justification.\rdblquote  {\i Philosophy and Phenomenological Research} 80:312\endash 326, {http://dx.doi.org/10.1111/j.1933-1592.2010.00331.x}{doi:10.1111/j.1933-1592.2010.00331.x}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Watson1977}{24}{1977}{{Watson}}{{}}{\*\bkmkend BIB_Watson1977}]\tab
Watson, Gary. 1977. \ldblquote Skepticism about Weakness of Will.\rdblquote  {\i Philosophical Review} 86:316\endash 339, {http://dx.doi.org/10.2307/2183785}{doi:10.2307/2183785}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Weatherson2003_WEAWGA}{25}{2003}{{Weatherson}}{{}}{\*\bkmkend BIB_Weatherson2003_WEAWGA}]\tab
Weatherson, Brian. 2003. \ldblquote {What Good Are Counterexamples? }\rdblquote  {\i Philosophical Studies} 115:1\endash 31, {http://dx.doi.org/10.1023/A:1024961917413}{doi:10.1023/A:1024961917413}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Weatherson2005_WEACWD}{26}{2005}{{Weatherson}}{{}}{\*\bkmkend BIB_Weatherson2005_WEACWD}]\tab
\emdash . 2005. \ldblquote {Can We Do Without Pragmatic Encroachment? }\rdblquote  {\i Philosophical Perspectives} 19:417\endash {}443, {http://dx.doi.org/10.1111/j.1520-8583.2005.00068.x}{doi:10.1111/j.1520-8583.2005.00068.x}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Weatherson2006_WEAQC}{27}{2006}{{Weatherson}}{{}}{\*\bkmkend BIB_Weatherson2006_WEAQC}]\tab
\emdash . 2006. \ldblquote Questioning Contextualism.\rdblquote  In Stephen\~Cade Hetherington (ed.), {\i Epistemology Futures}, 133\endash 147. Oxford: Oxford University Press.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Williamson1998_WILCOK}{28}{1998}{{Williamson}}{{}}{\*\bkmkend BIB_Williamson1998_WILCOK}]\tab
Williamson, Timothy. 1998. \ldblquote {Conditionalizing on Knowledge}.\rdblquote  {\i British Journal for the Philosophy of Science} 49:89\endash 121, {http://dx.doi.org/10.1093/bjps/49.1.89}{doi:10.1093/bjps/49.1.89}.\par
\pard\plain\s62\ql\fi-567\li567\sb0\sa0\f0\fs20\sl240\slmult1 \li450\fi-450 [{\v\*\bkmkstart BIB_Williamson2000_WILKAI}{29}{2000}{{Williamson}}{{}}{\*\bkmkend BIB_Williamson2000_WILKAI}]\tab
\emdash . 2000. {\i {Knowledge and its Limits}}. Oxford University Press.\par
}}}
