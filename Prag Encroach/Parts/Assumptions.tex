%Assumptions

Throughout this work, I'm going to make two prominent assumptions.

The first of these I've already flagged. It's the assumption that orthodox expected utility theory is, at least some of the time, a good theory of rational action. So, at least some of the time, the fact that an agent does not maximise expected utility implies that they are not acting rationally. 

This assumption needs to be clarified and qualified in a couple of respects. I mean to be endorsing \textit{causal}, not \textit{evidential} decision theory here, though I don't think anything here will turn on that. More substantively, I don't think that we should \textit{always} maximise expected utility. Arguably there are some moral constraints that are not reducible to constraints on the structure of the utility function.\footnote{There's a big debate on this point that I'm not going to get into here. See \cite{Portmore2009} for more details and references.} And even when we should maximise some function, it won't be clear just which function that is. I don't assume that `utility' always measures welfare, let alone some component of welfare like preference-satisfaction. But I think in some cases, like the three cases in the previous section, it is plausible that the agent should simply be trying to maximise their own expected welfare. That's not because of any broad theory about the nature of good action; it's simply that nothing else is relevant to their decisions for the next few minutes. As we'll see quite a bit in what follows, in cases where more than the agent's short-term welfare turns on a decision, describing and evaluating that decision can get more complicated.

The term `expected' in the reference to expected utility also calls for some unpacking. I assume that, at least some of the time, there is such a thing as the probability of various hypotheses given the agent's evidence. I don't assume this is always the case.\footnote{See \cite{Weatherson2007} for some discussion of why we shouldn't think things like epistemic probabilities are well defined in situations where the agent has very little evidence.} Nor do I assume this is always precise.\footnote{But see \cite{Weatherson2002-WEAKUA} for some discussion of why we can act as if probabilities are precise in at least some cases where they are not.} But I do assume that it is well-enough defined in enough cases that we can sensibly talk about the expected utility of various actions for an agent.

The second assumption is that some kind of functionalism about the mind is broadly true. In particular, I'm going to assume that the theory sketched in part one of \cite{Lewis1994b}, and filled out in much more detail in \cite{DBMJackson2007} is correct.\cite{Lewis1994b} says that he's not sure whether his view falls into the extension of 'functionalist' as that term is often used by philosophers. Whether that's true or not, I'm using 'functionalist' in such a way that Lewis's theory is a functionalist theory. Note that both Lewis and Braddon-Mitchell and Jackson do not take functionalism to be incompatible with an identity theory, and I won't either. Indeed, I'm going to make use of the idea that mental states are just brain states, and the role of functionalism is to tell us which functional role a brain state must fill to be a mental state, at a key point of the argument. I'm making these assumptions because I think they're true, and I think they provide a crucial insight into how we should answer the two questions I started with. It might be worried that readers less enamoured of functionalism will not find a work on epistemology for functionalists particularly interesting. I think such a worry is misplaced. Some people might find it interesting in the way that psychologists find morality for psychopaths interesting. More seriously, some readers might agree with me that if functionalism is true, the right epistemology has a certain structure, and take that to be evidence against functionalism. More hopefully, the fact that functionalist assumptions help resolve some tensions in modern epistemology might increase the credibility of functionalism in some people's eyes.

It's important to the kind of functionalism that I'm adopting that mental states have three kinds of proprietary functional roles. Mental states are associated with (a) input conditions, (b) output conditions, and (c) internal connections. So for a state to be a belief that \(p\), it should be (a) produced by evidence that indicates \(p\), (b) support the formation of other beliefs whose contents are supported by \(p\), and desires that are rational in a world where \(p\) is true, and (c) lead to action that maximises the agent's interests conditional on \(p\) being true. This is something of an idealisation; not all beliefs behave in just this way. But if a state rarely has one of these features among the class of creatures that includes the agent, then the functionalist will deny that it is really a belief.

In an earlier version of this work \citep{Weatherson2005-WEACWD}, I focussed on the second and third of these conditions. That led some writers to say that I was really talking about `acceptance', not about `belief'.\footnote{This seems to be what is suggested by \cite[149-151]{FantlMcGrath2009}, for instance.} I think that's because when people talk about acceptances, they mean mental states that satisfy clauses (b) or (c), or perhaps both, in the functionalist account of belief. Consider, for example, the case raised by L. Jonathan Cohen \citeyearpar[369]{Cohen1989} of the lawyer who doesn't believe his client is guilty, but organises the preparation of the defence around that assumption. The lawyer's state isn't a belief, presumably, because it doesn't have the right relationship to the evidence.\footnote{I'm actually a little suspicious that the case as described really works as a case of acceptance without belief. If the lawyer really acted as if the client is innocent, there are probably a few things he would do differently to how he would actually act. He might prepare more lines of appeal, because it's worth doing that for an innocent client. If it's a serious charge, he might enlist the support of academic and advocacy groups that help the wrongly accused. He might be more willing to have his child visit him at work while the client is there, etc. In practice it is rather rare for us to act exactly as if \(p\) is true, where \(p\) is relevant to action, and we have little reason to believe \(p\) is true. It's possible, which is enough for the theoretical distinction between acceptance and belief to be useful, but I think its frequency is sometimes overstated.}



%I’ve noticed, both in reading Fantl and McGrath’s book, and in talking to various people at Rutgers, that the position I took on pragmatic encroachment in Can We Do Without Pragmatic Encroachment? has often been misinterpreted. This has happened so often that I assume it is my fault. So here’s a nickel summary of the views of that paper. This isn’t quite what I currently believe, but it’s close. (Below I say a bit about how I’ve changed my views.)
%
%Functionalism is correct. That is, mental states are individuated functionally, and typically have three kinds of proprietary functional roles: relationships to inputs, relationships to other states, and relationships to outputs. The third of these is most important to the story here.
%Ramsey’s functional characterisation of credences is more or less right, at least as regards relationships to outputs. So, assuming the input and internal connections are in order, to have credence x in p is more or less to be willing to bet on p at odds 1 – x:x.
%Rational credences track evidential probabilities, in much the way Keynes suggested. So to have a rational credence in p just is to have one’s credence be (close enough to) the epistemic probability of p given E, where E is your actual evidence. (Note that there’s nothing pragmatic around yet, at least as long as evidence is not pragmatic.)
%The output condition for belief is that an agent (typically) believes that p iff for any A, B, the agent prefers A to B iff they prefer A ? p to B ? p. There are other conditions on belief (i.e. input conditions and internal connections), but this condition explains the relationship between stake variation and variation in justified belief.
%The previous point uses a tacit quantifier over actions. Actions A and B are in the relevant quantifier domain iff they are practically relevant to the agent. This is where stake variation impacts belief, and it is the only place that it does. So right now I believe that I’m listening to a Beatles song, but I wouldn’t continue to believe that if I had to bet my life on it. (It could be a very carefully done re-recording after all.) That’s because betting my life on this song being by the Beatles is not currently in the relevant quantifier domain, but could move into the quantifier domain if my practical situation changes.
%An agent has a rational belief in p iff they believe that p, and their credence in p is rational in the sense of point 3.
%As a consequence of all that, changing the stakes cannot change an agent from having a rational to having an irrational belief in p. But it can change them from having a rational belief in p to neither believing p nor being in a position to rationally believe that p.
%Some similar story holds for knowledge, though that part of the story is explicitly put off until a later paper. (And if you’d asked me at the time I’d have said that paper would have taken less than 5 years to write.)
%Here’s what I no longer think is correct in all that.
%
%I think the ‘internal connections’ part of the functional role is more important to interest-relativity than I thought at the time. I did (somewhat opaquely) discuss that role when discussing conjunction-introduction and related issues, but it should have been more upfront, and more detailed.
%I don’t think point 6 can be right, and in fact I suspect it fails in a way that undermines the larger project. The worry is that rationality doesn’t really require one’s credence exactly tracking the Keynesian epistemic probability. It at most requires that credence be close enough. But how close is close enough might be sensitive to pragmatic factors. I think this is similar to a worry that Fantl and McGrath raise, though they have different enough terminology to me that it’s a little hard to be sure.
%Point 8 really isn’t right. The problem is that irrational credences in other propositions seem more likely to defeat knowledge than to defeat rational belief.
%I’ll write more posts setting out those three bullet points, but for now I really just wanted to lay out for my own satisfaction an executive summary of Can We Do Without Pragmatic Encroachment.