\section{Fantl and McGrath on Interest-Relativity}

\section{Fantl and McGrath's Argument that Not Only Belief is Interest-Relative}

Fantl and McGrath's primary complaint against the Interest Relative Theory of Belief is that it is not strong enough to entail principles such as (JJ).

\begin{description}
\item[(JJ)] If you are justified in believing that \(p\), then \(p\) is warranted enough to justify you in \(\phi\)-ing, for any \(\phi\). \cite[99]{FantlMcGrath2009}
\end{description}

\noindent It's true that the Interest Relative Theory cannot be used to derive (JJ), at least on its intended reading. But that's because on the intended reading, it is false, and the Interest Relative Theory is true. So the fact that (JJ) can't be derived is a feature, not a bug.

The problem arises because of cases like the case we discussed at the end of chapter 1, namely cases where the agent has irrational beliefs elsewhere in her web of belief. Let's lay out the case carefully, because it will have a large role in the rest of this chapter. Here's what we're going to stipulate about \(S\).

\begin{itemize*}
\item She knows that \(p\) and \(q\) are independent, so her credence in any conjunction where one conjunct is a member of  \(\{p,  \neg p\}\) and the other is a member of \(\{q, \neg q\}\) will be the product of her credences in the conjuncts.
\item Her credence in \(p\) is 0.99, just as the evidence supports.
\item Her credence in \(q\) is also 0.99. This is unfortunate, since the rational credence in \(q\) given her evidence is 0.01.
\item She has a choice between taking and declining a bet with the following payoff structure.\footnote{I'm more interested in the abstract structure of the case than in whether any real-life situation is modelled by just this structure. But it might be worth noting the rough kind of situation where this kind of situation can arise. So let's say \(S\) has a particular bank account that is uninsured, but which currently paying 10\% interest, and she is deciding whether to deposit another \$1000 in it. Then \(p\) is the proposition that the bank will not collapse, and she'll get her money back, and \(q\) is the proposition that the interest will stay at 10\%. To make the model exact, we have to also assume that if the interest rate on her account doesn't stay at 10\%, it falls to 0.1\%. And we have to assume that the interest rate and the bank's collapse are probabilistically independent. Neither of these are at all realistic, but a realistic case would simply be more complicated, and the complications would obscure the philosophically interesting point.}
\begin{itemize*}
\item If \(p \wedge q\), she wins \$100.
\item If \(p \wedge \neg q\), she wins \$1.
\item If \(\neg p\), she loses \$1000.
\end{itemize*}
\item The marginal utility of money is close enough to constant that expected dollar returns correlate more or less precisely with expected utility returns.
\end{itemize*}

\noindent As can be easily computed, the expected utility of taking the bet given her credences is positive, it is just over \$89. Our agent \(S\) takes the bet. She doesn't compute the expected utility, but she is sensitive to it.\footnote{If she did compute the expected utility, then one of the things that would be salient for her is the expected utility of the bet. And the expected utility of the bet is different to its expected utility given \(p\). So if that expected utility is salient, she doesn't believe \(p\). And it's going to be important to what follows that she \textit{does} believe \(p\).} That is, had the expected utility given her credences been close to 0, she would have not acted until she made a computation. But from her perspective this looks like basically a free \$100, so she takes it. Happily, this all turns out well enough, since \(p\) is true. But it was a dumb thing to do. The expected utility of taking the bet given her evidence is negative, it is a little under -\$8. So she isn't warranted, given her evidence, in taking the bet.

I also claim the following three things are true of her.

\begin{enumerate*}
\item \(p\) is not justified enough to warrant her in taking the bet.
\item She believes \(p\).\footnote{In terms of the example discussed in the previous footnote, she believes that the bank will survive, i.e., that she'll get her money back if she deposits it.}
\item This belief is rational.
\end{enumerate*}

\noindent The argument for 1 is straightforward. She isn't warranted in taking the bet, so \(p\) isn't sufficiently warranted to justify it. This is despite the fact that \(p\) is obviously relevant. Indeed, given \(p\), taking the bet strictly dominates declining it. But still, \(p\) doesn't warrant taking this bet, because nothing warrants taking a bet with negative expected utility. Had the rational credence in \(p\) been higher, then the bet would have been reasonable. Had the reasonable credence in \(p\) been, say, 0.9999, then she would have been reasonable in taking the bet, and using \(p\) as a reason to do so. So there's a good sense in which \(p\) simply isn't warranted enough to justify taking the bet.\footnote{I think this is exactly the sense in which `is warranted enough' is being used in (JJ), though I'm not entirely sure about this. For present purposes, I plan to simply interpret (JJ) that way, and not return to exegetical issues.}

The argument for 2 is that she has a very high credence in \(p\), this credence is grounded in the evidence in the right way, and it leads her to act as if \(p\) is true, e.g. by taking the bet. It's true that her credence in \(p\) is not 1, and if you think credence 1 is needed for belief, then you won't like this example. But if you think that, you won't think there's much connection between (JJ) and pragmatic conditions in epistemology either. So that's hardly a position a defender of Fantl and McGrath's position can hold.\footnote{We do have to assume that \(\neg q\) is not so salient that attitudes conditional on \(\neg q\) are relevant to determining whether she believes \(p\). That's because conditional on \(\neg q\), she prefers to not take the bet, but conditional on \(\neg q \wedge p\), she prefers to take the bet. But if she is simply looking at this as a free \$100, then it's plausible that \(\neg q\) is not salient.}

The argument for 3 is that her attitude towards \(p\) tracks the evidence perfectly. She is making no mistakes with respect to \(p\). She is making a mistake with respect to \(q\), but not with respect to \(p\). So her attitude towards \(p\), i.e. belief, is rational.

I don't think the argument here strictly needs the assumption I'm about to make, but I think it's helpful to see one very clear way to support the argument of the last paragraph. The working assumption of this book is that talking about beliefs and talking about credences are simply two ways of modelling the very same things, namely minds.\footnote{Recall the discusison in section 2.1 of just this point.} If the agent both has a credence 0.99 in \(p\), and believes that \(p\), these are not two different states. Rather, there is one state of the agent, and two different ways of modelling it. So it is implausible, if not incoherent, to apply different valuations to the state depending on which modelling tools we choose to use. That is, it's implausible to say that while we're modelling the agent with credences, the state is rational, but when we change tools, and start using beliefs, the state is irrational. Given this outlook on beliefs and credences, premise 3 seems to follow immediately from the setup of the example.

\section{Objections and Replies}

So that's the argument that (JJ) is false. And if it's false, the fact that the Interest Relative Theory doesn't entail it is a feature, not a bug. But there are a number of possible objections to that position. I'll spend the rest of this chapter going over them.\footnote{Thanks here to a long blog comments thread with Jeremy Fantl and Matthew McGrath for making me formulate these points much more carefully. The original thread is at \url{http://tar.weatherson.org/2010/03/31/do-justified-beliefs-justify-action/}.}

%\objrep is my objections and replies code. It is defined in collpapers.
%\argconc is my code for making the next label C. It is defined in collpapers.

\objrep{
The following argument shows that \(S\) is not in fact justified in believing that \(p\).

\begin{enumerate*}
\item \(p\) entails that \(S\) should take the bet, and \(S\) knows this.
\item If \(p\) entails something, and \(S\) knows this, and she justifiably believes \(p\), she is in a position to justifiably believe the thing entailed.
\item \(S\) is not in a position to justifiably believe that she should take the bet.
\argconc
\item So, \(S\) does not justifiably believe that \(p\)
\end{enumerate*}}
{The problem here is that premise 1 is false. What's true is that \(p\) entails that \(S\) will be better off taking the bet than declining it. But it doesn't follow that she should take the bet. Indeed, it isn't actually true that she should take the bet, even though \(p\) is actually true. Not just is the entailment claim false, the world of the example is a counterinstance to it.

It might be controversial to use this very case to reject premise 1. But the falsity of premise 1 should be clear on independent grounds. What \(p\) entails is that \(S\) will be best off by taking the bet. But there are lots of things that will make me better off that I shouldn't do.  Imagine I'm standing by a roulette wheel, and the thing that will make me best off is betting heavily on the number than will actually come up. It doesn't follow that I should do that. Indeed, I should not do it. I shouldn't place any bets at all, since all the bets have a highly negative expected return. 

In short, all \(p\) entails is that taking the bet will have the best consequences. Only a very crude kind of consequentialism would identify what I should do with what will have the best returns, and that crude consequentialism isn't true. So \(p\) doesn't entail that \(S\) should take the bet. So premise 1 is false.}

\objrep{
Even though \(p\) doesn't \textit{entail} that \(S\) should take the bet, it does provide inductive support for her taking the bet. So if she could justifiably believe \(p\), she could justifiably (but non-deductively) infer that she should take the bet. Since she can't justifiably infer that, she isn't justified in taking the bet.
}
{The inductive inference here looks weak. One way to make the inductive inference work would be to deduce from \(p\) that taking the bet will have the best outcomes, and infer from that that the bet should be taken. But the last step doesn't even look like a reliable ampliative inference. The usual situation is that the best outcome comes from taking an \textit{ex ante} unjustifiable risk.

It may seem better to use \(p\) combined with the fact that conditional on \(p\), taking the bet has the highest \textit{expected} utility. But actually that's still not much of a reason to take the bet. Think again about cases, completely normal cases, where the action with the best outcome is an \textit{ex ante} unjustifiable risk. Call that action \(\phi\), and let \(B \phi\) be the proposition that \(\phi\) has the best outcome. Then \(B \phi\) is true, and conditional on \(B \phi\), \(\phi\) has an excellent expected return. But doing \(\phi\) is still running a dumb risk. Since these kinds of cases are normal, it seems it will very often be the case that this form of inference leads from truth to falsity. So it's not a reliable inductive inference.

More generally, we should worry quite a lot about \(S\)'s ability to draw inductive inferences about the propriety of the bet here. Unlike deductive inferences, inductive inferences can be defeated by a whole host of factors. If I've seen a lot of swans, in a lot of circumstances, and they've all been blue, that's a good reason to think the next swan I see will be blue. But it ceases to be a reason if I am told by a clearly reliable testifier that there are green swans in the river outside my apartment. And that's true even if I dismiss the testifier because I think he has a funny name, and I don't trust people with funny names. Now although \(S\) has evidence for \(p\), she also has a lot of evidence against \(q\), evidence that she is presumably ignoring since her credence in \(q\) is so high. Any story about how \(S\) can reason from \(p\) to the claim that she should have to take the bet will have to explain how her irrational attraction to \(q\) doesn't serve as a defeater, and I don't see how that could be done.}

\objrep{In the example, \(S\) isn't just in a position to justifiably believe \(p\), she is in a position to \textit{know} that she justifiably believes it. And from the fact that she justifiably believes \(p\), and the fact that if \(p\), then taking the bet has the best option, she can infer that she should take the bet.}
{It's possible at this point that we get to a dialectical impasse. I think this inference is non-deductive, because I think the example we're discussing here is one where the premises are true and the conclusion false. Presumably someone who doesn't like the example will think that it is a good deductive inference.

What makes the objection useful is that, unlike the inductive inference mentioned in the previous objection, this at least has the \textit{form} of a good inductive inference. Whenever you justifiably believe \(p\), and the best outcome given \(p\) is gained by doing \(\phi\), then \textit{usually} you should \(\phi\). Since \(S\) knows the premises are true, ceteris paribus that gives her a reason to believe the premise is probably true.

But other things aren't at all equal. In particular, this is a case where \(S\) has a highly irrational credence concerning a proposition whose probability is highly relevant to the expected utility of possible actions. Or, to put things another way, an inference from something to something else it is correlated with can be defeated by related irrational beliefs. (That's what the swan example above shows.) So if \(S\) tried to infer this way that she should take the bet, her irrational confidence in \(q\) would defeat the inference.

The objector might think I am being uncharitable here. The objection doesn't say that \(S\)'s knowledge provides an \textit{inductive} reason to take the bet. Rather, they say, it provides a \textit{conclusive} reason to take the bet. And conclusive reasons cannot be defeated by irrational beliefs elsewhere in the web. Here we reach an impasse. I knowledge that you justifiably believe \(p\) cannot provide a conclusive reason to bet on \(p\) because I think \(S\) knows she justifiably believes \(p\), but does not have a conclusive reason to bet on \(p\). That if, I think the premise the objector uses here is false because I think (JJ) is false. The person who believes in (JJ) won't be so impressed by this move. 

Having said all that, the more complicated example at the end of \cite{Weatherson2005-WEACWD} was designed to raise the same problem without the consequence that if \(p\) is true, the bet is sure to return a positive amount. In that example, conditionalising on \(p\) means the bet has a positive expected return, but still possibly a negative return. But in that case (JJ) still failed. If accepting there are cases where an agent justifiably believes \(p\), and knows this, but can't rationally bet on \(p\) is too much to accept, that more complicated example might be more persuasive. Otherwise, I concede that someone who believes (JJ) and thinks rational agents can use it in their reasoning will not think that a particular case if a counterexample to (JJ).}

\objrep{If \(S\) were ideal, then she wouldn't believe \(p\). That's because if she were ideal, she would have a lower credence in \(q\), and if that were the case, her credence in \(p\) would have to be much higher (close to 0.999) in order to count as a belief. So her belief is not justified.}
{The premise here, that if \(S\) were ideal she would not believe that \(p\), is true. The conclusion, that she is not justified in believing \(p\), does not follow. It's always a mistake to \textit{identify} what should be done with what is done in ideal circumstances. This is something that has long been known in economics. The \textit{locus classicus} of the view that this is a mistake is \cite{LipseyLancaster}. A similar point has been made in ethics in papers such as \cite{Watson1977} and \cite{KennettSmith1996b, KennettSmith1996a}. And it has been extended to epistemology by \cite{Williamson1998-WILCOK}.

All of these discussions have a common structure. It is first observed that the ideal is both \(F\) and \(G\). It is then stipulated that whatever happens, the thing being created (either a social system, an action, or a cognitive state) will not be \(F\). It is then argued that given the stipulation, the thing being created should not be \(G\). That is not just the claim that we shouldn't \textit{aim} to make the thing be \(G\). It is, rather, that in many cases being \(G\) is not the best way to be, given that \(F\)-ness will not be achieved. Lipsey and Lancaster argue that (in an admittedly idealised model) that it is actually quite unusual for \(G\) to be best given that the system being created will not be \(F\).

It's not too hard to come up with examples that fit this structure. Following Williamson, we might note that I'm justified in believing that there are no ideal cognitive agents, although were I ideal I would not believe this. Or imagine a student taking a ten question mathematics exam who has no idea how to answer the last question. She knows an ideal student would correctly answer an even number of questions, but that's no reason for her to throw out her good answer to question nine. In general, once we have stipulated one departure from the ideal, there's no reason to assign any positive status to other similarities to the idea. In particular, given that \(S\) has an irrational view towards \(q\), she won't perfectly match up with the ideal, so there's no reason it's good to agree with the ideal in other respects, such as not believing \(p\).

Stepping back a bit, there's a reason the Interest Relative Theory says that the ideal and justification come apart right here. On the Interest Relative Theory, like on any pragmatic theory of mental states, the \textit{identification} of mental states is a somewhat holistic matter. Something is a belief in virtue of its position in a much broader network. But the \textit{evaluation} of belief is (relatively) atomistic. That's why \(S\) is justified in believing \(p\), although if she were wiser she would not believe it. If she were wiser, i.e., if she had the right attitude towards \(q\), the very same credence in \(p\) would not count as a belief. Whether her state counts as a belief, that is, depends on wide-ranging features of her cognitive system. But whether the state is justified depends on more local factors, and in local respects she is doing everything right.}

\objrep{Since the ideal agent in \(S\)'s position would not believe \(p\), it follows that there is no \textit{propositional} justification for \(p\). Moreover, doxastic justification requires propositional justification\footnote{See \cite{Turri2010} for a discussion of recent views on the relationship between propositional and doxastic justification. This requirement seems to be presupposed throughout that literature.} So \(S\) is not doxastically justified in believing \(p\). That is, she isn't justified in believing \(p\).}
{I think there are two ways of understanding `propositional justification'. On one of them, the first sentence of the objection is false. On the other, the second sentence is false. Neither way does the objection go through.

The first way is to say that \(p\) is propositionally justified for an agent iff that agent's evidence justifies a credence in \(p\) that is high enough to count as a belief \textit{given the agent's other credences and preferences}. On that understanding, \(p\) is propositionally justified by \(S\)'s evidence. For all that evidence has to do to make \(p\) justified is to support a credence a little greater than 0.9. And by hypothesis, the evidence does that.

The other way is to say that \(p\) is propositionally justified for an agent iff that agent's evidence justifies a credence in \(p\) that is high enough to count as a belief \textit{given the agent's preferences and the credences supported by that evidence}. On this reading, the objection reduces to the previous objection. That is, the objection basically says that \(p\) is propositionally justified for an agent iff the ideal agent in her situation would believe it. And we've already argued that that is compatible with doxastic justification. So either the objection rests on a false premise, or it has already been taken care of.}

\objrep{If \(S\) is justified in believing \(p\), then \(S\) can use \(p\) as a premise in practical reasoning. If \(S\) can use \(p\) as a premise in practical reasoning, and \(p\) is true, and her belief in \(p\) is not Gettiered, then she knows \(p\). By hypothesis, her belief is true, and her belief is not Gettiered. So she should know \(p\). But in the previous section it was argued that she doesn't know \(p\). So by several steps of modus tollens, she isn't justified in believing \(p\).\footnote{Compare the `subtraction argument' on page 99 of \cite{FantlMcGrath2009}.}}
{Like the previous objection, this one turns on an equivocation, this time over the neologism `Gettiered'. Some epistemologists use this to simply mean that a belief is justified and true without constituting knowledge. By that standard, the third sentence is false. Or, at least, we haven't been given any reason to think that it is true. Given everything else that's said, the third sentence is a raw assertion that \(S\) knows that \(p\), and I don't think we should accept that.

The other way epistemologists sometimes use the term is to pick out justified true beliefs that fail to be knowledge for the reasons that the beliefs in the original examples from \cite{Gettier1963} fail to be knowledge. That is, it picks out a property that beliefs have when they are derived from a false lemma, or whatever similar property is held to be doing the work in the original Gettier examples. Now on this reading, \(S\)'s belief that \(p\) is not Gettiered. But it doesn't follow that it is known. There's no reason, once we've given up on the JTB theory of knowledge, to think that whatever goes wrong in Gettier's examples is the \textit{only} way for a justified true belief to fall short of knowledge. It could be that there's a practical defeater, as in this case. So the second sentence of the objection is false, and the objection again fails.

But it's worth pausing for a bit to reflect on why we should say that \(S\) does not know that \(p\). My main reason for saying this comes from thinking about how we lay out examples in decision theory. Compare the following two examples.

\begin{description*}
\item[Case 1] \(S\) has a counter and two options about what to do with it. If she places it on the table, she will get \$100,000. If she places it in her pocket, she will get nothing. There are no other salient choices, and there are no other effects associated with her action, and she prefers more money to less. What should she do?
\item[Case 2] \(S\) has a counter and two options about what to do with it. If she places it on the table, she is betting \$100,000 that the ball in the roulette wheel will land on a red number. There is a \nicefrac{18}{37} probability that this will happen, and if she bets, she'll win \$100,000 if the ball lands on a red, and lose \$100,000 if it does not. If she puts the counter in her pocket, she'll get nothing. There are no other salient choices, and there are no other effects associated with her action, and she prefers more money to less, and the marginal utility of money is constant for her over the financial range being discussed. What should she do?
\end{description*}

\noindent If we were in a standard decision theory class, the answers would be easy. She should put the counter on the table in the first case, and in her pocket in the second. 

But there's nothing in the description of the two cases that rules out their describing the very same facts. If in Case 2 the ball does actually land on a red, then everything that's true in Case 1 will be true in Case 2. And in Case 1, she should put the counter on the table. Presumably it would be a mistake to infer by modus tollens that the ball will not land on a red in Case 2, although a literal reading of the last few paragraphs would suggest that would be a good inference to draw. What is going on?

I think the obvious thing to say is that when we set up cases like Case 1 and Case 2, we don't just stipulate what's true in the story. We also stipulate that the agent stands in some epistemic relationship (broadly construed) to the facts as laid out in the story. So even if the ball in the roulette wheel in Case 2 lands on a red number, that doesn't turn the case into Case 1. That's because for us to be in Case 1, the ball would not only have to land on a red, but \(S\) would have to stand in the appropriate epistemic relation to that landing.

The big question then is what this salient epistemic relation might happen to be. I think it is knowledge. I don't have anything like a \textit{conclusive} argument for this. But I think there are a couple of reasons that push us that way. If we go with a relation weaker than knowledge, such as justified belief, then we won't be able to explain the fact that agents are meant to be able to rely on the structure of the problem, even in surprising circumstances. Agents are meant to have an indefeasible belief in the structure of the problem as stated, which is hard to do make consistent with the rationality of the agents unless you suppose that they know the problem has that structure. If you go with relation much stronger than knowledge, such as certainty, you'll be left with the problem that decision theory is not a particularly useful tool for modelling real-world interactions, since it is hard to ever be certain of anything. More positively, it's very natural when describing a decision theoretic problem to say that the agent knows various things that are in the setting out of the problem.

Now these problems don't only show up in decision theory textbooks. They also show up in, for example, epistemology papers. The problem with which we opened this section is just such a problem. Now let's say that the standard for including something in the description of the problem is knowledge. Then we could have simplified the problem a lot, as follows.

\begin{itemize*}
\item If \(S\) takes the bet, she wins \$100 if \(q\) and \$1 if \(\neg q\), and will not lose any money.
\item If \(S\) declines the bet, she gets nothing.
\end{itemize*}

\noindent And given that description of the bet, \(S\) should obviously take it, whatever her views on \(q\). Since \(S\) shouldn't take the bet, that means the bet has been misdescribed. And that means \(S\) does not know that \(p\).}