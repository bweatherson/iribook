In recent philosophical work, there have been interesting debates over these questions.

\begin{enumerate*}
\item What is the relationship between the subjects of traditional epistemology (e.g., rational belief, knowledge, etc.) and rational action?
\item What is the relationship between the subjects of traditional philosophy of mind (e.g., beliefs, desires, etc.) and the subjects of modern Bayesian epistemology such as credences, utility functions and preferences?
\end{enumerate*}

\noindent On the first question, John Hawthorne, Jason Stanley and Jeremy Fantl and Matthew McGrath have argued that a necessary condition on something being a piece knowledge is that it can ground rational action \citep{Hawthorne2004, Stanley2005-STAKAP,Hawthorne2008-HAWKAA,FantlMcGrath2009}. Hawthorne and Stanley go further, and say that only knowledge can provide a certain kind of grounding for rational action. This kind of constraint implies, given some plausible assumptions, that knowledge is interest-relative.

On the second question, various philosophers have argued that beliefs are just special kinds of credences. In particular, to believe that \(p\) is simply to have a credence in \(p\) above some threshold. This kind of view is defended by Richard \cite{Foley1993}, David \cite{Hunter1996}, David \cite{Christensen2005} and Scott \cite{Sturgeon2008}. I'm going to argue that this position isn't quite right. Just as the epistemologists mentioned in the previous paragraph argued that we should \textit{evaluate} binary states like belief using interest-relative standards, I'm going to argue that we should \textit{identify} beliefs in an interest-relative way.

It should be clear why the two questions are related. Bayesian theory suggests a strong connection between rational credences and utility functions, on the one hand, and rational action, on the other. If we think that answer is correct, then the `product' of the answers we give to questions 1 and 2 must match up with it. Now the answer the Bayesians give, that rational agents maximise expected utility, is controversial, and may not be appropriate in all circumstances. But it seems very plausible that in \textit{many} circumstances there is \textit{some} definition of utility such that rational action coincides with maximising expected utility. If we identify beliefs with certain kinds of credences, then say something about what role rational beliefs can play in the rationalisation of action, we might have ended up committing ourselves to the rationality of actions that don't maximise expected utility.

Here's an example of how that could happen. Let's say that we identify belief with credence greater than 0.95, and rational belief with rational credence greater than 0.95. And let's say we adopt the principle that if \(S\) rationally believes \(p\), and rationally believes that if \(p\) is true, then \(\phi\) will have the best outcome of any available action, then it is rationally permissible to \(\phi\). Then we'll end up saying that agents need not maximise expected utility in cases like the following.

\begin{description*}
\item[Bad Roulette:] \(S\) is facing a known to be fair roulette wheel that's about to be spun. She's holding a chip that will be worth \$100 if the ball lands on 17. Carrying the chip has some nominal costs. (It's better to not carry useless things, so anything has a small carrying cost.) Someone offers to take the chip off her hands; i.e, to carry the chip in exchange for getting the \$100 if the ball lands on 17. Let \(p\) be the proposition that the ball won't land on 17. \(S\) is rational, so her credence in it is \(\nicefrac{36}{37}\), which is greater than 0.95. By hypothesis, that means she believes \(p\). And she knows that if \(p\), she's best off taking the stranger's offer to carry the chip. So, by hypothesis, she should take the offer. But that's crazy. Or, at least, it doesn't maximise expected utility, which for present purposes we're treating as the same thing.
\end{description*}

\noindent So one of these two answers must be wrong. As it turns out, I'm going to argue that both answers are wrong.

What I hope is clear from this example is that certain combinations of answers to the two questions are ruled out. In this case neither element of the combination is clearly absurd. Indeed, the proposed answer to question 1 is taken straight from \cite{FantlMcGrath2009}, and the proposed answer to question 2 is in the spirit of the views of Foley, Hunter, Christensen and Sturgeon. But I think, and I hope this isn't an unfair reading, that none of these authors offers comprehensive answers to questions 1 and 2. If I'm right that an answer to either of these questions has to say something plausible about cases like \textbf{Bad Roulette}, then this is an important oversight. And the best way to address it is to try to answer both questions at once, as I'll do in this work.

\textbf{Bad Roulette} should be a familiar \textit{kind} of case to those who have been following the literature on contextualism and interest-relative invariantism over the past decade or so. It has a very similar structure to the Bank Cases \citep{Stanley2005-STAKAP, DeRose2009}, Airport Cases \citep{Cohen1999}, Train Cases \citep{FantlMcGrath2009} and the like that have dominated that literature. What's common to each of these cases is that there is a proposition and a choice such that (a) the agent's evidence strongly supports the proposition, and (b) if the proposition is true, the best results will come from taking one option, but (c) the agent maximises expected utility by taking some other option. For instance, in the Bank cases, the bank will very likely be open the next day, and if it is, the utility maximising option will be to return the next day.\footnote{Or at least that's how the Bank Cases are usually told. Personally, I'm not convinced it is true. I've rarely seen a bank line so long that it's worth driving back to the bank the next day. I suspect that most people, including most people reading and writing these cases, overestimate the disutility of queueing, and underestimate the disutility of driving. The Train Cases are more clearly of this form.} The upshot is that we can't identify both identify rational belief with rational high credence, and say that rational belief can directly support rational action.

But I want to stress two respects in which the role that \textbf{Bad Roulette} plays in my argument is unlike the role that those cases have played in earlier arguments for contextualism. 

First, what makes Bad Roulette distinctive is not that it is involves `High Stakes' in any particular way. \$100 is not nothing, but it isn't an amount that should trigger any super-high epistemic standards on its own. What matters is the \textit{odds} at which the agent is asked to bet on \(p\). If \(S\) accepts the offer, she gains something worth maybe a penny is \(p\) is true, and loses \$100 if \(p\) is false. In effect she's asked to bet on \(p\) at odds of 10,000:1. That's a bad bet even at low odds.

Second, I don't claim that it is obvious, either intuitively or reflectively, what to say about Bad Roulette. I'm not arguing from facts about that case, or intuitions about it. My starting point instead is that expected utility maximisation, i.e., orthodox decision theory, is at least some of the time the right theory of rational action for some natural measure of utility. We can stipulate that \(S\)'s cares, reasons and values are such that she should maximise some gently declining function of her material assets. So giving away her chip will definitely not maximise expected utility. So she shouldn't do it.

Let's go through a pair of cases, which I'll call the Map Cases, that make both of these points explicit.

\begin{description*}
\item[High Cost Map:] Brian is walking to the Mysterious Bookshop in lower Manhattan. He's pretty confident that it's on the corner of Warren Street and West Broadway. But he's been confused about this in the past, forgetting whether the east-west street is Warren or Murray, and whether the north-south street is Greenwich, West Broadway or Church. In fact he's right about the location this time, but he isn't justified in having a credence in his being correct greater than about 0.95. While he's walking there, he has two options. He could walk to where he thinks the shop is, and if it's not there walk around for a few minutes to the nearby corners to find where it is. Or he could call up directory assistance, pay \$1, and be told where the shop is. Since he's confident he knows where the shop is, and there's little cost to spending a few minutes walking around if he's wrong, he doesn't do this, and walks directly to the shop.
\item[Low Cost Map:] Just like the previous case, except that Brian has a new phone with more options. In particular, his new phone has a searchable map, so with a few clicks on the phone he can find where the store is. Using the phone has some very small costs. For example, it distracts him a little, which marginally raises the cost of bumping into another pedestrian. But the cost is very small compared to the cost of getting the location wrong. So even though he is very confident that he knows where the shop is, he double checks while walking there.
\end{description*}

\noindent I think the Map Cases are like the Bank Cases, Train Cases etc in all important respects. I think Brian knows where the shop is in High Cost Map, and doesn't know in Low Cost Map. And he doesn't know in Low Cost Map because the location of the shop has suddenly become the subject matter of a bet at very long odds. You should think of Brian's not checking the location of the shop on his phone-map as a bet on the location of the shop. If he wins the bet, he wins a few seconds of undistracted strolling. If he loses, he has to walk around a few blocks looking for a store. The disutility of the loss seems easily twenty times greater than the utility of the gain, and by hypothesis the probability of winning the bet is no greater than 0.95. So he shouldn't take the bet. Yet, I'm going to argue in subsequent chapters, if he knew where the store was, he would be justified in taking the bet. So he doesn't know where the store is.

Now this is not a case where higher \textit{stakes} defeat knowledge. If anything, the stakes are lower in Low Cost Map. But the relevant odds are longer, and that's what matters to knowledge. And I don't think the case is particularly intuitive. I'm happy to \textit{accept} a theory that says Brian knows in one case but not the other, but I don't think this is at all obvious. This shouldn't be too surprising. When it comes to reasoning about probability, we all tend to be susceptible to various errors. When we're going by snap judgments, or intuitions, we're even less reliable. When we're thinking about `tail risk', or the importance of obscure and hard to price events, we're less reliable still. In short, this is not an area where we should think native intelligence is a particularly trustworthy guide to the truth. That's not to say that we should let our theories have implications that are obviously false. Indeed, we shouldn't let our theories have implications that are non-obviously false. It's just that in this particular field we have lots of evidence that fewer things are indeed obvious than in other fields.

