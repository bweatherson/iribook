The theory sketched in the previous section seems to me right in the vast majority of cases. It fits in well with a broadly functionalist view of the mind, and as we'll see it handles some otherwise difficult cases with aplomb. But it needs to be supplemented and clarified a little to handle some difficult cases. In this section I'm going to supplement the theory a little to handle what I call `impractical propositions', and clarify a little the notion of conditional answers.

Jones has a false geographic belief: he believes that Los Angeles is west of Reno, Nevada.\footnote{I'm borrowing this example from Fred Dretske, who uses it to make some interesting points about dispositional belief.} This isn't because he's ever thought about the question. Rather, he's just disposed to say ``Of course'' if someone asks, ``Is Los Angeles west of Reno?'' That disposition has never been triggered, because no one's ever bothered to ask him this. Call the proposition that Los Angeles is west of Reno \(p\). 

The theory of the previous section will get the right result here: Jones does believe that \(p\). But it gets the right answer for an odd reason. Jones, it turns out, has very little interest in American geography right now. He's a schoolboy in St Andrews, Scotland, getting ready for school and worried about missing his schoolbus. There's no inquiry he's currently engaged in for which \(p\) is even close to relevant. So conditionalising on \(p\) doesn't change the answer to any inquiry he's engaged in, but that would be true no matter what his credence in \(p\) is.

There's an immediate problem here. Jones believes \(p\), since conditionalising on \(p\) doesn't change the answer to any relevant inquiry. But for the very same reason, conditionalising on \(\neg p\) doesn't change the answer to any relevant inquiry. It seems our theory has the bizarre result that Jones believes \(\neg p\) as well. That is both wrong and unfair. We end up attributing inconsistent beliefs to Jones simply because he's a harried schoolboy who isn't currently concerned with the finer points of geography of the American southwest.

Here's a way out of this problem. We supplement the theory of the previous section with these principles.

\begin{itemize*}
\item A proposition \(p\) is \textit{eligible} \textit{for belief} iff it satisfies the theory of the previous section. That is, \(p\) is eligible for belief iff conditionalising on \(p\) does not change the answer to any inquiry the agent is engaged in.
\item For any proposition \(p\), and any proposition \(q\) that is relevant or salient, among the actions that are (by stipulation!) open and salient with respect to \(p\) are \textit{believing that p}, \textit{believing that q}, \textit{not believing that p} and \textit{not believing that q}
\item For any proposition, the subject prefers believing it to not believing it iff (a) it is eligible for belief and (b) the agents degree of belief in the proposition is greater than \(\nicefrac{1}{2}\). 
\item The previous stipulation holds both unconditionally and conditional on \(p\), for any \(p\).
\end{itemize*}

\noindent This all looks moderately complicated, but I'll explain how it works in some detail as we go along. One simple consequence is that an agent only believes that \(p\) iff their degree of belief in \(p\) is greater than \(\nicefrac{1}{2}\). Since the schoolboy's degree of belief that Los Angeles is west of Reno is not greater than \(\nicefrac{1}{2}\), in fact it is considerably less, he doesn't believe \(\neg p\). On the other hand, since his degree of belief in \(p\) is considerably greater than \(\nicefrac{1}{2}\), he prefers to believe it than disbelieve it, so he believes it.

There are many possible objections to this position, which I'll address sequentially.

\medskip

\noindent \textit{Objection}: Even if one has a high degree of belief in \(p\), one might prefer to not believe \(p\) because one thinks that belief in \(p\) is bad for some other reason. Perhaps, if \(p\) is a proposition about one's own brilliance, it might be immodest to believe that \(p\).

\noindent \textit{Reply}: Any of these kinds of considerations should be put into the credences. If it is immodest to believe that you are a great philosopher, it is equally immodest to believe to a high degree that you are a great philosopher.

\medskip

\noindent \textit{Objection}: Belief that \(p\) is not an action in the ordinary sense of the term, so it's wrong to say that one of the \textit{actions} available to the agent is believing that \(p\).

\noindent \textit{Reply}: True, which is why this is described as a supplement to the original theory, rather than just cashing out its consequences.

\medskip

\noindent \textit{Objection}: It is impossible to choose to believe or not believe something, so we shouldn't be applying these kinds of criteria.

\noindent \textit{Reply}: I'm not as convinced of the impossibility of belief by choice as others are, for reasons set out in \cite{Weatherson2008-WEADAD} but I won't push that for present purposes. Let's grant that beliefs are always involuntary. So these `actions' aren't open actions in any interesting sense, and the theory of the previous section was really incomplete. As I said, this is a supplement to that theory.

It might be thought that if choices about beliefs are impossible, it's impossible for them to be `rational' or `irrational', so the functionalist's characteristic move of individuating states in everyone by the way those states behave in the ideal agent won't be available. But of course if some kind of state is unavoidable, the ideal agent doesn't avoid it. So we can still harness features of the ideal agent to functionalist ends. 

\medskip

\noindent \textit{Objection}: This just looks like a roundabout way of stipulating that to believe that \(p\), your degree of belief in \(p\) has to be greater than \(\nicefrac{1}{2}\). Why not just add that as an extra clause than going through these little understood detours about preferences about beliefs?

\noindent \textit{Reply}: There are three reasons for doing things this way rather than adding such a clause. 

First, it's nice to have a systematic theory rather than a theory with an ad hoc clause like that. 

Second, the effect of this constraint is much more than to restrict belief to propositions whose credence is greater than \(\nicefrac{1}{2}\). Consider a case where \(p\) and \(q\) and their conjunction are all salient, \(p\) and \(q\) are probabilistically independent, and the agent's credence in each is 0.7. Assume also that \(p, q\) and \(p \wedge q\) are completely irrelevant to any practical deliberation the agent must make. Then the criteria above imply that the agent does not believe that \(p\) or that \(q\). The reason is that the agent's credence in \(p \wedge q\) is 0.49, so she prefers to not believe \(p \wedge q\). But conditional on \(p\), her credence in \(p \wedge q\) is 0.7, so she prefers to believe it. So conditionalising on \(p\) does change her preferences with respect to believing \(p \wedge q\), so she doesn't believe \(p\). So the effect of these stipulations rules out much more than just belief in propositions whose credence is below \(\nicefrac{1}{2}\).

This suggests the third, and most important point. If we just stipulated that belief requires credence greater than \(\nicefrac{1}{2}\), we won't be able to support some plausible closure principles for rational belief. We'll return to those principles in (much) greater length when we discuss the `threshold view' of belief.

The other caveat to the theory is that it only works if we make a slightly non-standard assumption about conditional values in cases where an agent's action can harm others. I think the assumption is plausible; indeed, I think it is true. But without the assumption, the theory will have some bad results in the kinds of cases discussed by Jessica \cite{Brown200x}. \footnote{The following point is I think of some intrinsic interest, and will be crucial to what we say about some examples later on. But it does get a little deep in the decision-theoretic weeds, and some readers may prefer to skip to the next section.}

The assumption is, roughly, that for choices that may harm others, expected value is absolute value. It's easiest to see what this means using a simple case of three-way choice. The kind of example I'm considering here has been used for (slightly) different purposes by Frank \cite{Jackson1991}. 

The agent has to do \(\phi\) or \(\psi\). Failure to do either of these will lead to disaster, and is clearly unacceptable. Either \(\phi\) or \(\psi\) will avert the disaster, but one of them will be moderately harmful and the other one will not. The agent has time before the disaster to find out which of \(\phi\) and \(\psi\) is harmful and which is not for a nominal cost. Right now, her credence that \(\phi\) is the harmful one is, quite reasonably, \(\nicefrac{1}{2}\). So the agent has three choices:

\begin{itemize*}
\item Do \(\phi\);
\item Do \(\psi\); or
\item Wait and find out which one is not harmful, and do it.
\end{itemize*}

\noindent We'll assume that other choices, like letting the disaster happen, or finding out which one is harmful and doing it, are simply out of consideration. In any case, they are clearly dominated options, so the agent shouldn't do them. Let \(p\) be the propostion that \(\phi\) is the harmful one. Then if we assume the harm in question has a disutility of 10, and the disutility of waiting to act until we know which is the harmful one is 1, the values of the possible outcomes are as follows:

\begin{tabular}{r c c}
 & \(p\) & \(\neg p\) \\
Do \(\phi\) & -10 & 0 \\
Do \(\psi\) & 0 & -10 \\
Find out which is harmful & -1 & -1 \\
\end{tabular}

\noindent Given that \(Pr(p) = \nicefrac{1}{2}\), it's easy to compute that the expected value of doing either \(\phi\) or \(\psi\) is -5, while the expected value of finding out which is harmful is -1, so the agent should find out which thing is to be done before acting. So far most consequentialists would agree, and so probably would most non-consequentialists for most ways of fleshing out the abstract example I've described.\footnote{Some consequentialists say that what the agent should do depends on whether \(p\) is true. If \(p\) is true, she should do \(\psi\), and if \(p\) is false she should do \(\phi\). As we'll see, I have reasons for thinking this is rather radically wrong.}

But most consequentialists would also say something else about the example that I think is not exactly true. Just focus on the column in the table above where \(p\) is true. In that column, the highest value, 0, is alongside the action \textit{Do} \(\psi\). So you might think that conditional on \(p\), the agent should do \(psi\). That is, you might think the conditional expected value of doing \(\psi\), conditional on \(p\) being true, is 0, and that's higher than the conditional expected value of any other act, conditional on \(p\). If you thought that, you'd certainly be in agreement with the orthodox decision-theoretic treatment of this problem.

In the abstract statement of the situation above, I said that one of the options would be \textit{harmful}, but I didn't say who it would be harmful to. I think this matters. I think what I called the orthodox treatment of the situation is correct. But when the harm accrues to another person, particularly when it accrues to a person that the agent has a duty of care towards, then I think the orthodox treatment isn't quite right.

My reasons for this go back to Jackson's original discussion of the puzzle. Let the agent be a doctor, the actions \(\phi\) and \(\psi\) be her prescribing different medication to a patient, and the harm a severe allergic reaction that the patient will have to one of the medications. Assume that she can run a test that will tell her which medication the patient is allergic to, but the test will take a day. Assume that the patient will die in a month without either medication; that's the disaster that must be averted. And assume that the patient is is some discomfort that either medication would relieve; that's the small cost of finding out which medication is risk. Assume finally that there is no chance the patient will die in the day it takes to run the test, so the cost of running the test is really nominal.

A good doctor in that situation will find out which medication the patient is allergic to before ascribing either medicine. It would be reckless to ascribe a medicine that is unnecessary and that the patient might be allergic to. It is worse than reckless if the patient is \textit{actually} allergic to the medicine prescribed, and the doctor harms the patient. But even if she's lucky and prescribes the `right' medication, the recklessness remains. It was still, it seems, the wrong thing for her to do.

All of that is in Jackson's discussion of the case, though I'm not sure he'd agree with the way I'm about the incorporate these ideas into the formal decision theory. Even under the assumption that \(p\), prescribing \(\psi\) is still wrong, because it is reckless. That should be incorporated into the values we ascribe to different actions in different circumstances. The way I do it is to associate the value of each action, in each circumstance, with its actual expected value. So the decision table for the doctor's decision looks something like this.

\begin{tabular}{r c c}
 & \(p\) & \(\neg p\) \\
Do \(\phi\) & -5 & -5 \\
Do \(\psi\) & -5 & -5 \\
Find out which is harmful & -1 & -1 \\
\end{tabular}

\noindent In fact, the doctor is making a decision under certainty. She knows that the value of prescribing either medicine is -5, and the value of running the tests is -1, so she should run the tests.

In general, when an agent has a duty to maximise the expected value of some quantity \(Q\), then the value that goes into the agent's decision table in a cell is \textit{not} the value of \(Q\) in the world-action pair the agent represents. Rather, it's the expected value of \(Q\) given that world-action pair. In situations like this one where the relevant facts (e.g., which medicine the patient is allergic to) don't affect the evidence the agent has, the decision is a decision under \textit{certainty}. This is all as things should be. When you have obligations that are drawn in terms of the expected value of a variable, the actual values of that variable cease to be directly relevant to the decision problem.

The theory we end up with from all these reflections is somewhat hard to classify. On the one hand, the examples we've been looking at are ones where the values of various actions are all based on the value of various possible consequences. From that perspective the theory feels somewhat consequentialist. On the other hand, we've divorced good action from good consequences fairly sharply. We've got a situation where an agent doesn't know what action will have the best consequences, but does know what action has the highest value. I'll leave the question of whether this makes the theory consequentialist or non-consequentialist, or indeed the question of whether anything turns on this classification, to others. What's crucial to us is that we're assuming this theory is true. We'll have cause to return to it when discussing the effect of conditionalisation on cases where an agent may harm others she has various duties towards. As you might expect, the distinctive result we'll draw will be that conditionalisation has less effect than orthodox theory would suggest in these cases, since the agent was really making a decision under certainty.
