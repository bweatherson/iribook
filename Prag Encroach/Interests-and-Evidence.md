A simple version of interest-relative epistemology takes evidence, and evidential probabilities, to be interest-invariant. Indeed, it uses interest-invariant evidential probabilities to say precisely how changes in interest change what is rational to believe, and what is known. Unfortunately, the very reasons that motivate interest-relativity in epistemology generalise to arguments that evidence itself should be interest-relative. And this makes the simple version of interest-relative epistemology untenable.

The aim of this paper is to develop an interest-relative theory of evidence that solves this problem. In short, we'll look at a problem for interest-relative theories, and solve the problem by adding yet more interest-relativity to the theory.

Section one lays out the simple version of interest-relative epistemology that we'll use as a taking off point. The basic idea is that for any agent, and any proposition there is an interest-invariant body of evidence, and an interest-invariant probability for the proposition given that evidence. The agent rationally believes the proposition iff that interest-invariant evidential probability is close enough to one that the difference is practically irrelevant.

Section two lays out the problem. Whatever motivation we could have had for saying some knowledge is interest-relative also extends to the class of propositions that could go into the agent's evidence. So we should have an interest-relative theory of evidence. But this undermines the story from section one about how interests are relevant, since that story presupposes the availability of a body of evidence to use as an input to the theory of evidence.

Section three develops the solution. The key point is that for a process to generate evidence for an agent, that process must be reliable. This notion of reliability is vague in several respects. There are independent reasons to think that the right resolution of vagueness will generally be interest-relative [#Fara2000]. Here we use some techniques from game theory to develop an interest-relative account of evidence. That will provide us a body of evidence that can be merged with the simple theory from section one to get a more complicated, but more plausible, interest-relative epistemology.

# Simple Interest-Relative Epistemology

I'm going to set out a motivation for a relatively simple interest-relative epistemology in this section. The motivation tracks fairly closely the motivation offered by [#FantlMcGrath2002, FantlMcGrath2009;] and by [#Weatherson2005;], and a little less closely the motivations in [#Hawthorne2004;] and in [#Stanley2005;], so the broad outlines at least will be fairly familiar. And there are by now many responses to the motivation set out in those works. I'm not going to attempt reply to all those responses, though the way I fill in some of the details will ultimately make it easier to reply to at least some of them. But the purpose here is not to convince the unconvinced of the virtues of interest-relative epistemology. It is largely here to remind the reader of the arguments for interest-relativity, before setting up a puzzle (in section two) and offering a solution to that puzzle (in section three).

Interest-relative epistemology emerges as an attempt to deal with conflicting pressures within epistemology. On the one hand, we want to say that agents know things that are not as certain as the most obvious truths. On the other hand, we want knowledge to play a particular role in decision making, and it seems knowledge can only play that role if all knowledge is equally certain.  The first of these points requires little defence. I'm more certain that two plus two is four than that the Magna Carta was signed in 1215, but I know both these facts. To see the second point, consider someone who we would ordinarily describe as knowing when the Magna Carta was signed, who faces a choice between two envelopes.

* Envelope A contains $50 if two plus two equals four, nothing otherwise.
* Envelope B contains $50 if the Magna Carta was signed in 1215, nothing otherwise.

Assume that the agent knows both these facts about the envelopes, and nothing else. It seems, to me at least, that rationality requires they take envelope A. Even if they have a fair bit of evidence about Magna Carta, it is more likely that they have made a mistake about its date than that they have made a mistake about two plus two. So it would be a needless and pointless risk to take the less certain envelope.

But this is hard to square with the idea that they know the Magna Carta was signed in 1215. After all, if they know that is true, then very simple reasoning lets them know that Envelope B contains $50. And similarly simple reasoning lets them know that Envelope A contains at most $50. So they know that Envelope B is at least as good as Envelope A, so it is not irrational to take Envelope B.

It might be objected here that just because one knows something is worth doing, that may not be enough reason to do it. Perhaps, in certain cases, one needs to know that one knows, or even to be certain.[^Add citations] But the cases that motivate the thought that one needs more than knowledge typically only motivate the thought that knowledge is insufficient for action in high stakes cases. And the stakes here are not particularly high. Moreover, if one needs more than knowledge to act in this case, it isn't clear why the agent should be rationally compelled to take Envelope A. After all, we only specified that the agent knows that it contains $50 if two plus two equals four. We didn't say that she knows that she knows it, or that she is certain of it. So insisting on something stronger than knowledge being needed for action is no way to rescue the intuition that in this case, it is irrational to take Envelope B.

So we have a challenge. If we say that the agent knows when the Magna Carta was signed, we lack a good explanation for the irrationality of taking Envelope B. If we say that she does not, then it seems scepticism looms, for we can repeat the same argument for any proposition that is less certain than that two and two makes four. And there is very little we know with as great a certainty as we know that. 

Interest-relativity emerges as a way to slide between these poles. The agent as described doesn't know when the Magna Carta was signed. But this isn't because scepticism is generally true. Rather, it is because she is in a distinctive decision-making context. Agents in that context lose knowledge. But most agents are not in situations like that, and so most agents know a lot. Indeed, even this agent knows many other things about the world, and even about English history. They just lose some knowledge about the thirteenth century.[^Just how much knowledge they lose is a tricky question. I think the best response to some cases developed by Ram Neta is that they lose a little more than we might have first expected. (Include Neta cite, and see Author Paper section X for more details.)]

On the simple version of interest-relativity that I want to start with, we can say a bit more about why the agent loses knowledge. Say that the agent has some evidence ​_E_​, and that given ​_E_​, the probability that two plus two is four is 1, while the probability that the Magna Carta was signed in 1215 is 0.999. Moreover, say that it is a necessary condition of knowing that ​_p_​ that one has the same preferences over salient choices conditional on ​_p_​ as one does absolutely. Then the agent will strictly prefer envelope A to envelope B, since A's expected return is $50, while B's expected return is $49.95. But conditional on the Magna Carta being signed in 1215, both A and B have an expected return of $50. So the agent is indifferent between them. So the agent does not have the same preferences unconditionally as they have conditional on the Magna Carta being signed in 1215. So they don't know that the Magna Carta was signed in 1215. And they don't know this because its evidential probability is not sufficiently close to 1, given the practical situation they find themselves in

But this is obviously a very distinctive kind of practical situation. Typically, the difference between probability 0.999 and probability 1 is irrelevant. So typically, assuming that the agent's belief about the Magna Carta is true, safe, justified and so on, the agent will know when the Magna Carta was signed. It is only in the special case when it really matters whether this probability is 1 or fractionally short of 1 that the agent fails to know this.

The explanatory story I've just sketched presupposes that the evidential probability of a proposition is explanatorily prior to whether or not the agent knows it. A version of interest-relativity which does not take evidence, and evidential probability, to be prior to knowledge, cannot offer it. For example, it would be very hard to square the explanatory story I just offered with the version of interest-relativity that Jason [#Stanley2005;] defends. Stanley, following [#Williamson2000;], says that evidence is knowledge. I think the intent here is to say that knowledge is explanatorily prior to evidence; something is part of the agent's evidence because the agent knows it. Maybe that's reading too much into what Stanley and Williamson say, but they certainly mean to deny that evidential probability is explanatorily prior to knowledge. At the very least, that is incompatible with the 'knowledge first' research program.

Such a theory needs some explanation for why being offered the choice between the two envelopes causes the agent to lose a piece of knowledge about English history. As it stands, Stanley's theory doesn't have any such explanation, for he focusses on the way in which high stakes can cause loss of knowledge, and this isn't a case of high stakes. But that doesn't mean his theory is wrong, just that it is in this respect incomplete. (And all theories are incomplete in some respect.) 

Still, the fact that the theory which takes evidential probabilities to be explanatorily prior has an explanation for why the agent loses knowledge in this choice situation is a nice advantage of that theory, especially if rivals lack such an explanation. Or, at least, it would be a nice explanation if the explanation worked. And it may not. At the very least, it does not generalise to cases that seem like they should receive the same kind of explanation.

# Interests and Evidence

The main example in this section will involve F01. She is at an restaurant when she looks around and sees her old school friend F02 a couple of tables away. That is, she sees someone who in fact is F02, and  she immediately believes that person is F02, and this belief is justified by the distinctive physical appearance of F02, and is safe, and sensitive, and produced by a reliable mechanism for identifying long-time acquaintances. Ordinarily, we'd say that F01 knows that F02 is nearby.

Indeed, we may go further and say that this proposition is part of F01's evidence. It is a proposition she can use to justify other beliefs, such as the belief that F02 is still alive, that F02 is in town, and so on. We can imagine that if there were any calculations that led to the belief that it is F02 sitting there, these all took place at a deeply sub-personal level. In a realistic case, the first belief F01 will notice having is that F02 is nearby. She won't notice that the person two tables over has those distinctive shaped ears that F02 has, and infer that it must be F02. In a more realistic situation, what will be phenomenologically prior will be the observation that it is F02, and if someone asks how she knows it is F02, she will have to actively reconstruct why she believes the person to be F02.

We'll come back to this point in a little, but for now just notice the initial plausibility of the point that she simply gets a new piece of evidence from perceiving the world: F02 is nearby. If this is part of her evidence, then she knows it. It is a general rule that if an agent competently deduces _p_ from her evidence, she knows _p_. And she can certainly deduce _p_ from an evidence set that has _p_ as an element. So saying this is part of F01's evidence commits us to saying it is part of her knowledge.

You can probably guess where this is going. Imagine now that F01 is offered a choice of two envelopes, and this is what she knows about the envelopes.

* Envelope A contains $50 if two plus two is four, and nothing otherwise.
* Envelope B contains $50 if F02 is nearby, and nothing otherwise.

Now from one perspective, it doesn't matter which envelope she takes. Two plus two is four, and F02 is nearby, and she'll get $50 either way. But that just means that if F01 takes Envelope B, she won't suffer for it. That is consistent with it being a dumb risk to take Envelope B. And in fact, it is a dumb risk to take Envelope B.

Even though F01 has correctly identified F02, there are all sorts of ways in which she could have gone wrong. She could have misremembered which of her old school friends look just like _that_. It could have been someone else who looks a lot like F02. It wasn't; in fact we've stipulated that it wasn't even a close call that either of these possibilities obtain. But F01 has no reason to take this risk, when she can just take the $50 that she knows is in Envelope A.

So for just the reasons described in section 1, F01 doesn't know that F02 is nearby. At least, she doesn't know this in the context of choosing between Envelope A and Envelope B. So far this doesn't look problematic for the interest-relative theorist. We want to say that when she first looks over, she knows F02 is nearby, and this knowledge does not survive the transition to thinking about whether to take Envelope A or Envelope B. And the interest-relative theorist can say all those things. 

The problem is that we'd like to say a bit more about why being faced with just this choice destroys F01's knowledge of who is sitting near her. The simple explanations we used in section 1 won't work here. We can't say that F01 lacks knowledge that F02 is nearby because the evidential probability of that is too low. Or, at least, if we do say that, we owe a further explanation of why the evidential probability is too low. And that explanation must include why it is not part of F01's evidence that F02 is nearby. And that is what we were originally setting out to explain. It feels like we've gone in a circle.

In the next section I'm going to offer a way out of that circle. The core idea is that only reliable input sources provide evidence. It is hard to operationalise that idea. We want something like a binary distinction - something either is or is not part of evidence - and we have a continuously variable tool - a method is more or less reliable - to produce it. Any boundary we draw is going to seem arbitrary. The solution, I'll suggest, is a distinctive kind of interest-relativity.

But first I'll argue that there really is a problem here, and it can't be fixed by tinkering with the notion of evidence. 

Here is the view I will argue against for the rest of the section. Evidence really is interest-invariant. But evidence does not consist of propositions like _F02 is nearby_. Rather, it consists of propositions that the agent could not be wrong about. And the agent's knowledge in those propositions cannot be undermined by the practical situation changing.

The challenge for this position is to say what exactly the evidence is. It can't be anything external; the agent could be wrong about any such thing. And there is a strong argument against the idea that evidence could be entirely 'psychologised'. This argument is partially taken from work by Eric [#Schwitzgebel200x;], and partially from work by Timothy [#Williamson2007;].

It's worth taking a moment to see how hard it would be to come up with infallible evidence that could do the work we need it to do. We can't say that the person near F01 looks like F02. After all, F01 might be confusing her memory of F02 with some other old friend, so she doesn't infallibly know that this person looks like F02. We can't say merely things like that they have such and such shaped ears, eyes, hair etc. Those things are not going to be strong enough to make it at all probable that 