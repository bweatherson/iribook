\documentclass[oneside]{book}

\usepackage{../OldPapers/collpapers}
\usepackage{../OldPapers/singpaper}
%\usepackage{gb4e}
%\usepackage{ChapHeadStyle}

\renewcommand{\numbex}[2]{
\begin{enumerate*}
\setcounter{enumi}{\value{paper}}
\renewcommand{\labelenumi}{(\arabic{enumi})}
#2
\end{enumerate*}
\addtocounter{paper}{#1}}

\title{Defending Interest-Relative Invariantism}
\author{Brian Weatherson}

\begin{document}

%\begin{titlepage}
%\maketitle
%\end{titlepage}

%\setcounter{tocdepth}{1}

%\pagenumbering{roman} \tableofcontents \newpage \pagenumbering{arabic}

\newcounter{paper}
\setcounter{paper}{0}

\chapter[Defending IRI]{Defending Interest-Relative Invariantism auth=Blinded for review}

In recent years a number of authors have defended the interest-relativity of various epistemological claims, such as claims of the form \textit{S knows that p}, or \textit{S has a justified belief that p}. Views of this form are floated by John Hawthorne \citeyearpar{Hawthorne2004}, and endorsed by Jeremy Fantl and Matthew McGrath \citeyearpar{Fantl2002, FantlMcGrath2009}, Jason Stanley \citeyearpar{Stanley2005-STAKAP} and Brian Weatherson \citeyearpar{Weatherson2005-WEACWD}. The various authors differ quite a lot in how much interest-relativity they allow, and even more in their purported explanations for the interest-relativity, but what is common is the endorsement of some kind of interest-relativity in statements like  \textit{S knows that p}, or \textit{S has a justified belief that p}.

These views have, quite naturally, drawn a range of criticisms. The primary purpose of this paper is to respond to these criticisms and, as it says on the tin, defend interest-relative invariantism, or IRI for short.\footnote{`Interest-relative invariantism' is Jason Stanley's term for the view that `knows' is not context-sensitive, but whether $S$ knows that $p$ might depend on $S$'s interests. Some critics, such as Michael Blome-Tillmann \citeyearpar{MBT2009}, call IRI `subject-sensitive invariantism'. This is an unfortunate moniker. The only subject-\textit{insensitive} theory of knowledge has that for any \(S\) and \(T\), \(S\) knows that \(p\) iff \(T\) knows that \(p\). The view the critics target certainly isn't defined in opposition to \textit{this} generalisation.} But to defend the view, I first need to clarify three features of the view. The best version of IRI, and the only one I'm interested in defending, has these three features:

\begin{itemize*}
\item Odds, not stakes, are primarily what matter to knowledge.
\item Interests create defeaters.
\item Interest-relativity is an existential claim; it says that interests sometimes matter, not that they always do.
\end{itemize*}

\noindent In the first three sections, I'll say more about each of these three points. In the next six sections, I'll defend IRI against criticisms from five different authors.

\section{Odds and Stakes}

It is common to describe IRI as a theory where in `high stakes' situations, more evidence is needed for knowledge than in `low stakes' situations. But this is at best misleading. What really matters are the odds on any bet-like decision the agent faces with respect to the target proposition. More precisely, interests affect belief because  whether someone believes \(p\) depends \textit{inter alia} on whether their credence in \(p\) is high enough that any bet on \(p\) they actually face is a good bet. Raising the stakes of any bet on \(p\) does not directly change that, but changing the odds of the bets on \(p\) they face does change it. Now in practice due to the declining marginal utility of material goods, high stakes situations will usually be situations where an agent faces long odds. But it is the odds that matter to knowledge, not the stakes.

Some confusion on this point may have been caused by the Bank Cases that Stanley uses, and the Train Cases that Fantl and McGrath use, to motivate IRI. In those cases, the authors lengthen the odds the relevant agents face by increasing the potential losses the agent faces by getting the bet wrong. But we can make the same point by decreasing the amount the agent stands to gain by taking the bet. Let's go through a pair of cases, which I'll call the Map Cases, that illustrate this.

\begin{description*}
\item[High Cost Map:] Zeno is walking to the Mysterious Bookshop in lower Manhattan. He's pretty confident that it's on the corner of Warren Street and West Broadway. But he's been confused about this in the past, forgetting whether the east-west street is Warren or Murray, and whether the north-south street is Greenwich, West Broadway or Church. In fact he's right about the location this time, but he isn't justified in having a credence in his being correct greater than about 0.95. While he's walking there, he has two options. He could walk to where he thinks the shop is, and if it's not there walk around for a few minutes to the nearby corners to find where it is. Or he could call up directory assistance, pay \$1, and be told where the shop is. Since he's confident he knows where the shop is, and there's little cost to spending a few minutes walking around if he's wrong, he doesn't do this, and walks directly to the shop.
\item[Low Cost Map:] Just like the previous case, except that Zeno has a new phone with more options. In particular, his new phone has a searchable map, so with a few clicks on the phone he can find where the store is. Using the phone has some very small costs. For example, it distracts him a little, which marginally raises the likelihood of bumping into another pedestrian. But the cost is very small compared to the cost of getting the location wrong. So even though he is very confident about where the shop is, he double checks while walking there.
\end{description*}

\noindent I think the Map Cases are like the Bank Cases, Train Cases etc., in all important respects. I think Zeno knows where the shop is in High Cost Map, and doesn't know in Low Cost Map. And he doesn't know in Low Cost Map because the location of the shop has suddenly become the subject matter of a bet at very long odds. You should think of Zeno's not checking the location of the shop on his phone-map as a bet on the location of the shop. If he wins the bet, he wins a few seconds of undistracted strolling. If he loses, he has to walk around a few blocks looking for a store. The disutility of the loss seems easily twenty times greater than the utility of the gain, and by hypothesis the probability of winning the bet is no greater than 0.95. So he shouldn't take the bet. Yet  if he knew where the store was, he would be justified in taking the bet. So he doesn't know where the store is. Now this is not a case where higher \textit{stakes} defeat knowledge. If anything, the stakes are lower in Low Cost Map. But the relevant odds are longer, and that's what matters to knowledge.\footnote{Note that I'm not claiming that it is intuitive that Zeno has knowledge in High Cost Map, but not that Low Cost Map. Nor am I claiming that we should believe IRI because it gets the Map Cases right. In fact, I don't believe either of those things. Instead I believe Zeno has knowledge in High Cost Map and not in Low Cost Map because I believe IRI is correct, and that's what IRI says about the case. It is sometimes assumed, e.g, in the experimental papers I'll discuss in section \ref{sect:xphi}, that pairs of cases like these are meant to \textit{motivate}, and not just \textit{illustrate}, IRI. I can't speak for everyone's motivations, but I'm only using these cases as illustrations, not motivations.}

\section{Interests are Defeaters} \label{sect:defeat}

Interests can have a somewhat roundabout effect on knowledge. The IRI story goes something like this. If the agent has good but not completely compelling evidence for $p$, that is sometimes but not always sufficient for knowledge that $p$ if everything else goes right. It isn't sufficient if they face a choice where the right thing to do is different to the right thing to do conditional on $p$. That is, if adding $p$ to their background information would make a genuinely bad choice look like a good choice, they don't know that $p$. The reason is that if they did know $p$, they'd be in one of two unhappy states. The first such state is that they'd believe the choice is bad despite believing that conditional on something they believe, namely $p$, the choice is good. That's incoherent, and the incoherence defeats knowledge that $p$. The second such state is that they'd believe the choice is good. That's also irrational, and the irrationality defeats knowledge that $p$.\footnote{Note again that I'm not here purporting to argue for IRI, just set out its features. Obviously not everyone will agree that knowledge that $p$ can be defeated by irrationality or incoherence in closely related attitudes. But I think it is at least \textit{plausible} that there are coherence constraints like this on knowledge, which is all I need for a defence of IRI against critics.} 

Cases where knowledge is defeated because if the agent did know $p$, that would lead to problems elsewhere in their cognitive system, have a few quirky features. In particular, whether the agent knows $p$ can depend on very distant features. Consider the following kind of case.

\begin{quote}

\textbf{Confused Student}

Con is systematically disposed to affirm the consequent. That is, if he notices that he believes both $p$ and $q \rightarrow p$, he's disposed to either infer $q$, or if that's impermissible given his evidence, to ditch his belief in the conjunction of $p$ and $q \rightarrow p$. Con has completely compelling evidence for both $q \rightarrow p$ and $\neg q$. He has good but less compelling evidence for $p$. And this evidence tracks the truth of $p$ in just the right way for knowledge. On the basis of this evidence, Con believes $p$. Con has not noticed that he believes both $p$ and $q \rightarrow p$. If he did, he's unhesitatingly drop his belief that $p$, since he'd realise the alternatives (given his dispositions) involved dropping belief in a compelling proposition. Two questions:
\begin{itemize*}
\item Does Con know that $p$?
\item If Con were to think about the logic of conditionals, and reason himself out of the disposition to affirm the consequent, would he know that $p$?
\end{itemize*}
\end{quote}

\noindent I think the answer to the first question is \textit{No}, and the answer to the second question is \textit{Yes}. As it stands, Con's disposition to affirm the consequent is a doxastic defeater of his putative knowledge that $p$. Put another way, $p$ doesn't cohere well enough with the rest of Con's views for his belief that $p$ to count as knowledge. To be sure, $p$ coheres well enough with those beliefs by objective standards, but it doesn't cohere at all by Con's lights. Until he changes those lights, it doesn't cohere well enough to be knowledge.

I don't expect exactly everyone will agree with those judgments. Some people will simply reject that this kind of coherence by one's own lights is necessary for knowledge. Others might even reject the whole idea of doxastic defeaters. But I think the picture I've just sketched, one which puts reasonably tight coherence constraints on knowledge, is plausible enough to use in a defence of IRI. It certainly isn't so implausible that committing to it amounts to a \textit{reductio} of one's views. Yet as we'll frequently see below, some criticisms of IRI do suggest that any theory that allows for these kinds of coherence constraints or doxastic defeaters is thereby shown to be false.\footnote{This kind of criticism is most pronounced in the arguments I'll respond to in section \ref{sect:conj}, but it is somewhat pervasive.} I'm going to take that suggestion to be a \textit{reductio} of the criticisms.

\section{IRI is an Existential Theory} \label{sect:existential}

IRI theorists do not typically say that interests are \textit{always} relevant to knowledge. In fact, they hardly could be. If $p$ is not true, or the agent has very little evidence for $p$, the agent does not know $p$ whatever their interests. But an assumption that seems to shared by both some critics and some proponents of IRI is that IRI rests on some universal epistemic principles, not just on various existential principles. For instance, Jeremy Fantl and Matthew McGrath use a lot of principles like (JJ) in deriving IRI.

\begin{description}
\item[(JJ)] If you are justified in believing that \(p\), then \(p\) is warranted enough to justify you in \(\varphi\)-ing, for any \(\varphi\). \cite[99]{FantlMcGrath2009}
\end{description}

\noindent Now we it turns out, I think (JJ) is false. (I think it fails in cases where the agent is seriously mistaken about the risks and payoffs involved in doing $\varphi$, for instance.) But more importantly, we don't need anything nearly as strong as this to derive IRI. As long as there is \textit{some} sufficiently large range of cases where (JJ) holds, we'll be able to establish the existence of \textit{some} pair of cases which differ in whether the agent knows that $p$ in virtue of the interests the agent has.

Relatedly, the argument for a version of IRI in \cite{Weatherson2005-WEACWD} makes frequent appeal to standard Bayesian decision theory. This might suggest that such an argument stands and falls with the success of consequentialism in decision theory. (I mean to use `consequentialism' here roughly in the sense that it is used in \cite{Hammond1988}.) But again, this suggestion would be false. If consequentialism is true in some range of cases, we'll be able to use similar techniques to the ones used in that paper to show that there are the kinds of pairs of cases that IRI say exist.

\section{Experimental Objections} \label{sect:xphi}
As I mentioned in the discussion of the Map Cases, I don't think the argument for IRI rests on judgments, or intuitions, about similar cases. Rather, IRI can be independently motivated by, for instance, reflections on the relationship between belief and credence. It's a happy result, in my view, that IRI gets various Bank Cases and Map Cases right, but not essential to the view. If it turned out that the facts about the examples were less clear than we thought, that wouldn't \textit{undermine} the argument for IRI, since those facts weren't part of the best arguments for IRI. But if it turned out that the facts about those examples were quite different to what IRI predicts, that may \textit{rebut} the view, since it would then be shown to make false predictions.

This kind of rebuttal may be suggested by various recent experimental results, such as the results in \cite{May2010} and \cite{FeltzZarpentine2010}. I'm going to concentrate on the latter set of results here, though I think that what I say will generalise to related experimental work.\footnote{Note to editors: Because this work is not yet in press, I don't have page numbers for any of the quotes from Feltz and Zarpentine.} In fact, I think the experiments don't really tell against IRI, because IRI doesn't make \textit{any} unambiguous predictions about the cases at the centre of the experiments. The reason for this is related to the first point made in section one: it is odds, not stakes, that are most important.

Feltz and Zarpentine gave subjects related vignettes, such as the following pair. (Each subject only received one of the pair.)

\begin{description}
\item[High Stakes Bridge] John is driving a truck along a dirt road in a caravan of trucks. He comes across what looks like a rickety wooden bridge over a yawning thousand foot drop. He radios ahead to find out whether other trucks have made it safely over. He is told that all 15 trucks in the caravan made it over without a problem. John reasons that if they made it over, he will make it over as well. So, he thinks to himself, `I know that my truck will make it across the bridge.'

\item[Low Stakes Bridge] John is driving a truck along a dirt road in a caravan of trucks. He comes across what looks like a rickety wooden bridge over a three foot ditch. He radios ahead to find out whether other trucks have made it safely over. He is told that all 15 trucks in the caravan made it over without a problem. John reasons that if they made it over, he will make it over as well. So, he thinks to himself, `I know that my truck will make it across the bridge.' \citep[??]{FeltzZarpentine2010}
\end{description}

\noindent Subjects were asked to evaluate John's thought. And the result was that 27\% of the participants said that John does not know that the truck will make it across in \textbf{Low Stakes Bridge}, while 36\% said he did not know this in \textbf{High Stakes Bridge}. Feltz and Zarpentine say that these results should be bad for interest-relativity views. But it is hard to see just why this is so.

Note that the change in the judgments between the cases goes in the direction that IRI seems to predict. The change isn't trivial, even if due to the smallish sample size it isn't statistically significant in this sample. But should a view like IRI have predicted a larger change? To figure this out, we need to ask three questions.

\begin{enumerate*}
\item What are the costs of the bridge collapsing in the two cases?
\item What are the costs of not taking the bet, i.e., not driving across the bridge?
\item What is the rational credence to have in the bridge's sturdiness given the evidence John has?
\end{enumerate*}

IRI predicts that there is knowledge in Low Stakes Bridge but not in High Stakes Bridge only if the following equation is true:

\begin{equation*}
\frac{C_H}{G + C_H} > x > \frac{C_L}{G + C_L}
\end{equation*}

\noindent where $G$ is the gain the driver gets from taking a non-collapsing bridge rather than driving around (or whatever the alternative is), $C_H$ is the cost of being on a collapsing bridge in High Stakes Bridge, $C_L$ is the cost of being on a collapsing bridge in Low Stakes Bridge, and $x$ is the probability that the bridge will collapse. I assume $x$ is constant between the two cases. If that equation holds, then taking the bridge, i.e., acting as if the bridge is safe, maximises expected utility in Low Stakes Bridge but not High Stakes Bridge. So in High Stakes Bridge, adding the proposition that the bridge won't collapse to the agent's cognitive system produces incoherence, since the agent won't (at least rationally) act as if the bridge won't collapse. So if the equation holds, the agent's interests in avoiding $C_H$ creates a doxastic defeater in High Stakes Bridge.

But does the equation hold? Or, more relevantly, did the subjects of the experiment believe that the equation hold? None of the four variables has their values clearly entailed by the story, so we have to guess a little as to what the subjects' views would be. 

Feltz and Zarpentine say that the costs in ``High Stakes Bridge [are] very costly---certain death---whereas the costs in Low Stakes Bridge are likely some minor injuries and embarrassment.'' \cite[??]{FeltzZarpentine2010} I suspect both of those claims are wrong, or at least not universally believed. A lot more people survive bridge collapses than you may expect, even collapses from a great height.\footnote{In the West Gate bridge collapse in Melbourne in 1971, a large number of the victims were underneath the bridge; the people on top of the bridge had a non-trivial chance of survival. That bridge was 200 feet above the water, not 1000, but I'm not sure the extra height would matter greatly. Again from a slightly lower height, over 90\% of people on the bridge survived the I-35W collapse in Minneapolis in 2007.} And once the road below a truck collapses, all sorts of things can go wrong, even if the next bit of ground is only 3 feet away. (For instance, if the bridge collapses unevenly, the truck could roll, and the driver would probably suffer more than minor injuries.)

We aren't given any information as to the costs of not crossing the bridge. But given that 15 other trucks, with less evidence than John, have decided to cross the bridge, it seems plausible to think they are substantial. If there was an easy way to avoid the bridge, presumably the \textit{first} truck would have taken it. If $G$ is large enough, and $C_H$ small enough, then the only way for this equation to hold will be for $x$ to be low enough that we'd have independent reason to say that the driver doesn't know the bridge will hold.

But what is the value of $x$? John has a lot of information that the bridge will support his truck. If I've tested something for sturdiness two or three times, and it has worked, I won't even think about testing it again. Consider what evidence you need before you'll happily stand on a particular chair to reach something in the kitchen, or put a heavy television on a stand. Supporting a weight is the kind of thing that either fails the first time, or works fairly reliably. Obviously there could be some strain-induced effects that cause a subsequent failure\footnote{As I believe was the case in the I-35W collapse.}, but John really has a lot of evidence that the bridge will support him.

Given those three answers, it seems to me that it is a reasonable bet to cross the bridge. At the very least, it's no more of an unreasonable bet than the bet I make every day crossing a busy highway by foot. So I'm not surprised that 64\% of the subjects agreed that John knew the bridge would hold him. At the very least, that result is perfectly consistent with IRI, if we make plausible assumptions about how the subjects would answer the three numbered questions above.

And as I've stressed, these experiments are only a problem for IRI if the subjects are reliable. I can think of two reasons why they might not be. First, subjects tend to massively discount the costs and likelihoods of traffic related injuries. In most of the country, the risk of death or serious injury through motor vehicle accident is much higher than the risk of death or serious injury through some kind of crime or other attack, yet most people do much less to prevent vehicles harming them than they do to prevent criminals or other attackers harming them.\footnote{See the massive drop in the numbers of students walking or biking to school, reported in \cite{Ham2008}, for a sense of how big an issue this is.} Second, only 73\% of this subjects in \textit{this very experiment} said that John knows the bridge will support him in \textbf{Low Stakes Bridge}. This is just absurd. Unless the subjects endorse an implausible kind of scepticism, something has gone wrong with the experimental design. Given the fact that the experiment points broadly in the direction of IRI, and that with some plausible assumptions it is perfectly consistent with that theory, and that the  subjects seem unreasonably sceptical to the point of unreliability about epistemology, I don't think this kind of experimental work threatens IRI.

\section{Knowledge By Indifference and By Wealth}

Gillian Russell and John Doris \citeyearpar{RussellDoris2008} argue that Jason Stanley's account of knowledge leads to some implausible attributions of knowledge, and if successful their objections would generalise to other forms of IRI. I'm going to argue that Russell and Doris's objections turn on principles that are \textit{prima facie} rather plausible, but which ultimately we can reject for independent reasons.\footnote{I think the objections I make here are similar in spirit to those Stanley made in a comments thread on \href{http://el-prod.baylor.edu/certain_doubts/?p=616}{Certain Doubts}, though the details are new. The thread is at \href{http://el-prod.baylor.edu/certain_doubts/?p=616}{http://el-prod.baylor.edu/certain\_doubts/?p=616}}

Their objection relies on variants of the kind of case Stanley uses heavily in his \citeyearpar{Stanley2005-STAKAP} to motivate a pragmatic constraint on knowledge. Stanley imagines a character who has evidence which would normally suffice for knowledge that \(p\), but is faced with a decision where \(A\) is both the right thing to do if \(p\) is true, and will lead to a monumental material loss if \(p\) is false. Stanley intuits, and argues, that this is enough that they cease to know that \(p\). I agree, at least as long as the gains from doing \(A\) are low enough that doing \(A\) amounts to a bet on \(p\) at insufficiently favourable odds to be reasonable in the agent's circumstance.

Russell and Doris imagine two kinds of variants on Stanley's case. In one variant the agent doesn't care about the material loss. As I'd put it, the agent's indifference to material odds shortens the odds of the bet. That's because costs and benefits of bets should be measured in something like utils, not something like dollars. Given that, Russell and Doris object that ``you should have reservations ... about what makes [the knowledge claim] true: not giving a damn, however enviable in other respects, should not be knowledge-making.'' \citep[432]{RussellDoris2008}. Their other variant involves an agent with so much money that the material loss is trifling to them. Again, this lowers the effective odds of the bet, so by my lights they may still know that \(p\). But this is somewhat counter-intuitive. As Russell and Doris say, ``[m]atters are now even dodgier for practical interest accounts, because \textit{money} turns out to be knowledge making.'' \citep[433]{RussellDoris2008} And this isn't just because wealth can purchase knowledge. As they say, ``money may buy the \textit{instruments} of knowledge ... but here the connection between money and knowledge seems rather too direct.'' \citep[433]{RussellDoris2008}

The first thing to note about this case is that indifference and wealth aren't really producing knowledge. What they are doing is more like defeating a defeater. Remember that the agent in question had enough evidence, and enough confidence, that they would know \(p\) were it not for the practical circumstances. As I argued in section \ref{sect:defeat}, practical considerations enter debates about knowledge in part because they are distinctive kinds of defeaters. It seems that's what is going on here. And we have, somewhat surprisingly, independent evidence to think that indifference and wealth do matter to defeaters.

Consider two variants on Gilbert Harman's `dead dictator' example \citep[75]{Harman1973}. In the original example, an agent reads that the dictator has died through an actually reliable source. But there are many other news sources around, such that if the agent read them, she would lose her belief. Even if the agent doesn't read those sources, their presence can constitute defeaters to her putative knowledge that the dictator died.

In our first variant on Harman's example, the agent simply does not care about politics. It's true that there are many other news sources around that are ready to mislead her about the dictator's demise. But she has no interest in looking them up, nor is she at all likely to look them up. She mostly cares about sports, and will spend most of her day reading about baseball. In this case, the misleading news sources are too distant, in a sense, to be defeaters. So she still knows the dictator has died. Her indifference towards politics doesn't generate knowledge - the original reliable report is the knowledge generator - but her indifference means that a would-be defeater doesn't gain traction.

In the second variant, the agent cares deeply about politics, and has masses of wealth at hand to ensure that she knows a lot about it. Were she to read the misleading reports that the dictator has survived, then she would simply use some of the very expensive sources she has to get more reliable reports. Again this suffices for the misleading reports not to be defeaters. Even before the rich agent exercises her wealth, the fact that her wealth gives her access to reports that will correct for misleading reports means that the misleading reports are not actually defeaters. So with her wealth she knows things she wouldn't otherwise know, even before her money goes to work. Again, her money doesn't generate knowledge -- the original reliable report is the knowledge generator -- but her wealth means that a would-be defeater doesn't gain traction.

The same thing is true in Russell and Doris's examples. The agent has quite a bit of evidence that \(p\). That's why she knows \(p\). There's a potential practical defeater for \(p\). But due to either indifference or wealth, the defeater is immunised. Surprisingly perhaps, indifference and/or wealth can be the difference between knowledge and ignorance. But that's not because they can be in any interesting sense `knowledge makers', any more than I can make a bowl of soup by preventing someone from tossing it out. Rather, they can be things that block defeaters, both when the defeaters are the kind Stanley talks about, and when they are more familiar kinds of defeaters.

\section{Temporal Embeddings} \label{sect:time}

Michael \cite{MBT2009} has argued that tense-shifted knowledge ascriptions can be used to show that his version of Lewisian contextualism is preferable to IRI. Like Russell and Doris, his argument uses a variant of Stanley's Bank Cases.\footnote{In the interests of space, I won't repeat those cases yet again here.} Let \(O\) be that the bank is open Saturday morning. If Hannah has a large debt, she is in a high-stakes situation with respect to \(O\). In Blome-Tillman's version of the example, Hannah had in fact incurred a large debt, but on Friday morning the creditor waived this debt. Hannah had no way of anticipating this on Thursday. She has some evidence for \(O\), but not enough for knowledge if she's in a high-stakes situation. Blome-Tillmann says that this means after Hannah discovers the debt waiver, she could say (\ref{ex:ThursdayFriday}).

\numbex{1}{
\item \label{ex:ThursdayFriday} I didn't know \(O\) on Thursday, but on Friday I did.
}

\noindent But I'm not sure why this case should be problematic for any version of IRI, and very unsure why it should even look like a \textit{reductio} of IRI. As Blome-Tillmann notes, it isn't really a situation where Hannah's stakes change. She was never actually in a high stakes situation. At most her perception of her stakes change; she thought she was in a high-stakes situation, then realised that she wasn't. Blome-Tillmann argues that even this change in perceived stakes can be enough to make (\ref{ex:ThursdayFriday}) true if IRI is true. Now actually I agree that this change in perception could be enough to make (\ref{ex:ThursdayFriday}) true, but when we work through the reason that's so, we'll see that it isn't because of anything distinctive, let alone controversial, about IRI.

If Hannah is rational, then given her interests she won't be ignoring \(\neg O\) possibilities on Thursday. She'll be taking them into account in her plans. Someone who is anticipating \(\neg O\) possibilities, and making plans for them, doesn't know \(O\). That's not a distinctive claim of IRI. Any theory should say that if a person is worrying about \(\neg O\) possibilities, and planning around them, they don't know \(O\). And that's simply because knowledge requires a level of confidence that such a person simply does not show. If Hannah is rational, that will describe her on Thursday, but not on Friday. So (\ref{ex:ThursdayFriday}) is true not because Hannah's practical situation changes between Thursday and Friday, but because her psychological state changes, and psychological states are relevant to knowledge.

What if Hannah is, on Thursday, irrationally ignoring \(\neg O\) possibilities, and not planning for them even though her rational self wishes she were planning for them? In that case, it seems she still believes \(O\). After all, she makes the same decisions as she would as if \(O\) were sure to be true. But it's worth remembering that if Hannah does irrationally ignore \(\neg O\) possibilities, she is being irrational with respect to \(O\). And it's very plausible that this irrationality defeats knowledge. That is, you can't be irrational with respect to a proposition and know it. Irrationality excludes knowledge. That's what we saw in Con's case in section \ref{sect:defeat}, and it's all we see here as well. Note also that Con will know $p$ after fixing his consequent-affirming disposition but not before. That's just what happens with Hannah; a distant change in her cognitive system will remove a defeater, and after it does she gets more knowledge.

There's a methodological point here worth stressing. Doing epistemology with imperfect agents often results in facing tough choices, where any way to describe a case feels a little counterintuitive. If we simply hew to intuitions, we risk being led astray by just focussing on the first way a puzzle case is described to us. But once we think through Hannah's case, we see perfectly good reasons, independent of IRI, to endorse IRI's prediction about the case.

\section{Problematic Conjunctions} \label{sect:conj}
 George and Ringo both have \$6000 in their bank accounts. They both are thinking about buying a new computer, which would cost \$2000. Both of them also have rent due tomorrow, and they won't get any more money before then. George lives in New York, so his rent is \$5000. Ringo lives in Syracuse, so his rent is \$1000. Clearly, (\ref{REC}) and (\ref{RAC}) are true.

\numbex{2}{
\item \label{REC} Ringo has enough money to buy the computer.
\item \label{RAC} Ringo can afford the computer.}

\noindent And (\ref{GEC}) is true as well, though there's at least a reading of (\ref{GAC}) where it is false.

\numbex{2}{
\item \label{GEC} George has enough money to buy the computer.
\item \label{GAC} George can afford the computer.
}

\noindent Focus for now on (\ref{GEC}). It is a bad idea for George to buy the computer; he won't be able to pay his rent. But he has enough money to do so; the computer costs \$2000, and he has \$6000 in the bank. So (\ref{GEC}) is true. Admittedly there are things close to (\ref{GEC}) that aren't true. He hasn't got enough money to buy the computer and pay his rent. You might say that he hasn't got enough money to buy the computer given his other financial obligations. But none of this undermines (\ref{GEC}). The point of this little story is to respond to another argument Blome-Tillmann offers against IRI. Here is how he puts the argument. (Again I've changed the numbering and some terminology for consistency with this paper.)

\begin{quote}
\noindent Suppose that John and Paul have exactly the same evidence, while John is in a low-stakes situation towards $p$ and Paul in a high-stakes situation towards $p$. Bearing in mind that IRI is the view that whether one knows $p$ depends on one's practical situation, IRI entails that one can truly assert:

\numbex{1}{
\item \label{SameEv} John and Paul have exactly the same evidence for $p$, but only John has enough evidence to know $p$, Paul doesn't.} \cite[328-9]{MBT2009}
\end{quote}

\noindent And this is meant to be a problem, because (\ref{SameEv}) is intuitively false.

But IRI doesn't entail any such thing. Paul does have enough evidence to know that $p$, just like George has enough money to buy the computer. Paul can't know that $p$, just like George can't buy the computer, because of their practical situations. But that doesn't mean he doesn't have enough evidence to know it. So, contra Blome-Tillmann, IRI doesn't entail this problematic conjunction.

In a footnote attached to this, Blome-Tillmann tries to reformulate the argument.

\begin{quote}
\noindent I take it that having enough evidence to `know $p$' in $C$ just means having evidence such that one is in a position to `know $p$' in $C$, rather than having evidence such that one `knows $p$'. Thus, another way to formulate (\ref{SameEv}) would be as follows: `John and Paul have exactly the same evidence for $p$, but only John is in a position to know $p$, Paul isn't.' \cite[329n23]{MBT2009}
\end{quote}

\noindent The `reformulation' is obviously bad, since having enough evidence to know $p$ isn't the same as being in a position to know it, any more than having enough money to buy the computer puts George in a position to buy it. But might there be a different problem for IRI here? Might it be that IRI entails (\ref{PosK}), which is false?

\numbex{1}{
\item \label{PosK} John and Paul have exactly the same evidence for $p$, but only John is in a position to know $p$, Paul isn't.}

\noindent There isn't a problem with (\ref{PosK}) because almost any epistemological theory will imply that conjunctions like that are true. In particular, any epistemological theory that allows for the existence of defeaters to not supervene on the possession of evidence will imply that conjunctions like (\ref{PosK}) are true. Again, it matters a lot that IRI is suggesting that traditional epistemologists did not notice that there are distinctively pragmatic defeaters. Once we see that, we'll see that conjunctions like (\ref{PosK}) are not surprising at all.

Consider again Con, and his friend Mod who is disposed to reason by modus ponens and not by affirming the consequent. We could say that Con and Mod have the same evidence for $p$, but only Mod is in a position to know $p$. There are only two ways to deny that conjunction. One is to interpret `position to know' so broadly that Con is in a position to know $p$ because he could change his inferential dispositions. But then we might as well say that Paul is in a position to know $p$ because he could get into a different `stakes' situation. Alternatively, we could say that Con's inferential dispositions count as a kind of evidence against $p$. But that stretches the notion of evidence beyond a breaking point. Note that we didn't say Con had any \textit{reason} to affirm the consequent, just that he does. Someone might adopt, or change, a poor inferential habit because they get new evidence. But they need not do so, and we shouldn't count their inferential habits as evidence they have.

If that case is not convincing, we can make the same point with a simple Gettier-style case.

\begin{quote}
\textbf{Getting the Job}

In world 1, at a particular workplace, someone is about to be promoted. Agnetha knows that Benny is the management's favour\-ite choice for the promotion. And she also knows that Benny is Swedish. So she comes to believe that the promotion will go to someone Swedish. Unsurprisingly, management does choose Benny, so Agnetha's belief is true.

World 2 is similar, except there it is Anni-Frid who knows that Benny is the management's favourite choice for the promotion, that Benny is Swedish. So \textit{she} comes to believe that the promotion will go to someone Swedish. But in this world Benny quits the workplace just before the promotion is announced, and the management unexpectedly passes over a lot of Danish workers to promote another Swede, namely Bj\"orn. So Anni-Frid's belief that the  promotion will go to someone Swedish is true, but not in a way that she could have expected.
\end{quote}

\noindent In that story, I think it is clear that Agnetha and Anni-Frid have exactly the same evidence that the job will go to someone Swedish, but only Agnetha is in a position to know this, Anni-Frid is not. The fact that an intermediate step is false in Anni-Frid's reasoning, but not Agnetha's, means that Anni-Frid's putative knowledge is defeated, but Agnetha's is not. And when that happens, we can have differences in knowledge without differences in evidence. So it isn't an argument against IRI that it allows differences in knowledge without differences in evidence.

\section{Holism and Defeaters} \label{sect:holism}
The big lesson of the last few sections is that interests create defeaters. Sometimes an agent can't know $p$ because adding $p$ to her stock of beliefs would introduce either incoherence or irrationality. The reason is normally that the agent faces some decision where it is, say, bad to do $\varphi$, but good to do $\varphi$ given $p$. In that situation, if she adds $p$, she'll either incoherently think that it's bad to do $\varphi$ although it's good to do it given what is (by her lights) true, or she'll irrationally think that it's good to do $\varphi$. Moreover, the IRI theorist says, being either incoherent or irrational in this way blocks knowledge, so the agent doesn't know $p$.

But there are other, more roundabout, ways in which interests can mean that believing $p$ would entail incoherence or irrationality. One of these is illustrated by an example alleged by Ram Neta to be hard for interest-relative theorists to accommodate.

\begin{quote}
Kate needs to get to Main Street by noon: her life depends upon it. She is desperately searching for Main Street when she comes to an intersection and looks up at the perpendicular street signs at that intersection. One street sign says ``State Street'' and the perpendicular street sign says ``Main Street.'' Now, it is a matter of complete indifference to Kate whether she is on State Street--nothing whatsoever depends upon it. \citep[182]{Neta2007}
\end{quote}

\noindent Let's assume for now that Kate is rational; dropping this assumption introduces mostly irrelevant complications.\footnote{It means we constantly have to run through both the irrationality horn of the dilemma from the first paragraph of this section as well as the incoherence horn, but the two horns look very similar in practice.} Kate will not believe she's on Main Street. She would only have that belief if she took it to be settled that she's on Main, and hence not worthy of spending further effort investigating. But presumably she won't do that. The rational thing for her to do is to get confirming (or, if relevant, confounding) evidence for the appearance that she's on Main. If it were settled that she was on Main, the rational thing to do would be to try to relax, and be grateful that she had found Main Street. Since she has different attitudes about what to do \textit{simpliciter} and conditional on being on Main Street, she doesn't believe she's on Main Street.

So far so good, but what about her attitude towards the proposition that she's on State Street? She has enough evidence for that proposition that her credence in it should be rather high. And no practical issues turn on whether she is on State. So she believes she is on State, right?

Not so fast! Believing that she's on State has more connections to her cognitive system than just producing actions. Note in particular that street signs are hardly basic epistemic sources. They are the kind of evidence we should be `conservative' about in the sense of \cite{Pryor2004-PRYWWW}. We should only use them if we antecedently believe they are accurate. So for Kate to believe she's on State, she'd have to believe street signs around here are accurate. If not, she'd incoherently be relying on a source she doesn't trust, even though it is not a basic source.\footnote{The caveats here about basic sources are to cancel any suggestion that Kate has to antecedently believe that any source is reliable before she uses it. As \cite{Pryor2000-PRYTSA} notes, that view is problematic. The view that we only get knowledge from a street sign if we antecedently have reason to trust it is not so implausible.} But if she believes the street signs are accurate, she'd believe she was on Main, and that would lead to practical incoherence. So there's no way to coherently add the belief that she's on State Street to her stock of beliefs. So she doesn't know, and can't know, that she's either on State or on Main. This is, in a roundabout way, due to the high stakes Kate faces.

Neta thinks that the best way for the interest-relative theorist to handle this case is to say that the high stakes associated with the proposition that Kate is on Main Street imply that certain methods of belief formation do not produce knowledge. And he argues, plausibly, that such a restriction will lead to implausibly sceptical results. But that's not the right way for the interest-relative theorist to go. What they should say is that Kate can't know she's on State Street because the only grounds for that belief is intimately connected to a proposition that, in virtue of her interests, she needs very large amounts of evidence to believe.

\section{Non-Consequentialist Cases}
None of the replies yet have leaned heavily on the point from section \ref{sect:existential}, the fact that IRI is an existential claim. This reply will make heavy use of that fact.

If an agent is merely trying to get the best outcome for themselves, then it makes sense to represent them as a utility maximiser. But when agents have to make decisions that might involve them causing harm to others if certain propositions turn out to be true, then I think it is not so clear that orthodox decision theory is the appropriate way to model the agents. That's relevant to cases like this one, which Jessica Brown has argued are problematic for the epistemological theories John Hawthorne and Jason Stanley have recently been defending.\footnote{The target here is not directly the interest-relativity of their theories, but more general principles about the role of knowledge in action and assertion. But it's important to see how IRI handles the cases that Brown discusses, since these cases are among the strongest challenges that have been raised to IRI.}

\begin{quote}
A student is spending the day shadowing a surgeon. In the morning he observes her in clinic examining patient A who has a diseased left kidney. The decision is taken to remove it that afternoon. Later, the student observes the surgeon in theatre where patient A is lying anaesthetised on the operating table. The operation hasn't started as the surgeon is consulting the patient's notes. The student is puzzled and asks one of the nurses what's going on: 

\textbf{Student}: I don't understand. Why is she looking at the patient's records? She was in clinic with the patient this morning. Doesn't she even know which kidney it is? 

\textbf{Nurse}: Of course, she knows which kidney it is. But, imagine what it would be like if she removed the wrong kidney. She shouldn't operate before checking the patient's records. \citep[1144-1145]{Brown2008-BROKAP}
\end{quote}

\noindent It is tempting, but I think mistaken, to represent the surgeon's choice as follows. Let \textbf{Left} mean the left kidney is diseased, and \textbf{Right} mean the right kidney is diseased.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{Left} & \textbf{Right} \\
\textbf{Remove left kidney} & \(1\) & \(-1\) \\
\textbf{Remove right kidney} & \(-1\) & \(1\) \\
\textbf{Check notes} & \(1-\varepsilon\) & \(1-\varepsilon\) \\
\end{tabular}
\end{center}

\noindent Here \(\varepsilon\) is the trivial but non-zero cost of checking the chart. Given this table, we might reason that since the surgeon knows that she's in the left column, and removing the left kidney is the best option in that column, she should remove the left kidney rather than checking the notes.

But that reasoning assumes that the surgeon does not have any obligations over and above her duty to maximise expected utility. And that's very implausible, since consequentialism is a fairly implausible theory of medical ethics.\footnote{I'm not saying that consequentialism is wrong as a theory of medical ethics. But if it is right, so many intuitions about medical ethics are going to be mistaken that such intuitions have no evidential force. And Brown's argument relies on intuitions about this case having evidential value. So I think for her argument to work, we have to suppose non-consequentialism about medical ethics.}

It's not clear exactly what the obligation the surgeon has. Perhaps it is an obligation to not just know which kidney to remove, but to know this on the basis of evidence she has obtained while in the operating theatre. Or perhaps it is an obligation to make her belief about which kidney to remove as sensitive as possible to various possible scenarios. Before she checked the chart, this counterfactual was false: \textit{Had she misremembered which kidney was to be removed, she would have a true belief about which kidney was to be removed.} Checking the chart makes that counterfactual true, and so makes her belief that the left kidney is to be removed a little more sensitive to counterfactual possibilities. 

However we spell out the obligation, it is plausible given what the nurse says that the surgeon has some such obligation. And it is plausible that the `cost' of violating this obligation, call it \(\delta\), is greater than the cost of checking the notes. So here is the decision table the surgeon faces.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{Left} & \textbf{Right} \\
\textbf{Remove left kidney} & \(1-\delta\) & \(-1-\delta\) \\
\textbf{Remove right kidney} & \(-1-\delta\) & \(1-\delta\) \\
\textbf{Check notes} & \(1-\varepsilon\) & \(1-\varepsilon\) \\
\end{tabular}
\end{center}

\noindent And it isn't surprising, or a problem for an interest-relative theory of knowledge, that the surgeon should check the notes, even if she believes \textit{and knows} that the left kidney is the diseased one.

There is a very general point here. The best arguments for IRI start with the role that knowledge plays in a particular theory of decision or reasoning. It's easiest to make the arguments for IRI work if that theory is orthodox (consequentialist) decision theory. That doesn't mean that the arguments for IRI presuppose that consequentialism is always the right decision theory. As long as consequentialism is correct \textit{in the case described}, the argument for IRI can work. (By consequentialism being correct in a case, I mean that it can be preferable to choose $\varphi$ over $\psi$ in some case because $\varphi$ has the higher expected utility. It's plausible that there are such cases because it's plausible that there are choices we face where the options differ in no normatively salient respect except expected utiilty.) Remember, the IRI theorist is trying to prove an existential: there is a pair of cases that differ with respect to knowledge in virtue of differing with respect to interests. And that just needs consequentialism to be locally true. The only way medical cases like Brown's could be counterexamples to IRI is if we assumed that consequentialism was globally true, but it probably isn't, so IRI survives the examples.


%\newpage
%\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{../OldPapers/phil_review}
\bibliography{IRIbib}

\end{document}
