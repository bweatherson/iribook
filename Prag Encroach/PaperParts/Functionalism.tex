\subsection{Interests and Functional Roles}
The previous section was largely devoted to proving an existential claim: there is \textit{some} interest-relativity to knowledge. Or, if you prefer, it proved a negative claim: the best theory of knowledge is \textit{not} interest-neutral. But this negative conclusion invites a philosophical challenge: what is the best explanation of the interest-relativity of knowledge? My answer is in two parts. Part of the interest-relativity of knowledge comes from the interest-relativity of belief, and part of it comes from the fact that interests generate certain kinds of doxastic defeaters.

We can see that belief is interest-relative by seeing that the best functionalist theory of belief is interest-relative. Since the best functionalist theory of belief is the true theory of belief, that means belief is interest-relative. That last sentence assumed a non-trivial premise, namely that functionalism about belief is true. I'm not going to argue for that, since the argument would take at least a book. (The book in question might look a lot like \cite{DBMJackson2007}.) But I am going to, in this section, argue for what I said in the first sentence of this paragraph, namely that the best functionalist account of belief is interest-relative.

In my \citeyearpar{Weatherson2005-WEACWD}, I suggested that was all of the explanation of the interest-relativity of knowledge. I was wrong, and section 3 of this paper will show why I was wrong. Interests also generate certain kinds of doxastic defeaters, and they enter independently into the explanation of why knowledge is interest-relative. Very roughly, the idea is that $S$ doesn't know $p$ if $S$'s belief that $p$ does not sufficiently cohere with the rest of what she should believe and does believe. If this coherence constraint fails, there is a doxastic defeater. When is some incoherence too much incoherence for knowledge? That turns out to be interest-relative, in cases like the case of Coraline described in the next section. But we're getting ahead of ourselves, the first task is to link functionalism about belief and the interest-relativity of belief.

We start not with the functional characterisaton of belief, but with something closeby, the functional characterisation of credence. Frank \cite{RamseyTruthProb} provides a clear statement of one of the key functional roles of credences; their connection to action. Of course, Ramsey did not take himself to be providing one component of the functional theory of credence. He took himself to be providing a behaviourist/operationalist reduction of credences to dispositions. But we do not have to share Ramsey's metaphysics to use his key ideas. Those ideas include that it's distinctively \textit{betting} dispositions that are crucial to the account of credence, and that all sorts of actions in everyday life constitute bets.

The connection to betting behaviour lives on today most prominently in the work on `representation theorems'.\footnote{See \cite{Maher1993} for the most developed account in recent times.} What a representation theorem shows is that for any agent whose pairwise preferences satisfy some structural constraints, there is a probability function and a utility function such that the agent prefers bet \(X\) to bet \(Y\) just in case the expected utility of \(X\) (given that probability and utility function) is greater than that of \(Y\). Moreover, the probability function is unique (and the utility function is unique up to positive affine transformations). Given that, it might seem plausible to identify the agent's credence with this probability function, and the agent's (relative) values with this utility function.

Contemporary functionalism goes along with much, but not quite all, of this picture. The betting preferences are an important part of the functional role of a credence; indeed, they just are the output conditions. But there are two other parts to a functional role: an input condition and a set of internal connections. So the functionalist thinks that the betting dispositions are not quite sufficient for having credences. A pre-programmed automaton might have dispositions to accept (or at least move as if accepting) various bets, but this will not be enough for credences \citep{DBMJackson2007}. So when we're considering whether someone is in a particular credal state, we have to consider not just their actions, and their disposition to act, but the connections between the alleged credal state and other states.

The same will be true of belief. Part of what it is to believe \(p\) is to act as if \(p\) is true. Another part is to have other mental states that make sense in light of \(p\). This isn't meant to rule out the possibility of inconsistent beliefs. As David \cite{Lewis1982c} points out, it is easy to have inconsistent beliefs if we don't integrate our beliefs fully. What it is meant to rule out is the possibility of a belief that is not at all integrated into the agent's cognitive system. If the agent believes \(q\), but refuses to infer \(p \wedge q\), and believes \(p \rightarrow r\), but refuses to infer \(r\), and so on for enough other beliefs, she doesn't really believe \(p\). 

When writers start to think about the connection between belief and credence, they run into the following problem fairly quickly. The alleged possibility where \(S\) believes \(p\), and doesn't believe \(q\), but her credence in \(q\) is higher than her credence in \(p\) strikes many theorists as excessively odd.\footnote{I'm going to argue below that cases like that of Barry and Beth suggest that in practice this isn't nearly as odd as it first seems.} That suggests the so-called `threshold view' of belief, that belief is simply credence above a threshold. It is also odd to say that a rational, reflective agent could believe \(p\), believe \(q\), yet take it as an open question whether \(p \wedge q\) is true, refusing to believe or disbelieve it. Finally, it seems we can believe things to which we don't give credence 1. In the case of Barry and Beth from section 1, for example, before the genie comes in, it seems Barry does believe the musician is Beth. But he doesn't have credence 1 in this, since having credence 1 means being disposed to make a bet at any odds. And we can't have all three of these ideas. That is, we can't accept the `threshold view', the closure of belief under conjunction, and the possibility that propositions with credence less than one are believed.

We can raise the same kind of problem by looking directly at functional roles. A key functional role of credences is that if an agent has credence \(x\) in \(p\) she should be prepared to buy a bet that returns 1 util if \(p\), and 0 utils otherwise, iff the price is no greater than \(x\) utils. A key functional role of belief is that if an agent believes \(p\), and recognises that \(\phi\) is the best thing to do given \(p\), then she'll do \(\phi\). Given \(p\), it's worth paying any price up to 1 util for a bet that pays 1 util if \(p\). So believing \(p\) seems to mean being in a functional state that is like having credence 1 in \(p\). But as we argued in the previous paragraph, it is wrong to identify belief with credence 1.

If we spell out more carefully what the functional states of credence and belief are, a loophole emerges in the argument that belief implies credence 1. The interest-relative theory of belief exploits that loophole. What's the difference, in functional terms, between having credence \(x\) in \(p\), and having credence \(x + \varepsilon\) in \(p\)? Well, think again about the bet that pays 1 util if \(p\), and 0 utils otherwise. And imagine that bet is offered for \(x + \nicefrac{\varepsilon}{2}\) utils. The person whose credence is \(x\) will decline the offer; the person whose credence is \(x + \varepsilon\) will accept it. Now it will usually be that no such bet is on offer.\footnote{There are exceptions, especially in cases where \(p\) concerns something significant to financial markets, and the agent trades financial products. If you work through the theory that I'm about to lay out, one consequence is that such agents should have very few unconditional beliefs about financially-sensitive information, just higher and lower credences. I think that's actually quite a nice outcome, but I'm not going to rely on that in the argument for the view.} No matter; as long as one agent is \textit{disposed} to accept the offer, and the other agent is not, that suffices for a difference in credence.

The upshot of that is that differences in credences might be, indeed usually will be, constituted by differences in dispositions concerning how to act in choice situations far removed from actuality. I'm not usually in a position of having to accept or decline a chance to buy a bet for 0.9932 utils that the local coffee shop is currently open. Yet whether I would accept or decline such a bet matters to whether my credence that the coffee shop is open is 0.9931 or 0.9933. This isn't a problem with the standard picture of how credences work. It's just an observation that the high level of detail embedded in the picture relies on taking the constituents of mental states to involve many dispositions.

It isn't clear that belief should be defined in terms of the same kind of dispositions involving better behaviour in remote possibilities. It's true that if I believe that \(p\), and I'm rational enough, I'll act as if \(p\) is true. Is it also true that if I believe \(p\), I'm disposed to act as if \(p\) is true no matter what choices are placed in front of me? I don't see any reason to say yes, and there are a few reasons to say no. As we say in the case of Barry and Beth, Barry can believe that \(p\), but be disposed to \textit{lose that belief} rather than act on it if odd choices, like that presented by the genie, emerge.

%Quantifier domain variation
This suggests the key difference between belief and credence 1. For a rational agent, a credence of 1 in $p$ means that the agent is disposed to answer a wide range of questions the same way she would answer that question conditional on \(p\). That follows from the fact that these four principles are trivial theorems of the orthodox theory of expected utility.\footnote{The presentation in this section, as in \cite{Weatherson2005-WEACWD}, assumes at least a weak form of consequentialism. This was arguably a weakness of the earlier paper. We'll return to the issue of what happens in cases where the agent doesn't, and perhaps shouldn't, maximise expected utility, at the end of the section.} 

\begin{description*}
\item[C1AP] For all \(q, x\), if \(\Pr(p) = 1\) then \(\Pr(q) = x\) iff \(\Pr(q | p) = x\).
\item[C1CP] For all \(q, r\), if \(\Pr(p) = 1\) then \(\Pr(q) \geq \Pr(r)\) iff \(\Pr(q | p) \geq \Pr(r | p)\).
\item[C1AU] For all \(\phi, x\), if \(\Pr(p) = 1\) then \(U(\phi) = x\) iff \(U(\phi | p) = x\).
\item[C1CP] For all \(\phi, \psi\), if \(\Pr(p) = 1\) then \(U(\phi) \geq U(\psi)\) iff \(U(\phi | p) \geq U(\psi | p)\).
\end{description*}

\noindent In the last two lines, I use \(U(\phi)\) to denote the expected utility of \(\phi\), and \(U(\phi | p)\) to denote the expected utility of \(\phi\) conditional on \(p\). It's often easier to write this as simply \(U(\phi \wedge p)\), since the utility of \(\phi\) conditional on \(p\) just is the utility of doing \(\phi\) in a world where \(p\) is true. That is, it is the utility of \(\phi \wedge p\) being realised. But we get a nicer symmetry between the probabilistic principles and the utility principles if we use the explictly conditional notation for each.

If we make the standard kinds of assumptions in orthodox decision theory, i.e., assume at least some form of probabilism and consequentialism\footnote{I mean here consequentialism in roughly the sense used by \cite{Hammond1988}.}, then the agent will answer each of these questions the same way simpliciter and conditional on \(p\).

\begin{itemize*}
\item How probable is \(q\)?
\item Is \(q\) or \(r\) more probable?
\item How good an idea is it to do \(\phi\)?
\item Is it better to do \(\phi\) or \(\psi\)?
\end{itemize*}

\noindent Each of those questions is schematic. As in the more technical versions given above, they quantify over propositions and actions, albeit tacitly in the case of these versions. And these quantifiers have a very large domain. The standard theory is that an agent whose credence in \(p\) is 1 will have the same credence in \(q\) as in \(q\) given \(p\) for any \(q\) whatsoever.

Now in one sense, exactly the same things are true if the agent believes \(p\). If one is wondering whether \(q\) or \(r\) is more probable, and one believes \(p\), then the fact that \(p\) will be taken as a given in structring this inquiry. So conditionalising on \(p\) should not change the answer to the question. And the same goes for any actions \(\phi\) and \(\psi\) that the agent is choosing between. But this isn't true unrestrictedly. That's what we saw in the case of Barry and Beth. Which choices are on the table will change which things the agent will take as given, will use to structure inquiry, will believe. If we return to the technical version of our four questions, we might put all this the following way.

\begin{description*}
\item[BAP] For all relevant \(q, x\), if \(p\) is believed then \(\Pr(q) = x\) iff \(\Pr(q | p) = x\).
\item[BCP] For all relevant \(q, r\), if \(p\) is believed then \(\Pr(q) \geq \Pr(r)\) iff \(\Pr(q | p) \geq \Pr(r | p)\).
\item[BAU] For all relevant \(\phi, x\), if \(p\) is believed then \(U(\phi) = x\) iff \(U(\phi | p) = x\).
\item[BCP] For all relevant \(\phi, \psi\), if \(p\) is believed then \(U(\phi) \geq U(\psi)\) iff \(U(\phi | p) \geq U(\psi | p)\).
\end{description*}

\noindent This is where interests, theoretical or practical, matter. The functional definition of belief looks a lot like the functional definition of credence 1. But there's one key difference. Both definitions involve quantifiers: quantifying over propositions, actions and values. In the definition of credence 1, those quantifiers are (largely) unrestricted. In the definition of belief, those quantifiers are tightly restricted, and the restrictions are in terms of the interests, practical and theoretical, of the agent.

In the earlier paper I went into a lot of detail about just what `relevant' means in this context, and I won't repeat that here. But I will say a little about one point I didn't sufficiently stress in that paper: the importance of the restriction of \textbf{BAP} and \textbf{BAU} to \textit{relevant} values of \(x\). This lets us have the following nice consequence.

Charlie is trying to figure out exactly what the probability of \(p\) is. That is, for any \(x \in [0, 1]\), whether \(\Pr(p) = x\) is a relevant question. Now Charlie is well aware that \(\Pr(p | p) = 1\). So unless \(\Pr(p) = 1\), Charlie will give a different answer to the questions \textit{How probable is p?} and \textit{Given p, how probable is p?}. So unless Charlie holds that \(\Pr(p)\) is 1, she won't count as believing that \(p\). One consequence of this is that Charlie can't reason, ``The probability of \(p\) is exactly 0.978, so \(p\).'' That's all to the good, since that looks like bad reasoning. And it looks like bad reasoning even though in some circumstances Charlie can rationally believe propositions that she (rationally) gives credence 0.978 to.

But note that the reasoning in the previous paragraph assumes that every question of the form \textit{Is the probability of p equal to x?} is relevant. In practice, fewer questions than that will be relevant. Let's say that the only questions relevant to Charlie are of the form \textit{What is the probability of \(p\) to one decimal place?}. And assume that no other questions become relevant in the course of her inquiry into this question.\footnote{This is probably somewhat unrealistic. It's hard to think about whether \(\Pr(p)\) is closer to 0.7 or 0.8 without raising to salience questions about, for example, what the second decimal place in \(\Pr(p)\) is. This is worth bearing in mind when coming up with intuitions about the cases in this paragraph.} Charlie decides that to the first decimal place, \(\Pr(p) = 1.0\), i.e., \(\Pr(p) > 0.95\). That is compatible with simply believing that \(p\). And that seems right; if for practical purposes, the probability of \(p\) is indistinguishable from 1, then the agent is confident enough in \(p\) to believe it.