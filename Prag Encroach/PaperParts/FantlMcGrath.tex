\section{Fantl and McGrath on Interest-Relativity}

Jeremy Fantl and Matthew McGrath \citeyearpar{FantlMcGrath2009} have argued that my interest-relative theory of belief cannot explain all of the interest-relativity in epistemology. I'm going to agree with their conclusion, but not with their premises. At this point you might suspect, dear reader, that you're about to be drawn into a micro-battle between two similar but not quite identical explanations of the same (alleged) phenomena. And you wouldn't be entirely mistaken. But I think seeing why Fantl and McGrath's objections to my theory fail will show us something interesting about the relationship between interests and knowledge. In particular, it will show us that interests can generate certain kinds of \textit{defeaters} of claims to knowledge.

Fantl and McGrath's primary complaint against the interest-relative theory of belief I developed in my \cite{Weatherson2005-WEACWD} and in the previous section is that it is not strong enough to entail principles such as (JJ).

\begin{description}
\item[(JJ)] If you are justified in believing that \(p\), then \(p\) is warranted enough to justify you in \(\varphi\)-ing, for any \(\varphi\). \cite[99]{FantlMcGrath2009}
\end{description}

\noindent In practice, what this means is that there can't be a salient $p, \varphi$ such that:

\begin{itemize*}
\item The agent is justified in believing $p$;
\item The agent is not warranted in doing $\varphi$; but
\item If the agent had more evidence for $p$, and nothing else, the agent would be be warranted in doing $\varphi$.
\end{itemize*}

\noindent That is, once you've got enough evidence, or warrant, for justified belief in $p$, then you've got enough evidence for $p$ as matters for any decision you face. This seems intuitive, and Fantl and McGrath back up its intuitiveness with some nicely drawn examples.

Now it's true that the interest-relative theory of belief cannot be used to derive (JJ), at least on the reading of it I just provided. But that's because on the intended reading, it is false, and the interest-relative theory is true. So the fact that (JJ) can't be derived is a feature, not a bug. The problem arises because of cases like that of Coraline. Here's what we're going to stipulate about Coraline.

\begin{itemize*}
\item She knows that \(p\) and \(q\) are independent, so her credence in any conjunction where one conjunct is a member of  \(\{p,  \neg p\}\) and the other is a member of \(\{q, \neg q\}\) will be the product of her credences in the conjuncts.
\item Her credence in \(p\) is 0.99, just as the evidence supports.
\item Her credence in \(q\) is also 0.99. This is unfortunate, since the rational credence in \(q\) given her evidence is 0.01.
\item She has a choice between taking and declining a bet with the following payoff structure.\footnote{I'm more interested in the abstract structure of the case than in whether any real-life situation is modelled by just this structure. But it might be worth noting the rough kind of situation where this kind of situation can arise. So let's say Coraline has a particular bank account that is uninsured, but which currently paying 10\% interest, and she is deciding whether to deposit another \$1000 in it. Then \(p\) is the proposition that the bank will not collapse, and she'll get her money back, and \(q\) is the proposition that the interest will stay at 10\%. To make the model exact, we have to also assume that if the interest rate on her account doesn't stay at 10\%, it falls to 0.1\%. And we have to assume that the interest rate and the bank's collapse are probabilistically independent. Neither of these are at all realistic, but a realistic case would simply be more complicated, and the complications would obscure the philosophically interesting point.} (Assume that the marginal utility of money is close enough to constant that expected dollar returns correlate more or less precisely with expected utility returns.)
\end{itemize*}

\begin{center}
\begin{tabular}{r c c c}
 & \textbf{\(p \wedge q\)} & \textbf{\(p \wedge \neg q\)} & \textbf{\(\neg p\)} \\
\textbf{Take bet} & \$100 & \$1 & \(-\$1000\) \\
\textbf{Decline bet} & 0 & 0 & 0 \\
\end{tabular}
\end{center}

\noindent As can be easily computed, the expected utility of taking the bet given her credences is positive, it is just over \$89. And Coraline takes the bet. She doesn't compute the expected utility, but she is sensitive to it.\footnote{If she did compute the expected utility, then one of the things that would be salient for her is the expected utility of the bet. And the expected utility of the bet is different to its expected utility given \(p\). So if that expected utility is salient, she doesn't believe \(p\). And it's going to be important to what follows that she \textit{does} believe \(p\).} That is, had the expected utility given her credences been close to 0, she would have not acted until she made a computation. But from her perspective this looks like basically a free \$100, so she takes it. Happily, this all turns out well enough, since \(p\) is true. But it was a dumb thing to do. The expected utility of taking the bet given her evidence is negative, it is a little under -\$8. So she isn't warranted, given her evidence, in taking the bet.

I also claim the following three things are true of her.

\begin{enumerate*}
\item \(p\) is not justified enough to warrant her in taking the bet.
\item She believes \(p\).\footnote{In terms of the example discussed in the previous footnote, she believes that the bank will survive, i.e., that she'll get her money back if she deposits it.}
\item This belief is rational.
\end{enumerate*}

\noindent The argument for 1 is straightforward. She isn't warranted in taking the bet, so \(p\) isn't sufficiently warranted to justify it. This is despite the fact that \(p\) is obviously relevant. Indeed, given \(p\), taking the bet strictly dominates declining it. But still, \(p\) doesn't warrant taking this bet, because nothing warrants taking a bet with negative expected utility. Had the rational credence in \(p\) been higher, then the bet would have been reasonable. Had the reasonable credence in \(p\) been, say, 0.9999, then she would have been reasonable in taking the bet, and using \(p\) as a reason to do so. So there's a good sense in which \(p\) simply isn't warranted enough to justify taking the bet.\footnote{I'm assuming here that the interpretation of (JJ) that I gave above is correct, though actually I'm not entirely sure about this. For present purposes, I plan to simply interpret (JJ) that way, and not return to exegetical issues.}

The argument for 2 is that she has a very high credence in \(p\), this credence is grounded in the evidence in the right way, and it leads her to act as if \(p\) is true, e.g. by taking the bet. It's true that her credence in \(p\) is not 1, and if you think credence 1 is needed for belief, then you won't like this example. But if you think that, you won't think there's much connection between (JJ) and pragmatic conditions in epistemology either. So that's hardly a position a defender of Fantl and McGrath's position can hold.\footnote{We do have to assume that \(\neg q\) is not so salient that attitudes conditional on \(\neg q\) are relevant to determining whether she believes \(p\). That's because conditional on \(\neg q\), she prefers to not take the bet, but conditional on \(\neg q \wedge p\), she prefers to take the bet. But if she is simply looking at this as a free \$100, then it's plausible that \(\neg q\) is not salient.}

The argument for 3 is that her attitude towards \(p\) tracks the evidence perfectly. She is making no mistakes with respect to \(p\). She is making a mistake with respect to \(q\), but not with respect to \(p\). So her attitude towards \(p\), i.e. belief, is rational.

I don't think the argument here strictly needs the assumption I'm about to make, but I think it's helpful to see one very clear way to support the argument of the last paragraph. The working assumption of my project on interest-relativity has been that talking about beliefs and talking about credences are simply two ways of modelling the very same things, namely minds. If the agent both has a credence 0.99 in \(p\), and believes that \(p\), these are not two different states. Rather, there is one state of the agent, and two different ways of modelling it. So it is implausible, if not incoherent, to apply different valuations to the state depending on which modelling tools we choose to use. That is, it's implausible to say that while we're modelling the agent with credences, the state is rational, but when we change tools, and start using beliefs, the state is irrational. Given this outlook on beliefs and credences, premise 3 seems to follow immediately from the setup of the example.

So that's the argument that (JJ) is false. And if it's false, the fact that the interest-relative theory doesn't entail it is a feature, not a bug. But there are a number of possible objections to that position. I'll spend the rest of this section, and this paper, going over them.\footnote{Thanks here to a long blog comments thread with Jeremy Fantl and Matthew McGrath for making me formulate these points much more carefully. The original thread is at \url{http://tar.weatherson.org/2010/03/31/do-justified-beliefs-justify-action/}.}

%\objrep is my objections and replies code. It is defined in collpapers.
%\argconc is my code for making the next label C. It is defined in collpapers.

\objrep{
The following argument shows that Coraline is not in fact justified in believing that \(p\).

\begin{enumerate*}
\item \(p\) entails that Coraline should take the bet, and Coraline knows this.
\item If \(p\) entails something, and Coraline knows this, and she justifiably believes \(p\), she is in a position to justifiably believe the thing entailed.
\item Coraline is not in a position to justifiably believe that she should take the bet.
\argconc
\item So, Coraline does not justifiably believe that \(p\)
\end{enumerate*}}
{The problem here is that premise 1 is false. What's true is that \(p\) entails that Coraline will be better off taking the bet than declining it. But it doesn't follow that she should take the bet. Indeed, it isn't actually true that she should take the bet, even though \(p\) is actually true. Not just is the entailment claim false, the world of the example is a counterinstance to it.

It might be controversial to use this very case to reject premise 1. But the falsity of premise 1 should be clear on independent grounds. What \(p\) entails is that Coraline will be best off by taking the bet. But there are lots of things that will make me better off that I shouldn't do.  Imagine I'm standing by a roulette wheel, and the thing that will make me best off is betting heavily on the number than will actually come up. It doesn't follow that I should do that. Indeed, I should not do it. I shouldn't place any bets at all, since all the bets have a highly negative expected return. 

In short, all \(p\) entails is that taking the bet will have the best consequences. Only a very crude kind of consequentialism would identify what I should do with what will have the best returns, and that crude consequentialism isn't true. So \(p\) doesn't entail that Coraline should take the bet. So premise 1 is false.}

\objrep{
Even though \(p\) doesn't \textit{entail} that Coraline should take the bet, it does provide inductive support for her taking the bet. So if she could justifiably believe \(p\), she could justifiably (but non-deductively) infer that she should take the bet. Since she can't justifiably infer that, she isn't justified in taking the bet.
}
{The inductive inference here looks weak. One way to make the inductive inference work would be to deduce from \(p\) that taking the bet will have the best outcomes, and infer from that that the bet should be taken. But the last step doesn't even look like a reliable ampliative inference. The usual situation is that the best outcome comes from taking an \textit{ex ante} unjustifiable risk.

It may seem better to use \(p\) combined with the fact that conditional on \(p\), taking the bet has the highest \textit{expected} utility. But actually that's still not much of a reason to take the bet. Think again about cases, completely normal cases, where the action with the best outcome is an \textit{ex ante} unjustifiable risk. Call that action \(\varphi\), and let \(B \varphi\) be the proposition that \(\varphi\) has the best outcome. Then \(B \varphi\) is true, and conditional on \(B \varphi\), \(\varphi\) has an excellent expected return. But doing \(\varphi\) is still running a dumb risk. Since these kinds of cases are normal, it seems it will very often be the case that this form of inference leads from truth to falsity. So it's not a reliable inductive inference.

More generally, we should worry quite a lot about Coraline's ability to draw inductive inferences about the propriety of the bet here. Unlike deductive inferences, inductive inferences can be defeated by a whole host of factors. If I've seen a lot of swans, in a lot of circumstances, and they've all been blue, that's a good reason to think the next swan I see will be blue. But it ceases to be a reason if I am told by a clearly reliable testifier that there are green swans in the river outside my apartment. And that's true even if I dismiss the testifier because I think he has a funny name, and I don't trust people with funny names. Now although Coraline has evidence for \(p\), she also has a lot of evidence against \(q\), evidence that she is presumably ignoring since her credence in \(q\) is so high. Any story about how Coraline can reason from \(p\) to the claim that she should have to take the bet will have to explain how her irrational attraction to \(q\) doesn't serve as a defeater, and I don't see how that could be done.}

\objrep{In the example, Coraline isn't just in a position to justifiably believe \(p\), she is in a position to \textit{know} that she justifiably believes it. And from the fact that she justifiably believes \(p\), and the fact that if \(p\), then taking the bet has the best option, she can infer that she should take the bet.}
{It's possible at this point that we get to a dialectical impasse. I think this inference is non-deductive, because I think the example we're discussing here is one where the premises are true and the conclusion false. Presumably someone who doesn't like the example will think that it is a good deductive inference.

What makes the objection useful is that, unlike the inductive inference mentioned in the previous objection, this at least has the \textit{form} of a good inductive inference. Whenever you justifiably believe \(p\), and the best outcome given \(p\) is gained by doing \(\varphi\), then \textit{usually} you should \(\varphi\). Since Coraline knows the premises are true, ceteris paribus that gives her a reason to believe the premise is probably true.

But other things aren't at all equal. In particular, this is a case where Coraline has a highly irrational credence concerning a proposition whose probability is highly relevant to the expected utility of possible actions. Or, to put things another way, an inference from something to something else it is correlated with can be defeated by related irrational beliefs. (That's what the swan example above shows.) So if Coraline tried to infer this way that she should take the bet, her irrational confidence in \(q\) would defeat the inference.

The objector might think I am being uncharitable here. The objection doesn't say that Coraline's knowledge provides an \textit{inductive} reason to take the bet. Rather, they say, it provides a \textit{conclusive} reason to take the bet. And conclusive reasons cannot be defeated by irrational beliefs elsewhere in the web. Here we reach an impasse. I say that knowledge that you justifiably believe \(p\) cannot provide a conclusive reason to bet on \(p\) because I think Coraline knows she justifiably believes \(p\), but does not have a conclusive reason to bet on \(p\). That if, I think the premise the objector uses here is false because I think (JJ) is false. The person who believes in (JJ) won't be so impressed by this move. 

Having said all that, the more complicated example at the end of \cite{Weatherson2005-WEACWD} was designed to raise the same problem without the consequence that if \(p\) is true, the bet is sure to return a positive amount. In that example, conditionalising on \(p\) means the bet has a positive expected return, but still possibly a negative return. But in that case (JJ) still failed. If accepting there are cases where an agent justifiably believes \(p\), and knows this, but can't rationally bet on \(p\) is too much to accept, that more complicated example might be more persuasive. Otherwise, I concede that someone who believes (JJ) and thinks rational agents can use it in their reasoning will not think that a particular case is a counterexample to (JJ).}

\objrep{If Coraline were ideal, then she wouldn't believe \(p\). That's because if she were ideal, she would have a lower credence in \(q\), and if that were the case, her credence in \(p\) would have to be much higher (close to 0.999) in order to count as a belief. So her belief is not justified.}
{The premise here, that if Coraline were ideal she would not believe that \(p\), is true. The conclusion, that she is not justified in believing \(p\), does not follow. It's always a mistake to \textit{identify} what should be done with what is done in ideal circumstances. This is something that has long been known in economics. The \textit{locus classicus} of the view that this is a mistake is \cite{LipseyLancaster}. A similar point has been made in ethics in papers such as \cite{Watson1977} and \cite{KennettSmith1996b, KennettSmith1996a}. And it has been extended to epistemology by \cite{Williamson1998-WILCOK}.

All of these discussions have a common structure. It is first observed that the ideal is both \(F\) and \(G\). It is then stipulated that whatever happens, the thing being created (either a social system, an action, or a cognitive state) will not be \(F\). It is then argued that given the stipulation, the thing being created should not be \(G\). That is not just the claim that we shouldn't \textit{aim} to make the thing be \(G\). It is, rather, that in many cases being \(G\) is not the best way to be, given that \(F\)-ness will not be achieved. Lipsey and Lancaster argue that (in an admittedly idealised model) that it is actually quite unusual for \(G\) to be best given that the system being created will not be \(F\).

It's not too hard to come up with examples that fit this structure. Following \cite[209]{Williamson2000-WILKAI}, we might note that I'm justified in believing that there are no ideal cognitive agents, although were I ideal I would not believe this. Or imagine a student taking a ten question mathematics exam who has no idea how to answer the last question. She knows an ideal student would correctly answer an even number of questions, but that's no reason for her to throw out her good answer to question nine. In general, once we have stipulated one departure from the ideal, there's no reason to assign any positive status to other similarities to the idea. In particular, given that Coraline has an irrational view towards \(q\), she won't perfectly match up with the ideal, so there's no reason it's good to agree with the ideal in other respects, such as not believing \(p\).

Stepping back a bit, there's a reason the interest-relative theory says that the ideal and justification come apart right here. On the interest-relative theory, like on any pragmatic theory of mental states, the \textit{identification} of mental states is a somewhat holistic matter. Something is a belief in virtue of its position in a much broader network. But the \textit{evaluation} of belief is (relatively) atomistic. That's why Coraline is justified in believing \(p\), although if she were wiser she would not believe it. If she were wiser, i.e., if she had the right attitude towards \(q\), the very same credence in \(p\) would not count as a belief. Whether her state counts as a belief, that is, depends on wide-ranging features of her cognitive system. But whether the state is justified depends on more local factors, and in local respects she is doing everything right.}

\objrep{Since the ideal agent in Coraline's position would not believe \(p\), it follows that there is no \textit{propositional} justification for \(p\). Moreover, doxastic justification requires propositional justification\footnote{See \cite{Turri2010} for a discussion of recent views on the relationship between propositional and doxastic justification. This requirement seems to be presupposed throughout that literature.} So Coraline is not doxastically justified in believing \(p\). That is, she isn't justified in believing \(p\).}
{I think there are two ways of understanding `propositional justification'. On one of them, the first sentence of the objection is false. On the other, the second sentence is false. Neither way does the objection go through.

The first way is to say that \(p\) is propositionally justified for an agent iff that agent's evidence justifies a credence in \(p\) that is high enough to count as a belief \textit{given the agent's other credences and preferences}. On that understanding, \(p\) is propositionally justified by Coraline's evidence. For all that evidence has to do to make \(p\) justified is to support a credence a little greater than 0.9. And by hypothesis, the evidence does that.

The other way is to say that \(p\) is propositionally justified for an agent iff that agent's evidence justifies a credence in \(p\) that is high enough to count as a belief \textit{given the agent's preferences and the credences supported by that evidence}. On this reading, the objection reduces to the previous objection. That is, the objection basically says that \(p\) is propositionally justified for an agent iff the ideal agent in her situation would believe it. And we've already argued that that is compatible with doxastic justification. So either the objection rests on a false premise, or it has already been taken care of.}

\objrep{If Coraline is justified in believing \(p\), then Coraline can use \(p\) as a premise in practical reasoning. If Coraline can use \(p\) as a premise in practical reasoning, and \(p\) is true, and her belief in \(p\) is not Gettiered, then she knows \(p\). By hypothesis, her belief is true, and her belief is not Gettiered. So she should know \(p\). But she doesn't know \(p\). So by several steps of modus tollens, she isn't justified in believing \(p\).\footnote{Compare the `subtraction argument' on page 99 of \cite{FantlMcGrath2009}.}}
{Like the previous objection, this one turns on an equivocation, this time over the neologism `Gettiered'. Some epistemologists use this to simply mean that a belief is justified and true without constituting knowledge. By that standard, the third sentence is false. Or, at least, we haven't been given any reason to think that it is true. Given everything else that's said, the third sentence is a raw assertion that Coraline knows that \(p\), and I don't think we should accept that.

The other way epistemologists sometimes use the term is to pick out justified true beliefs that fail to be knowledge for the reasons that the beliefs in the original examples from \cite{Gettier1963} fail to be knowledge. That is, it picks out a property that beliefs have when they are derived from a false lemma, or whatever similar property is held to be doing the work in the original Gettier examples. Now on this reading, Coraline's belief that \(p\) is not Gettiered. But it doesn't follow that it is known. There's no reason, once we've given up on the JTB theory of knowledge, to think that whatever goes wrong in Gettier's examples is the \textit{only} way for a justified true belief to fall short of knowledge. It could be that there's a practical defeater, as in this case. So the second sentence of the objection is false, and the objection again fails.

But note I'm conceding to the objector that Coraline does not know $p$. That's important, because it reveals another way in which knowledge is interest-relative. We can see that Coraline does not know that $p$ by noting that if she did know $p$, we could represent her decision like this:

\begin{center}
\begin{tabular}{r c c}
 & \textbf{\(q\)} & \textbf{\( \neg q\)}  \\
\textbf{Take bet} & \$100 & \$1  \\
\textbf{Decline bet} & 0 & 0  \\
\end{tabular}
\end{center}

\noindent And now she should clearly take the bet, since it is a dominating option. Her irrational credence in $q$ simply wouldn't matter. But she can't rationally take the bet, so this table must be wrong, so she doesn't know $p$.

This seems odd. Why should an irrational credence in $q$ defeat knowledge of $p$? The answer is that beliefs have to sufficiently cohere with our other beliefs to be knowledge. If I have a very firm belief that $\neg p$, and also a belief that $p$, the latter belief cannot be knowledge even if it is grounded in the facts in just the right way. The contradictory belief that $\neg p$ is a doxastic defeater.

Now in practice almost all of our beliefs are in some tension with some other subset of our beliefs. Unless we are perfectly coherent, which we never are, there will be lots of ways to argue that any particular belief does not sufficiently cohere with our broader doxastic state, and hence is defeated from being knowledge. There must be some restrictions on when incoherence with some part of the rest of one's beliefs defeats knowledge.

I think an interest-relative story here is most likely to succeed. If we said Coraline knew that $p$, that would produce a tension between the fact that she does (rationally) believe that taking the bet is best given $p$, and that she should believe that declining the bet is not best \textit{simpliciter}. And that matters because whether to take the bet or not is a live decision for her. That is, incoherencies or irrationalities that manifest in decision problems that are salient can defeat knowledge.

Put another way, an agent only knows $p$ if we can properly model any decision she's really facing with a decision table where $p$ is taken as given.\footnote{Or, at least, we can model any such decision where standard decision theory applies. Perhaps we'll have to exclude cases where one of the options violates a deontological constraint. As noted in section 1, the argument for interest-relativity of knowledge is neutral on how to handle such cases.} Crediting Coraline with knowing $p$ will mean we get the table for this decision, i.e., whether to take this very bet, wrong. And that's a live decision for her. Those two facts together entail that she doesn't know $p$. Neither alone would suffice for defeating knowledge of $p$. And that's a part of the explanation for the interest-relativity of knowledge, a part I left out of the story in my \citeyearpar{Weatherson2005-WEACWD}.
}