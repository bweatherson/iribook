\subsection{Two Caveats}
The theory sketched so far seems to me right in the vast majority of cases. It fits in well with a broadly functionalist view of the mind, and it handles difficult cases, like that of Kate, nicely. But it needs to be supplemented and clarified a little to handle some other difficult cases. In this section I'm going to supplement the theory a little to handle what I call `impractical propositions', and say a little about morally loaded action.

Jones has a false geographic belief: he believes that Los Angeles is west of Reno, Nevada.\footnote{I'm borrowing this example from Fred Dretske, who uses it to make some interesting points about dispositional belief.} This isn't because he's ever thought about the question. Rather, he's just disposed to say ``Of course'' if someone asks, ``Is Los Angeles west of Reno?'' That disposition has never been triggered, because no one's ever bothered to ask him this. Call the proposition that Los Angeles is west of Reno \(p\). 

The theory given so far will get the right result here: Jones does believe that \(p\). But it gets the right answer for an odd reason. Jones, it turns out, has very little interest in American geography right now. He's a schoolboy in St Andrews, Scotland, getting ready for school and worried about missing his schoolbus. There's no inquiry he's currently engaged in for which \(p\) is even close to relevant. So conditionalising on \(p\) doesn't change the answer to any inquiry he's engaged in, but that would be true no matter what his credence in \(p\) is.

There's an immediate problem here. Jones believes \(p\), since conditionalising on \(p\) doesn't change the answer to any relevant inquiry. But for the very same reason, conditionalising on \(\neg p\) doesn't change the answer to any relevant inquiry. It seems our theory has the bizarre result that Jones believes \(\neg p\) as well. That is both wrong and unfair. We end up attributing inconsistent beliefs to Jones simply because he's a harried schoolboy who isn't currently concerned with the finer points of the geography of the American southwest.

Here's a way out of this problem in four relatively easy steps. First, we say that which questions are relevant questions is not just relative to the agent's interests, but also relevant to the proposition being considered. A question may be relevant relative to \(p\), but not relative to \(q\). Second, we say that relative to \(p\), the question of whether to believe \(p\) is a relevant question. Third, we say that an agent only prefers believing \(p\) to not believing it if their credence in \(p\) is greater than their credence in \(\neg p\), i.e., if their credence in \(p\) is greater than \(\nicefrac{1}{2}\). Finally, we say that when the issue is whether the subject believes that \(p\), the question of whether to believe \(p\) is not just a relvant question on its own, but it stays being a relevant question conditional on any \(q\) that is relevant to the subject. In the earlier paper \citep{Weatherson2005-WEACWD} I argue that this solves the problem raised by impractical propositions in a smooth and principled way.

That's the first caveat. The second is one that isn't discussed in the earlier paper. If the agent is merely trying to get the best outcome for themselves, then it makes sense to represent them as a utility maximiser. And within orthodox decision theory, it is easy enough to talk about, and reason about, conditional utilities. That's important, because conditional utilities play an important role in the theory of belief offered here. But if the agent faces moral constraints on her decision, it isn't always so easy to think about conditional utilities.

When agents have to make decisions that might involve them causing harm to others if certain propositions turn out to be true, then I think it is best to supplement orthodox decision theory with an extra assumption. The assumption is, roughly, that for choices that may harm others, expected value is absolute value. It's easiest to see what this means using a simple case of three-way choice. The kind of example I'm considering here has been used for (slightly) different purposes by Frank \cite{Jackson1991}. 

The agent has to do \(\varphi\) or \(\psi\). Failure to do either of these will lead to disaster, and is clearly unacceptable. Either \(\varphi\) or \(\psi\) will avert the disaster, but one of them will be moderately harmful and the other one will not. The agent has time before the disaster to find out which of \(\varphi\) and \(\psi\) is harmful and which is not for a nominal cost. Right now, her credence that \(\varphi\) is the harmful one is, quite reasonably, \(\nicefrac{1}{2}\). So the agent has three choices:

\begin{itemize*}
\item Do \(\varphi\);
\item Do \(\psi\); or
\item Wait and find out which one is not harmful, and do it.
\end{itemize*}

\noindent We'll assume that other choices, like letting the disaster happen, or finding out which one is harmful and doing it, are simply out of consideration. In any case, they are clearly dominated options, so the agent shouldn't do them. Let \(p\) be the propostion that \(\varphi\) is the harmful one. Then if we assume the harm in question has a disutility of 10, and the disutility of waiting to act until we know which is the harmful one is 1, the values of the possible outcomes are as follows:

\begin{center}
\begin{tabular}{r c c}
 & \(p\) & \(\neg p\) \\
\textbf{Do \(\varphi\)} & -10 & 0 \\
\textbf{Do \(\psi\)} & 0 & -10 \\
\textbf{Find out which is harmful} & -1 & -1 \\
\end{tabular}
\end{center}

\noindent Given that \(Pr(p) = \nicefrac{1}{2}\), it's easy to compute that the expected value of doing either \(\varphi\) or \(\psi\) is -5, while the expected value of finding out which is harmful is -1, so the agent should find out which thing is to be done before acting. So far most consequentialists would agree, and so probably would most non-consequentialists for most ways of fleshing out the abstract example I've described.\footnote{Some consequentialists say that what the agent should do depends on whether \(p\) is true. If \(p\) is true, she should do \(\psi\), and if \(p\) is false she should do \(\varphi\). As we'll see, I have reasons for thinking this is rather radically wrong.}

But most consequentialists would also say something else about the example that I think is not exactly true. Just focus on the column in the table above where \(p\) is true. In that column, the highest value, 0, is alongside the action \textit{Do} \(\psi\). So you might think that conditional on \(p\), the agent should do \(\psi\). That is, you might think the conditional expected value of doing \(\psi\), conditional on \(p\) being true, is 0, and that's higher than the conditional expected value of any other act, conditional on \(p\). If you thought that, you'd certainly be in agreement with the orthodox decision-theoretic treatment of this problem.

In the abstract statement of the situation above, I said that one of the options would be \textit{harmful}, but I didn't say who it would be harmful to. I think this matters. I think what I called the orthodox treatment of the situation is correct when the harm accrues to the person making the decision. But when the harm accrues to another person, particularly when it accrues to a person that the agent has a duty of care towards, then I think the orthodox treatment isn't quite right.

My reasons for this go back to Jackson's original discussion of the puzzle. Let the agent be a doctor, the actions \(\varphi\) and \(\psi\) be her prescribing different medication to a patient, and the harm a severe allergic reaction that the patient will have to one of the medications. Assume that she can run a test that will tell her which medication the patient is allergic to, but the test will take a day. Assume that the patient will die in a month without either medication; that's the disaster that must be averted. And assume that the patient is is some discomfort that either medication would relieve; that's the small cost of finding out which medication is risk. Assume finally that there is no chance the patient will die in the day it takes to run the test, so the cost of running the test is really nominal.

A good doctor in that situation will find out which medication the patient is allergic to before ascribing either medicine. It would be \textit{reckless} to ascribe a medicine that is unnecessary and that the patient might be allergic to. It is worse than reckless if the patient is actually allergic to the medicine prescribed, and the doctor harms the patient. But even if she's lucky and prescribes the `right' medication, the recklessness remains. It was still, it seems, the wrong thing for her to do.

All of that is in Jackson's discussion of the case, though I'm not sure he'd agree with the way I'm about the incorporate these ideas into the formal decision theory. Even under the assumption that \(p\), prescribing \(\psi\) is still wrong, because it is reckless. That should be incorporated into the values we ascribe to different actions in different circumstances. The way I do it is to associate the value of each action, in each circumstance, with its actual expected value. So the decision table for the doctor's decision looks something like this.

\begin{center}
\begin{tabular}{r c c}
 & \(p\) & \(\neg p\) \\
\textbf{Do \(\varphi\)} & -5 & -5 \\
\textbf{Do \(\psi\)} & -5 & -5 \\
\textbf{Find out which is harmful} & -1 & -1 \\
\end{tabular}
\end{center}

\noindent In fact, the doctor is making a decision under certainty. She knows that the value of prescribing either medicine is -5, and the value of running the tests is -1, so she should run the tests.

In general, when an agent has a duty to maximise the expected value of some quantity \(Q\), then the value that goes into the agent's decision table in a cell is \textit{not} the value of \(Q\) in the world-action pair the agent represents. Rather, it's the expected value of \(Q\) given that world-action pair. In situations like this one where the relevant facts (e.g., which medicine the patient is allergic to) don't affect the evidence the agent has, the decision is a decision under \textit{certainty}. This is all as things should be. When you have obligations that are drawn in terms of the expected value of a variable, the actual values of that variable cease to be directly relevant to the decision problem.

One upshot of these considerations is that when moral and epistemic considerations get entangled, for example when agents have a moral duty not to take certain kinds of risks, it can get tricky to apply the theory of belief developed here. In a separate paper (``Defending Interest-Relative Invariantism'') I've shown how this idea can help respond to some criticisms of similar views raised by Jessica \cite{Brown2008-BROKAP}.