\subsection{Two Caveats}
The theory sketched so far seems to me right in the vast majority of cases. It fits in well with a broadly functionalist view of the mind, and it handles difficult cases, like that of Charlie, nicely. But it needs to be supplemented and clarified a little to handle some other difficult cases. In this section I'm going to supplement the theory a little to handle what I call `impractical propositions', and say a little about morally loaded action.

Jones has a false geographic belief: he believes that Los Angeles is west of Reno, Nevada.\footnote{I'm borrowing this example from Fred Dretske, who uses it to make some interesting points about dispositional belief.} This isn't because he's ever thought about the question. Rather, he's just disposed to say ``Of course'' if someone asks, ``Is Los Angeles west of Reno?'' That disposition has never been triggered, because no one's ever bothered to ask him this. Call the proposition that Los Angeles is west of Reno \(p\). 

The theory given so far will get the right result here: Jones does believe that \(p\). But it gets the right answer for an odd reason. Jones, it turns out, has very little interest in American geography right now. He's a schoolboy in St Andrews, Scotland, getting ready for school and worried about missing his schoolbus. There's no inquiry he's currently engaged in for which \(p\) is even close to relevant. So conditionalising on \(p\) doesn't change the answer to any inquiry he's engaged in, but that would be true no matter what his credence in \(p\) is.

There's an immediate problem here. Jones believes \(p\), since conditionalising on \(p\) doesn't change the answer to any relevant inquiry. But for the very same reason, conditionalising on \(\neg p\) doesn't change the answer to any relevant inquiry. It seems our theory has the bizarre result that Jones believes \(\neg p\) as well. That is both wrong and unfair. We end up attributing inconsistent beliefs to Jones simply because he's a harried schoolboy who isn't currently concerned with the finer points of geography of the American southwest.

Here's a way out of this problem in four relatively easy steps.\footenote{The recipe here is similar to that given in \cite{Weatherson2005-WEACWD}, but the motivation is streamlined. Thanks to Jacob Ross for helpful suggestions here.} First, we say that which questions are relevant questions is not just relative to the agent's interests, but also relevant to the proposition being considered. A question may be relevant relative to \(p\), but not relative to \(q\). Second, we say that relative to \(p\), the question of whether $p$ is more probable than $\neg p$ is a relevant question. Third, we infer from that that an agent only believes $p$ if their credence in \(p\) is greater than their credence in \(\neg p\), i.e., if their credence in \(p\) is greater than \(\nicefrac{1}{2}\). Finally, we say that when the issue is whether the subject believes that \(p\), the question of whether $p$ is more probable than $\neg p$ is not only relevant on its own, but it stays being a relevant question conditional on any \(q\) that is relevant to the subject. In the earlier paper \citep{Weatherson2005-WEACWD} I argue that this solves the problem raised by impractical propositions in a smooth and principled way.

That's the first caveat. The second is one that isn't discussed in the earlier paper. If the agent is merely trying to get the best outcome for themselves, then it makes sense to represent them as a utility maximiser. And within orthodox decision theory, it is easy enough to talk about, and reason about, conditional utilities. That's important, because conditional utilities play an important role in the theory of belief offered at the start of this section. But if the agent faces moral constraints on her decision, it isn't always so easy to think about conditional utilities.

When agents have to make decisions that might involve them causing harm to others if certain propositions turn out to be true, then I think it is best to supplement orthodox decision theory with an extra assumption. The assumption is, roughly, that for choices that may harm others, expected value is absolute value. It's easiest to see what this means using a simple case of three-way choice. The kind of example I'm considering here has been used for (slightly) different purposes by Frank \cite{Jackson1991}. 

The agent has to do \(\varphi\) or \(\psi\). Failure to do either of these will lead to disaster, and is clearly unacceptable. Either \(\varphi\) or \(\psi\) will avert the disaster, but one of them will be moderately harmful and the other one will not. The agent has time before the disaster to find out which of \(\varphi\) and \(\psi\) is harmful and which is not for a nominal cost. Right now, her credence that \(\varphi\) is the harmful one is, quite reasonably, \(\nicefrac{1}{2}\). So the agent has three choices:

\begin{itemize*}
\item Do \(\varphi\);
\item Do \(\psi\); or
\item Wait and find out which one is not harmful, and do it.
\end{itemize*}

\noindent We'll assume that other choices, like letting the disaster happen, or finding out which one is harmful and doing it, are simply out of consideration. In any case, they are clearly dominated options, so the agent shouldn't do them. Let \(p\) be the propostion that \(\varphi\) is the harmful one. Then if we assume the harm in question has a disutility of 10, and the disutility of waiting to act until we know which is the harmful one is 1, the values of the possible outcomes are as follows:

\begin{center}
\begin{tabular}{r c c}
 & \(p\) & \(\neg p\) \\
\textbf{Do \(\varphi\)} & -10 & 0 \\
\textbf{Do \(\psi\)} & 0 & -10 \\
Find out which is harmful & -1 & -1 \\
\end{tabular}
\end{center}

\noindent Given that \(Pr(p) = \nicefrac{1}{2}\), it's easy to compute that the expected value of doing either \(\varphi\) or \(\psi\) is -5, while the expected value of finding out which is harmful is -1, so the agent should find out which thing is to be done before acting. So far most consequentialists would agree, and so probably would most non-consequentialists for most ways of fleshing out the abstract example I've described.\footnote{Some consequentialists say that what the agent should do depends on whether \(p\) is true. If \(p\) is true, she should do \(\psi\), and if \(p\) is false she should do \(\varphi\). As we'll see, I have reasons for thinking this is rather radically wrong.}

But most consequentialists would also say something else about the example that I think is not exactly true. Just focus on the column in the table above where \(p\) is true. In that column, the highest value, 0, is alongside the action \textit{Do} \(\psi\). So you might think that conditional on \(p\), the agent should do \(\psi\). That is, you might think the conditional expected value of doing \(\psi\), conditional on \(p\) being true, is 0, and that's higher than the conditional expected value of any other act, conditional on \(p\). If you thought that, you'd certainly be in agreement with the orthodox decision-theoretic treatment of this problem.

In the abstract statement of the situation above, I said that one of the options would be \textit{harmful}, but I didn't say who it would be harmful to. I think this matters. I think what I called the orthodox treatment of the situation is correct when the harm accrues to the person making the decision. But when the harm accrues to another person, particularly when it accrues to a person that the agent has a duty of care towards, then I think the orthodox treatment isn't quite right.

My reasons for this go back to Jackson's original discussion of the puzzle. Let the agent be a doctor, the actions \(\varphi\) and \(\psi\) be her prescribing different medication to a patient, and the harm a severe allergic reaction that the patient will have to one of the medications. Assume that she can run a test that will tell her which medication the patient is allergic to, but the test will take a day. Assume that the patient will die in a month without either medication; that's the disaster that must be averted. And assume that the patient is is some discomfort that either medication would relieve; that's the small cost of finding out which medication is risk. Assume finally that there is no chance the patient will die in the day it takes to run the test, so the cost of running the test is really nominal.

A good doctor in that situation will find out which medication the patient is allergic to before ascribing either medicine. It would be \textit{reckless} to ascribe a medicine that is unnecessary and that the patient might be allergic to. It is worse than reckless if the patient is actually allergic to the medicine prescribed, and the doctor harms the patient. But even if she's lucky and prescribes the `right' medication, the recklessness remains. It was still, it seems, the wrong thing for her to do.

All of that is in Jackson's discussion of the case, though I'm not sure he'd agree with the way I'm about the incorporate these ideas into the formal decision theory. Even under the assumption that \(p\), prescribing \(\psi\) is still wrong, because it is reckless. That should be incorporated into the values we ascribe to different actions in different circumstances. The way I do it is to associate the value of each action, in each circumstance, with its actual expected value. So the decision table for the doctor's decision looks something like this.

\begin{center}
\begin{tabular}{r c c}
 & \(p\) & \(\neg p\) \\
\textbf{Do \(\varphi\)} & -5 & -5 \\
\textbf{Do \(\psi\)} & -5 & -5 \\
Find out which is harmful & -1 & -1 \\
\end{tabular}
\end{center}

\noindent In fact, the doctor is making a decision under certainty. She knows that the value of prescribing either medicine is -5, and the value of running the tests is -1, so she should run the tests.

In general, when an agent has a duty to maximise the expected value of some quantity \(Q\), then the value that goes into the agent's decision table in a cell is \textit{not} the value of \(Q\) in the world-action pair the agent represents. Rather, it's the expected value of \(Q\) given that world-action pair. In situations like this one where the relevant facts (e.g., which medicine the patient is allergic to) don't affect the evidence the agent has, the decision is a decision under \textit{certainty}. This is all as things should be. When you have obligations that are drawn in terms of the expected value of a variable, the actual values of that variable cease to be directly relevant to the decision problem.

Similar morals carry across to theories that offer a smaller role to expected utility in determining moral value. In particular, it's often true that decisions where it is uncertain what course of action will produce the best outcome might still, in the morally salient sense, be decisions under certainty. That's because the uncertainty doesn't impact how we should weight the different possible outcomes, as in orthodox utility theory, but how we should value them. That's roughly what I think is going on in cases like this one, which Jessica Brown has argued are problematic for the epistemological theories John Hawthorne and Jason Stanley have recently been defending.\footnote{The target here is not directly the interest-relativity of their theories, but more general principles about the role of knowledge in action and assertion. Since my theories are close enough, at least in consequences, to Hawthorne and Stanley's, it is important to note how my theory handles the case.}

\begin{quote}
A student is spending the day shadowing a surgeon. In the morning he observes her in clinic examining patient A who has a diseased left kidney. The decision is taken to remove it that afternoon. Later, the student observes the surgeon in theatre where patient A is lying anaesthetised on the operating table. The operation hasn't started as the surgeon is consulting the patient's notes. The student is puzzled and asks one of the nurses what's going on: 

\textbf{Student}: I don't understand. Why is she looking at the patient's records? She was in clinic with the patient this morning. Doesn't she even know which kidney it is? 

\textbf{Nurse}: Of course, she knows which kidney it is. But, imagine what it would be like if she removed the wrong kidney. She shouldn't operate before checking the patient's records. \citep[1144-1145]{Brown2008-BROKAP}
\end{quote}

\noindent It is tempting, but for reasons I've been going through here mistaken, to represent the surgeon's choice as follows. Let \textbf{Left} mean the left kidney is diseased, and \textbf{Right} mean the right kidney is diseased.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{Left} & \textbf{Right} \\
\textbf{Remove left kidney} & \(1\) & \(-1\) \\
\textbf{Remove right kidney} & \(-1\) & \(1\) \\
\textbf{Check notes} & \(1-\varepsilon\) & \(1-\varepsilon\) \\
\end{tabular}
\end{center}

\noindent Here \(\varepsilon\) is the trivial but non-zero cost of checking the chart. Given this table, we might reason that since the surgeon knows that she's in the left column, and removing the left kidney is the best option in that column, she should remove the left kidney rather than checking the notes.

But that reasoning assumes that the surgeon does not have any epistemic obligations over and above her duty to maximise expected utility. And that's very implausible. It's totally implausible on a non-consequentialist moral theory. A non-consequentialist may think that some people have just the same obligations that the consequentialist says they have -- legislators are frequently mentioned as an example -- but surely they wouldn't think \textit{surgeons} are in this category. And even a consequentialist who thinks that surgeons have special obligations in terms of their institutional role should think that the surgeon's obligations go above and beyond the obligation every agent has to maximise expected utility.

It's not clear exactly what the obligation the surgeon has. Perhaps it is an obligation to not just know which kidney to remove, but to know this on the basis of evidence she has obtained while in the operating theatre. Or perhaps it is an obligation to make her belief about which kidney to remove as sensitive as possible to various possible scenarios. Before she checked the chart, this counterfactual was false: \textit{Had she misremembered which kidney was to be removed, she would have a true belief about which kidney was to be removed.} Checking the chart makes that counterfactual true, and so makes her belief that the left kidney is to be removed a little more sensitive to counterfactual possibilities. 

However we spell out the obligation, it is plausible given what the nurse says that the surgeon has some such obligation. And it is plausible that the `cost' of violating this obligation, call it \(\delta\) is greater than the cost of checking the notes. So here is the decision table the surgeon faces.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{Left} & \textbf{Right} \\
\textbf{Remove left kidney} & \(1-\delta\) & \(-1-\delta\) \\
\textbf{Remove right kidney} & \(-1-\delta\) & \(1-\delta\) \\
\textbf{Check notes} & \(1-\varepsilon\) & \(1-\varepsilon\) \\
\end{tabular}
\end{center}

\noindent And it isn't surprising, or a problem for an interest-relative theory of knowledge or belief, that the surgeon should check the notes, even if she believes \textit{and knows} that the left kidney is the diseased one.