\subsection{Interests and Functional Roles}
The previous section was largely devoted to proving an existential claim: there is \textit{some} interest-relativity to knowledge. Or, if you prefer, it proved a negative claim: the best theory of knowledge is \textit{not} interest-neutral. But this negative conclusion invites a philosophical challenge: what is the best explanation of the interest-relativity of knowledge? My answer is in two parts. Part of the interest-relativity of knowledge comes from the interest-relativity of belief, and part of it comes from the fact that interests generate certain kinds of doxastic defeaters. It's the second part, the part that is new to this paper, that makes the theory a version of non-doxastic IRI.

Here's my theory of belief. $S$ believes that $p$ iff conditionalising on $p$ doesn't change $S$'s answer to any relevant question. I'm using `relevance' here in a non-technical sense; I say a lot more about how to cash out the notion in my \citeyearpar{Weatherson2005-WEACWD}. The key thing to note is that relevance is interest-relative, so the theory of belief is interest-relative. There is a bit more to say about what kind of \textit{questions} are important for this definition of belief. In part because I've changed my mind a little bit on this since the earlier paper, I'll spend a bit more time on it. The following four kinds of questions are the most important.

\begin{itemize*}
\item How probable is $q$?
\item Is $q$ or $r$ more probable?
\item How good an idea is it to do $\phi$?
\item Is it better to do $\phi$ or $\psi$?
\end{itemize*}

\noindent The theory of belief says that someone who believes that $p$doesn't change their answer to any of these questions upon conditionalising on $p$. Putting this formally, and making the restriction to relevant questions explicit, we get the following theorems of our theory of belief.\footnote{In the last two lines, I use $U(\phi)$ to denote the expected utility of $\phi$, and $U(\phi | p)$ to denote the expected utility of $\phi$ conditional on $p$. It's often easier to write this as simply $U(\phi \wedge p)$, since the utility of $\phi$ conditional on $p$ just is the utility of doing $\phi$ in a world where $p$ is true. That is, it is the utility of $\phi \wedge p$ being realised. But we get a nicer symmetry between the probabilistic principles and the utility principles if we use the explictly conditional notation for each.}

\begin{description*}
\item[BAP] For all relevant $q, x$, if $p$ is believed then $\Pr(q) = x$ iff $\Pr(q | p) = x$.
\item[BCP] For all relevant $q, r$, if $p$ is believed then $\Pr(q) \geq \Pr(r)$ iff $\Pr(q | p) \geq \Pr(r | p)$.
\item[BAU] For all relevant $\phi, x$, if $p$ is believed then $U(\phi) = x$ iff $U(\phi | p) = x$.
\item[BCU] For all relevant $\phi, \psi$, if $p$ is believed then $U(\phi) \geq U(\psi)$ iff $U(\phi | p) \geq U(\psi | p)$.
\end{description*}

\noindent In the earlier paper I focussed on \textbf{BAU} and \textbf{BCU}. But \textbf{BAP} and \textbf{BCP} are important as well. Indeed, focussing on them lets us derive a nice result. 

Charlie is trying to figure out exactly what the probability of $p$ is. That is, for any $x \in [0, 1]$, whether $\Pr(p) = x$ is a relevant question. Now Charlie is well aware that $\Pr(p | p) = 1$. So unless $\Pr(p) = 1$, Charlie will give a different answer to the questions \textit{How probable is p?} and \textit{Given p, how probable is p?}. So unless Charlie holds that $\Pr(p)$ is 1, she won't count as believing that $p$. One consequence of this is that Charlie can't reason, ``The probability of $p$ is exactly 0.978, so $p$.'' That's all to the good, since that looks like bad reasoning. And it looks like bad reasoning even though in some circumstances Charlie can rationally believe propositions that she (rationally) gives credence 0.978 to. Indeed, in some circumstances she can rationally believe something \textit{in virtue} of it being 0.978 probable.

That's because the reasoning in the previous paragraph assumes that every question of the form \textit{Is the probability of p equal to x?} is relevant. In practice, fewer questions than that will be relevant. Let's say that the only questions relevant to Charlie are of the form \textit{What is the probability of $p$ to one decimal place?}. And assume that no other questions become relevant in the course of her inquiry into this question.\footnote{This is probably somewhat unrealistic. It's hard to think about whether $\Pr(p)$ is closer to 0.7 or 0.8 without raising to salience questions about, for example, what the second decimal place in $\Pr(p)$ is. This is worth bearing in mind when coming up with intuitions about the cases in this paragraph.} Charlie decides that to the first decimal place, $\Pr(p) = 1.0$, i.e., $\Pr(p) > 0.95$. That is compatible with simply believing that $p$. And that seems right; if for practical purposes, the probability of $p$ is indistinguishable from 1, then the agent is confident enough in $p$ to believe it.

So there are some nice features of this theory of belief. Indeed, there are several reasons to believe it. It is, I have argued, the best functionalist account of belief. I'm not going to argue for functionalism about the mind, since the argument would take at least a book. (The book in question might look a lot like \cite{DBMJackson2007}.) But I do think functionalism is true, and so the best functionalist theory of belief is the best theory of belief.

The argument for this theory of belief in my  \citeyearpar{Weatherson2005-WEACWD} rested heavily on the flaws of rival theories. We can see those flaws by looking at a tension that any theory of the relationship between belief and credence must overcome. Each of the following three principles seems to be plausible.

\begin{enumerate*}
\item If $S$ has a greater credence in $p$ than in $q$, and she believes $q$, then she believes $p$ as well; and if her credences in both $p$ and $q$ are rational, and her belief in $q$ is rational, then so is her belief in $p$.
\item If $S$ rationally believes $p$ and rationally believes $q$, then it is open to her to rationally believe $p \wedge q$ without changing her credences.
\item $S$ can rationally believe $p$ while having credence of less than 1 in $p$.
\end{enumerate*}

\noindent But these three principles, together with some principles that are genuinely uncontroversial, entail an absurd result. By 3, there is some $p$ such that \textit{Cr}$(p) = x < 1$, and $p$ is believed. (\textit{Cr} is the function from any proposition to our agent's credence in that propositions.) Let $S$ know that a particular fair lottery has $l$ tickets, where $l > \nicefrac{1}{1-x}$. The uncontroversial principle we'll use is that in such a case $S$'s credence that any given ticket will lose should be $\nicefrac{l-1}{l}$. Since $\nicefrac{l-1}{l} > x$, it follows by 1 that $S$ believes of each ticket that it will lose. Since her credences are rational, these beliefs are rational. By repeated applications of 2 then, the agent can rationally believe that each ticket will lose. But she rationally gives credence 0 to the proposition that each ticket will lose. So by 1 she can rationally believe any proposition in which her credence is greater than 0. This is absurd.\footnote{See \cite{Sturgeon2008-STURAT} for discussion of a similar puzzle for anyone trying to tell a unified story of belief and credence.}

I won't repeat all the gory details here, but one of the consequences of the discussion in \cite{Weatherson2005-WEACWD} was that we could hold on to 3, and onto restricted versions of 1 and 2. In particular, if we restricted 1 and 2 to relevant propositions (in some sense) they became true, although the unrestricted version is false. A key part of the argument of the earlier paper was that this was a better option than the more commonly taken option of holding on to unrestricted versions of 1 and 3, at the cost of abandoning 2 even in clear cases. But one might wonder why I'm holding so tightly on to 3. After all, there is a functionalist argument that 3 is false.

A key functional role of credences is that if an agent has credence $x$ in $p$ she should be prepared to buy a bet that returns 1 util if $p$, and 0 utils otherwise, iff the price is no greater than $p$ utils. A key functional role of belief is that if an agent believes $p$, and recognises that $\phi$ is the best thing to do given $p$, then she'll do $\phi$. Given $p$, it's worth paying any price up to 1 util for a bet that pays 1 util if $p$. So believing $p$ seems to mean being in a functional state that is like having credence 1 in $p$.

But this argument isn't quite right. If we spell out more carefully what the functional roles of credence and belief are, a loophole emerges in the argument that belief implies credence 1. The interest-relative theory of belief turns out to exploit that loophole. What's the difference, in functional terms, between having credence $x$ in $p$, and having credence $x + \varepsilon$ in $p$? Well, think again about the bet that pays 1 util if $p$, and 0 utils otherwise. And imagine that bet is offered for $x + \nicefrac{\varepsilon}{2}$ utils. The person whose credence is $x$ will decline the offer; the person whose credence is $x + \varepsilon$ will accept it. Now it will usually be that no such bet is on offer.\footnote{There are exceptions, especially in cases where $p$ concerns something significant to financial markets, and the agent trades financial products. If you work through the theory that I'm about to lay out, one consequence is that such agents should have very few unconditional beliefs about financially-sensitive information, just higher and lower credences. I think that's actually quite a nice outcome, but I'm not going to rely on that in the argument for the view.} No matter; as long as one agent is \textit{disposed} to accept the offer, and the other agent is not, that suffices for a difference in credence.

The upshot of that is that differences in credences might be, indeed usually will be, constituted by differences in dispositions concerning how to act in choice situations far removed from actuality. I'm not usually in a position of having to accept or decline a chance to buy a bet for 0.9932 utils that the local coffee shop is currently open. Yet whether I would accept or decline such a bet matters to whether my credence that the coffee shop is open is 0.9931 or 0.9933. This isn't a problem with the standard picture of how credences work. It's just an observation that the high level of detail embedded in the picture relies on taking the constituents of mental states to involve many dispositions.

One of the crucial features of the theory of belief I'm defending is that what an agent believes is in general \textit{insensitive} to such abtruse dispositions, although it is very sensitive to dispositions about practical matters. It's true that if I believe that $p$, and I'm rational enough, I'll act as if $p$ is true. Is it also true that if I believe $p$, I'm disposed to act as if $p$ is true no matter what choices are placed in front of me? The theory being defended here says no, and that seems plausible. As we say in the case of Barry and Beth, Barry can believe that $p$, but be disposed to \textit{lose that belief} rather than act on it if odd choices, like that presented by the genie, emerge.

%Quantifier domain variation
This suggests the key difference between belief and credence 1. For a rational agent, a credence of 1 in $p$ means that the agent is disposed to answer a wide range of questions the same way she would answer that question conditional on $p$. That follows from the fact that these four principles are trivial theorems of the orthodox theory of expected utility.\footnote{The presentation in this section, as in the earlier paper, assumes at least a weak form of consequentialism in the sense of \cite{Hammond1988}. This was arguably a weakness of the earlier paper. We'll return to the issue of what happens in cases where the agent doesn't, and perhaps shouldn't, maximise expected utility, at the end of the section.} 

\begin{description*}
\item[C1AP] For all $q, x$, if $\Pr(p) = 1$ then $\Pr(q) = x$ iff $\Pr(q | p) = x$.
\item[C1CP] For all $q, r$, if $\Pr(p) = 1$ then $\Pr(q) \geq \Pr(r)$ iff $\Pr(q | p) \geq \Pr(r | p)$.
\item[C1AU] For all $\phi, x$, if $\Pr(p) = 1$ then $U(\phi) = x$ iff $U(\phi | p) = x$.
\item[C1CP] For all $\phi, \psi$, if $\Pr(p) = 1$ then $U(\phi) \geq U(\psi)$ iff $U(\phi | p) \geq U(\psi | p)$.
\end{description*}

\noindent Those look a lot like the theorems of the theory of belief that we discussed above. But note that these claims are \textit{unrestricted}, whereas in the theory of belief, we restricted attention to relevant actions, propositions, utilities and probabilities. That turns out to be the difference between belief and credence 1. Since that difference is interest-relative, belief is interest-relative.

I used to think that that was all the interest-relativity we needed in epistemology. Now I don't, for reasons that I'll go through in section three. (Readers who care more about the theory of knowledge than the theory of belief may want to skip ahead to that section.) But first I want to clean up some loose ends in the acount of belief.
