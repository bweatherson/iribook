

\input{PaperParts/RevisedIntro}
%In my \citeyearpar{Weatherson2005-WEACWD}, I defended three claims. Roughly, they were:
%
%\begin{enumerate*}
%\renewcommand{\labelenumi}{(\arabic{enumi})}
%\item Whether a person believes that \(p\) depends on their interests, broadly construed.
%\item Whether a person knows that \(p\) depends on their interests, broadly construed.
%\item The explanation for why (2) is true is simply that (1) is true; we don't need to add any interest-relativity to epistemology that isn't already present in our theory of belief.
%\end{enumerate*}
%
%\noindent The focus in that paper was on justified belief, and in particular that the only reason justified belief is interest-relative is that belief is interest-relative. But I did strongly suggest that I thought the same was true of knowledge. In this paper I want to give a new argument for (2), and argue against (3). That is, I now think that some of the interest-relativity of knowledge cannot be explained by the interest-relativity of belief. This is largely because I'm now convinced by a version of the example that Jason \cite{Stanley2005-STAKAP} calls `Ignorant High Stakes'. It will be easier to see why this case is a problem for the package of views I earlier adopted by looking at the new argument for (2). In the earlier paper I argued that the best way to understand the relationship between belief and degree of belief was to see that belief was interest-relative.\footnote{Scott \cite{Sturgeon2008-STURAT} offers an excellent treatment of how puzzling this relationship is. But I think Sturgeon doesn't pay sufficient attention to the possibilities for interest-relative treatments of the relationship, and so is led to adopt a sub-optimal resolution of the puzzles he raises.} I'll start this paper by arguing that the best way to understand our orthodox practices in decision theory leads to an interest-relative theory of knowledge.

\section{The Interest-Relativity of Knowledge}

\subsection{The Struction of Decision Problems}

Professor Dec is teaching introductory decision theory to her undergraduate class. She is trying to introduce the notion of a dominant choice. So she introduces the following problem, with two states, \(S_1\) and \(S_2\), and two choices, \(C_1\) and \(C_2\), as is normal for introductory problems.

\begin{center}
\begin{tabular}{r c c}
 & \(S_1\) & \(S_2\) \\
\(C_1\) & -\$200 & \$1000 \\
\(C_2\) & -\$100 & \$1500 
\end{tabular}
\end{center}

\noindent She's hoping that the students will see that \(C_1\) and \(C_2\) are bets, but \(C_2\) is clearly the better bet. If \(S_1\) is actual, then both bets lose, but \(C_2\) loses less money. If \(S_2\) is actual, then both bets win, but \(C_2\) wins more. So \(C_2\) is better. That analysis is clearly wrong if the state is causally dependent on the choice, and controversial if the states are evidentially dependent on the choices. But Professor Dec has not given any reason for the students to think that the states are dependent on the choices in either way, and in fact the students don't worry about that kind of dependence.

That doesn't mean, however, that the students all adopt the analysis that Professor Dec wants them to. One student, Stu, is particularly unwilling to accept that \(C_2\) is better than \(C_1\). He thinks, on the basis of his experience, that when more than \$1000 is on the line, people aren't as reliable about paying out on bets. So while \(C_1\) is guaranteed to deliver \$1000 if \(S_2\), if the agent bets on \(C_2\), she might face some difficulty in collecting on her money.

Given the context, i.e., that they are in an undergraduate decision theory class, it seems that Stu has misunderstood the question that Professor Dec intended to ask. But it is a little harder than it first seems to specify just exactly what Stu's mistake is. It isn't that he thinks Professor Dec has \textit{misdescribed} the situation. It isn't that he thinks the agent won't collect \$1500 if she chooses \(C_2\) and is in \(S_2\). He just thinks that she \textit{might} not be able to collect it, so the expected payout might really be a little less than \$1500.

But Stu is not the only problem that Professor Dec has. She also has trouble convincing Dom of the argument. He thinks there should be a third state added, $S_3$. In $S_3$, there is a vengeful God who is about to end the world, and take everyone who chose $C_1$ to heaven, while sending everyone who chose $C_2$ to hell. Since heaven is better than hell, $C_2$ does not dominate $C_1$; it is worse in $S_3$. If decision theory is to be useful, we must say something about why we can leave states like $S_3$ off the decision table.

So in order to teach decision theory, Professor Dec has to answer two questions.\footnote{If we are convinced that the right decision is the one that maximises expected utility, there is a sense in which these questions collapse. For the expected utility theorist, we can solve Dom's question by making sure the states are logically exhaustive, and making the `payouts' in each state be expected payouts. But the theory that the correct decision is the one that maximises expected utility, while plausibly true, is controversial. It shouldn't be assumed when we are investigating the semantics of decision tables.}

\begin{enumerate*}
\item What makes it legitimate to write something on the decision table, such as the `\$1500' we write in the bottom right cell of Dec's table?
\item What makes it legitimate to leave something off a decision table, such as leaving Dom's state $S_3$ off the table?
\end{enumerate*}

\noindent Let's start with a simpler problem that helps with both questions. Alice is out of town on a holiday, and she faces the following decision choice concerning what to do with a token in her hand.

\begin{center}
\begin{tabular}{r c}
\textbf{Choice} & \textbf{Outcome} \\
Put token on table & Win \$1000 \\
Put token in pocket & Win nothing
\end{tabular}
\end{center}

\noindent This looks easy, especially if we've taken Professor Dec's class. Putting the token on the table dominates putting the token in her pocket. It returns \$1000, versus no gain. So she should put the token on the table.

I've left Alice's story fairly schematic; let's fill in some of the details. Alice is on holiday at a casino. It's a fair casino; the probabilities of the outcomes of each of the games is just what you'd expect. And Alice knows this. The table she's standing at is a roulette table. The token is a chip from the casino worth \$1000. Putting the token on the table means placing a bet. As it turns out, it means placing a bet on the roulette wheel landing on 28. If that bet wins she gets her token back and another token of the same value. There are many other bets she could make, but Alice has decided not to make all but one of them. Since her birthday is the 28\(^{\text{th}}\), she is tempted to put a bet on 28; that's the only bet she is considering. If she makes this bet, the objective chance of her winning is \(\nicefrac{1}{38}\), and she knows this. As a matter of fact she will win, but she doesn't know this. (This is why the description in the table I presented above is truthful, though frightfully misleading.) As you can see, the odds on this bet are terrible. She should have a chance of winning around \(\nicefrac{1}{2}\) to justify placing this bet.\footnote{Assuming Alice's utility curve for money curves downwards, she should be looking for a slightly higher chance of winning than \(\nicefrac{1}{2}\) to place the bet, but that level of detail isn't relevant to the story we're telling here.} So the above table, which makes it look like placing the bet is the dominant, and hence rational, option, is misleading.

Just how is the table misleading though? It isn't because what is says is false. If Alice puts the token on the table she wins \$1000; and if she doesn't, she stays where she is. It isn't, or isn't just, that Alice doesn't believe the table reflects what will happen if she places the bet. As it turns out, Alice is smart, so she doesn't form beliefs about chance events like roulette wheels. But even if she did, that wouldn't change how misleading the table is. The table suggests that it is rational for Alice to put the token on the table. In fact, that is irrational. And it would still be irrational if Alice believes, \textit{irrationally}, that the wheel will land on 28.

A better suggestion is that the table is misleading because Alice doesn't \textit{know} that it accurately depicts the choice she faced. If she did know that these were the outcomes to putting the token on the table versus in her pocket, it seems it would be rational for her to put it on the table. If we take it as tacit in a presentation of a decision problem that the agent knows that the table accurately depicts the outcomes of various choices in different states, then we can tell a plausible story about what the miscommunication between Professor Dec and her students was. Stu was assuming that if the agent wins \$1500, she might not be able to easily collect. That is, he was assuming that the agent does not know that she'll get \$1500 if she chooses \(C_2\) and is in state \(S_2\). Professor Dec, if she's anything like other decision theory professors, will have assumed that the agent did know exactly that. And the miscommunication between Professor Dec and Dom also concerns knowledge. When Dec wrote that table up, she was saying that the agent knew that $S_1$ or $S_2$ obtained. And when she says it is best to take dominating options, she means that it is best to take options that one knows to have better outcomes. So here are the answers to Stu and Dom's challenges.

\begin{enumerate*}
\item It is legitimate to write something on the decision table, such as the `\$1500' we write in the bottom right cell of Dec's table, iff the decision maker knows it to be true.
\item It is legitimate to leave something off a decision table, such as leaving Dom's state $S_3$ off the table, iff the decision maker knows it not to obtain.
\end{enumerate*}

\noindent Perhaps those answers are not correct, but what we can clearly see by reflecting on these cases is that the standard presentation of a decision problem presupposes not just that the table states what will happen, but the agent stands in some special doxastic relationship to the information explicitly on the table (such as that Alice will get \$1500 if $C_2$ and $S_2$) and implied by where the table ends (such as that $S_3$ will not happen). Could that relationship be weaker than knowledge? It's true that it is hard to come up with clear counterexamples to the suggestion that the relationship is merely justified true belief. But I think it is somewhat implausible to hold that the standard presentation of an example merely presupposes that the agent has a justified true belief that the table is correct, and does not in addition know that the table is correct.

My reasons for thinking this are similar to one of the reasons Timothy Williamson \citep[Ch. 9]{Williamson2000-WILKAI} gives for doubting that one's evidence is all that one justifiably truly believes. To put the point in Lewisian terms, it seems that knowledge is a much more \textit{natural} relation than justified true belief. And when ascribing contents, especially contents of tacitly held beliefs, we should strongly prefer to 
ascribe more rather than less natural contents.\footnote{I'm here retracting some things I said a few years ago in a paper on philosophical methodology \citep{Weatherson2003-WEAWGA}. There I argued that identifying knowledge with justified true belief would give us a theory on which knowledge was more natural than a theory on which we didn't identify knowledge with any other epistemic property. I now think that is wrong for a couple of reasons. First, although it's true (as I say in the earlier paper) that knowledge can't be primitive or perfectly natural, this doesn't make it less natural than justification, which is also far from a fundamental feature of reality. Indeed, given how usual it is for languages to have a simple representation of knowledge, we have some evidence that it is very natural for a term from a special science. Second, I think in the earlier paper I didn't fully appreciate the point (there attributed to Peter Klein) that the Gettier cases show that the property of being a justified true belief is not particularly natural. In general, when \(F\) and \(G\) are somewhat natural properties, then so is the property of being \(F \wedge G\). But there are exceptions, especially in cases where these are properties that a whole can have in virtue of a part having the property. In those cases, a whole that has an \(F\) part and a \(G\) part will be \(F \wedge G\), but this won't reflect any distinctive property of the whole. And one of the things the Gettier cases show is that the properties of \textit{being justified} and \textit{being true}, as applied to belief, fit this pattern. 

Note that even if you think that philosophers are generally too quick to move from instinctive reactions to the Gettier case to abandoning the justified true belief theory of knowledge, this point holds up. What is important here is that on sufficient reflection, the Gettier cases show that some justified true beliefs are not knowledge, and that the cases in question also show that being a justified true belief is not a particularly natural or unified property. So the point I've been making in the last this footnote is independent of the point I wanted to stress in ``What Good are Counterexamples?'', namely, that philosophers in some areas (especially epistemology) are insufficiently reformist in their attitude towards our intuitive reactions to cases.}

So the `special doxastic relationship' is not weaker than knowledge. Could it be stronger? Could it be, for example, that the relationship is certainty, or some kind of iterated knowledge? Plausibly in some game-theoretic settings it is stronger -- it involves not just knowing that the table is accurate, but knowing that the other player knows the table is accurate. In some cases, the standard treatment of games will require positing even more iterations of knowledge. For convenience, it is sometimes explicitly stated that iterations continue indefinitely, so each party knows the table is correct, and knows each party knows this, and knows each party knows that, and knows each party knows \textit{that}, and so on. An early example of this in philosophy is in the work by David \cite{Lewis1969a} on convention. But it is usually acknowledged (again in a tradition extending back at least to Lewis) that only the first few iterations are actually needed in any problem, and it seems a mistake to attribute more iterations than are actually used in deriving solutions to any particular game. 

The reason that would be a mistake is that we want game theory, and decision theory, to be applicable to real-life situations. There is very little that we know, and know that we know, and know we know we know, and so on indefinitely \citep[Ch. 4]{Williamson2000-WILKAI}. There is, perhaps, even less that we are certain of. If we only could say that a person is making a particular decision when they stand in these very strong relationships to the parameters of the decision table, then people will almost never be making the kinds of decision we study in decision theory. Since decision theory and game theory are not meant to be that impractical, I conclude that the `special doxastic relationship' cannot be that strong. It could be that in some games, the special relationship will involve a few iterations of knowledge, but in decision problems, where the epistemic states of others are irrelevant, even that is unnecessary, and simple knowledge seems sufficient.

It might be argued here that we shouldn't expect to apply decision theory directly to real-life problems, but only to idealised versions of them, so it would be acceptable to, for instance, require that the things we put in the table are, say, things that have probability exactly 1. In real life, virtually nothing has probability 1. In an idealisation, many things do. But to argue this way seems to involve using `idealisation' in an unnatural sense. There is a sense in which, whenever we treat something with non-maximal probability as simply given in a decision problem that we're ignoring, or abstracting away from, some complication. But we aren't \textit{idealising}. On the contrary, we're modelling the agent as if they were irrationally certain in some things which are merely very very probable. 

So it's better to say that any application of decision theory to a real-life problem will involve ignoring certain (counterfactual) logical or metaphysical possibilities in which the decision table is not actually true. But not any old abstraction will do. We can't ignore just anything, at least not if we want a good model. Which abstractions are acceptable? The response I've offered to Dom's challenge suggests an answer to this: we can abstract away from any possibility in which something the agent actually knows is false. I don't have a knock-down argument that this is the best of all possible abstractions, but nor do I know of any alternative answer to the question which abstractions are acceptable which is nearly as plausible.

We might be tempted to say that we can abstract away from anything such that the difference between its probability and 1 doesn't make a difference to the ultimate answer to the decision problem. More carefully, the idea would be that we can have the decision table represent that $p$ iff $p$ is true and treating $\Pr(p)$ as 1 rather than its actual value doesn't change what the agent should do. I think this is the most plausible story one could tell about decision tables if one didn't like the knowledge first story that I tell. But I also don't think it works, because of cases like the following.
Luc is lucky; he's in a casino where they are offering better than fair odds on roulette. Although the chance of winning any bet is \nicefrac{1}{38}, if Luc bets \$10, and his bet wins, he will win \$400. (That's the only bet on offer.) Luc, like Alice, is considering betting on 28. As it turns out, 28 won't come up, although since this is a fair roulette wheel, Luc doesn't know this. Luc, like most agents, has a declining marginal utility for money. He currently has \$1,000, and for any amount of money $x$, Luc gets utility $u(x) = x^{\nicefrac{1}{2}}$ out of having $x$. So Luc's current utility (from money) is, roughly, 31.622. If he bets and loses, his utility will be, roughly 31.464. And if he bets and wins, his utility will be, roughly, 37.417. So he stands to gain about 5.794, and to lose about 0.159. That is, his expected utility goes down if he takes the bet, so he shouldn't take it. Of course, if the probability of losing was 1, and not merely $\nicefrac{37}{38}$, he shouldn't take the bet too. Does that mean it is acceptable, in presenting Luc's decision problem, to leave off the table any possibility of him winning, since he won't win, and setting the probability of losing to 1 rather than $\nicefrac{37}{38}$ doesn't change the decision he should make? Of course not; that would horribly misstate the situation Luc finds himself in. It would misrepresent  how sensitive Luc's choice is to his utility function, and to the size of the stakes. If Luc's utility function was $u(x) = x^{\nicefrac{3}{4}}$, then he should take the bet. If his utility function is unchanged, but the bet was \$1 against \$40, rather than \$10 against \$400, he should take the bet. Leaving off the possibility of winning hides these facts, and terribly misrepresents Luc's situation.
I've argued that the states we can `leave off' a decision table are the states that the agent knows not to obtain. The argument is largely by elimination. If we can only leave off things that have probability 1, then decision theory would be useless; but it isn't. If we say we can leave off things if setting their probability at 1 is an accepable idealisation, we need a theory of acceptable idealisations. If this is to be a rival to my theory, the idealisation had better not be it's acceptable to treat anything known as having probability 1. But the most natural alternative idealisation badly misrepresents Luc's case. If we say that what can be left off is not what's known not to obtain, but what is, say, justifiably truly believed not to obtain, we need an argument for why people would naturally use such an unnatural standard. This doesn't even purport to be a conclusive argument, but these considerations point me towards thinking that knowledge determines what we can leave off.

I also cheated a little in making this argument. When I described Alice in the casino, I made a few explicit comments about her information states. And every time, I said that she \textit{knew} various propositions. It seemed plausible at the time that this is enough to think those propositions should be incorporated into the table we use to represent her decision. That's some evidence against the idea that more than knowledge, perhaps iterated knowledge or certainty, is needed before we add propositions to the decision table.

\subsection{From Decision Theory to Interest-Relativity}

This way of thinking about decision problems offers a new perspective on the issue of whether we should always be prepared to bet on what we know.\footnote{This issue is of course central to the plotline in \cite{Hawthorne2004}.} To focus intuitions, let's take a concrete case. Barry is sitting in his apartment one evening when he hears a musician performing in the park outside. The musician, call her Beth, is one of Barry's favourite musicians, so the music is familiar to Barry. Barry is excited that Beth is performing in his neighbourhood, and he decides to hurry out to see the show. As he prepares to leave, a genie appears an offers him a bet.\footnote{Assume, perhaps implausibly, that the sudden appearance of the genie is evidentially irrelevant to the proposition that the musician is Beth. The reasons this may be implausible are related to the arguments in \cite[14-15]{RunyonGuysDolls}. Thanks here to Jeremy Fantl.} If he takes the bet, and the musician is Beth, then the genie will give Barry ten dollars. On the other hand, if the musician is not Beth, he will be tortured in the fires of hell for a millenium. Let's put Barry's options in table form.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{Musician is Beth} & \textbf{Musician is not Beth} \\
\textbf{Take Bet} & Win \$10 & 1000 years of torture \\
\textbf{Decline Bet} & Status quo & Status quo \\
\end{tabular}
\end{center}

\noindent Intuitively, it is extremely irrational for Barry to take the bet. People do make mistakes about identifying musicians, even very familiar musicians, by the strains of music that drift up from a park. It's not worth risking a millenium of torture for \$10.

But it also seems that we've misstated the table. Before the genie showed up, it seemed clear that Barry knew that the musician was Beth. That was why he went out to see her perform. (If you don't think this is true, make the sounds from the park clearer, or make it that Barry had some prior evidence that Beth was performing which the sounds from the park remind him of. It shouldn't be too hard to come up with an evidential base such that (a) in normal circumstances we'd say Barry knew who was performing, but (b) he shouldn't take this genie's bet.) Now our decision tables should reflect the knowledge of the agent making the decision. If Barry knows that the musician is Beth, then the second column is one he knows will not obtain. So let's write the table in the standard form.

\begin{center}
\begin{tabular}{r c c}
 & \textbf{Musician is Beth} &  \\
\textbf{Take Bet} & Win \$10  \\
\textbf{Decline Bet} & Status quo &  \\
\end{tabular}
\end{center}

\noindent And it is clear what Barry's decision should be in this situation. Taking the bet dominates declining it, and Barry should take dominating options.

What has happened? It is incredibly clear that Barry should decline the bet, yet here we have an argument that he should take the bet. If you accept that the bet should be declined, then it seems to me that there are three options available.

\begin{enumerate*}
\item Barry never knew that the musician was Beth.
\item Barry did know that the musician was Beth, but this knowledge was destroyed by the genie's offer of the bet.
\item States of the world that are known not to obtain should still be represented in decision problems, so taking the bet is not a dominating option.
\end{enumerate*}

\noindent The first option is basically a form of scepticism. If the take-away message from the above discussion is that Barry doesn't know the musician is Beth, we can mount a similar argument to show that he knows next to nothing.\footnote{The idea that interest-relativity is a way of fending off scepticism is a very prominent theme in \cite{FantlMcGrath2009}.} And the third option would send us back into the problems about interpreting and applying decision theory that we spent the first few pages trying to get out of.

So it seems that the best solution here, or perhaps the least bad solution, is to accept that knowledge is interest-relative. Barry did know that the musician was Beth, but the genie's offer destroyed that knowledge. When Barry was unconcerned with bets at extremely long odds on whether the musician is Beth, he knows Beth is the musician. Now that he is interested in those bets, he doesn't know that.\footnote{On the version of IRI I'm defending, Barry is free to be interested in whatever he likes. If he started wondering about whether it would be rational to take such a bet, he loses the knowledge that Beth is the musician, even if there is no genie and the bet isn't offered. The existence of the genie's offer makes the bet a practical interest; merely wondering about the genie's offer makes the bet a cognitive interest. But both kinds of interests are relevant to knowledge.}

The argument here bears more than a passing resemblance to the arguments in favour of interest-relativity that are made by Hawthorne, Stanley, and Fantl and McGrath. But I think the focus on decision theory shows how we can get to interest-relativity with very weak premises.\footnote{As they make clear in their \citeyearpar{Hawthorne2008-HAWKAA}, Hawthorne and Stanley are interested in defending relatively strong premises linking knowledge and action independently of the argument for the interest-relativity of knowledge. What I'm doing here is showing how that conclusion does not rest on anything nearly as strong as the principles they believe, and so there is plenty of space to disagree with their general principles, but accept interest-relativity. The strategy here isn't a million miles from the point noted in Fantl and McGrath \citeyearpar[72n14]{FantlMcGrath2009} when they note that much weaker premises than the ones they endorse imply a failure of `purism'.} In particular, the only premises I've used to derive an interest-relative conclusion are:

\begin{enumerate*}
\item Before the genie showed up, Barry knew the musician was Beth.
\item It's rationally permissible, \textit{in cases like Barry's}, to take dominating options.
\item It's always right to model decision problems by including what the agent knows in the `framework'. That is, our decision tables should include what the agent knows about the payoffs in different states, and leave off any state the agent knows not to obtain.
\item It is rationally impermissible for Barry to take the genie's offered bet.
\end{enumerate*}

\noindent The second premise there is \textit{much} weaker than the principles linking knowledge and action defended in previous arguments for interest-relativity. It isn't the claim that one can always act on what one knows, or that one can only act on what one knows, or that knowledge always (or only) provides reason to act. It's just the claim that in one very specific type of situation, in particular when one has to make a relatively simple bet, which affects nobody but the person making the bet, it's rationally permissible to take a dominating option. In conjunction with the third premise, it entails that \textit{in those kind of cases}, the fact that one knows taking the bet will lead to a better outcome suffices for making acceptance of the bet rationally permissible. It doesn't say anything about what else might or might not make acceptance rationally permissible. It doesn't say anything about what suffices for rationally permissibility in other kinds of cases, such as cases where someone else's interests are at stake, or where taking the bet might violate a deontological constraint, or any other way in which real-life choices differ from the simplest decision problems.\footnote{I have more to say about those cases in section 2.2.} It doesn't say anything about any other kind of permissibility, e.g., moral permissibility. But it doesn't need to, because we're only in the business of proving that there is \textit{some} interest-relativity to knowledge, and an assumption about practical rationality in some range of cases suffices to prove that.\footnote{Also note that I'm not taking as a premise any claim about what Barry knows after the bet is offered. A lot of work on interest-relativity has used such premises, or premises about related intuitions. This seems like a misuse of the method of cases to me. That's not because we should never use intuitions about cases, just that these cases are too hard to think that snap judgments about them are particularly reliable. In general, we can know a lot about cases by quickly reflecting on them. Similarly, we know a lot about which shelves are level and which are uneven by visual inspection, i.e., `eyeballing'. But when different eyeballs disagree, it's time to bring in other tools. That's the approach of this paper. I don't have a story about why the various eyeballs disagree about cases like Barry's; that seems like a task best undertaken by a psychologist not a philosopher \citep{Ichikawa2009}.}

The case of Barry and Beth also bears some relationship to one of the kinds of case that have motivated contextualism about knowledge. Indeed, it has been widely noted in the literature on interest-relativity that interest-relativity can explain away many of the puzzles that motivate contextualism. And there are difficulties that face any contextualist theory \citep{Weatherson2006-WEAQC}. So I prefer an \textit{invariantist} form of interest-relativity about knowledge. That is, my view is a form of interest-relative-invariantism, or IRI.\footnote{This is obviously not a full argument against contextualism; that would require a much longer paper than this.}

Now everything I've said here leaves it open whether the interest-relativity of knowledge is a natural and intuitive theory, or whether it is a somewhat unhappy concession to difficulties that the case of Barry and Beth raise. I think the former is correct, and interest-relativity is fairly plausible on its own merits, but it would be consistent with my broader conclusions to say that in fact the interest-relative theory of knowledge is very implausible and counterintuitive. If we said that, we could still justify the interest-relative theory by noting that we have on our hands here a paradoxical situation, and any option will be somewhat implausible. This consideration has a bearing on how we should think about the role of intuitions about cases, or principles, in arguments that knowledge is interest-relative. Several critics of the view have argued that the view is counter-intuitive, or that it doesn't accord with the reactions of non-expert judges.\footnote{See, for instance, \cite{MBT2009}, or \cite{FeltzZarpentine2010}.} In a companion paper, ``Defending Interest-Relative Invariantism'', I note that those arguments usually misconstrue what the consequences of interest-relative theories of knowledge are. But even if they don't, I don't think there's any quick argument that if interest-relativity is counter-intuitive, it is false. After all, the only alternatives that seem to be open here are very counter-intuitive.

Finally, it's worth noting that if Barry is rational, he'll stop (fully) believing that the musician is Beth once the genie makes the offer. Assuming the genie allows this, it would be very natural for Barry to try to acquire more information about the singer. He might walk over to the window to see if he can see who is performing in the park. So this case leaves it open whether the interest-relativity of knowledge can be explained fully by the interest-relativity of belief. I used to think it could be; I no longer think that. To see why this is so, it's worth rehearsing how the interest-relative theory of belief runs.