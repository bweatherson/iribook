Here's a problem for interest-relative theories. (The form is one I learned from Tom Donaldson.) Consider a circumstance where someone has a fairly clear vision that p. For instance, they see someone they recognise across a room. They are then offered a very long odds bet on p, long enough that it would be intuitively irrational to take the bet. We want to say that they don't, in virtue of the bet, know that p. And so it can't be that p isn't part of their evidence, if E->K. 

But why isn't it rational to take the bet? We sort of want to say that it is because their evidence isn't good enough. But why isn't their evidence good enough? After all, in normal circumstances, p would be part of their evidence. So we want to say that the bet means that p isn't part of their evidence, but the reason that the bet changes things is that their evidence isn't good enough to take the bet. It feels like there is a circle here, and perhaps a vicious one.

The solution is to think like a game-theorist. There are multiple solutions to the set of questions: what is their evidence? what do they know? what should they do? And there are principles for choosing among these solutions. And these principles get us to roughly the right answers. The principles won't get us quite everything we intuitively wanted, I suspect, but we shouldn't fear being slightly revisionary.

Start with a plausible reliabilist notion of evidence. A proposition p is part of S's evidence if some system that is reliable in the circumstances outputs p. 'Reliable' is vague, twice over. There is a generality problem issue, what are similar circumstances. And there is a threshold issue, what is it to be reliable enough. So at this stage there is no determinate answer to the question what is S's evidence. Rather, there are a set of notions, evidence-1, evidence-2, etc.

But we can do better than this. Each concept of evidence can evaluate the resultant state of the agent believing and acting as if one of the other evidence concepts delimited their evidence. Some of these other concepts will lead to reckless action. Some other concepts will lead to losing knowledge; i.e., not believing things that were available to be known. 

But we don't have to stop there. Think of what we do in games like Stag Hunt, where there are multiple equilibria. We look for the risk-optimal equilibria. To a first approximation (a very first) we look for the expected value of each choice given that the other party may randomly choose from among the equilibria. (Actually, there are a few technical choices here, hopefully that won't matter.)

Let's say that's our theory of evidence. The evidence is the risk-optimal equilibrium solution to the game. This is a bit like Fara on vagueness; interests determine which of the vague choices is the correct one in a situation. So when the stakes change, the way that various solutions evaluate other solutions will change, so the risk-optimal equilibrium changes, so the evidence changes, so the rational action changes.

It's all a bit of a hack, and I'm not committed to the details. But it shows how we could hang on to evidentialism and interest-relativity about evidence.