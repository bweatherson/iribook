## The Structure of Decision Problems {#structure}

So far I've argued that to believe $p$ is to be disposed, at least sometimes, to use a decision table where $p$ is true in every possibility. In this section I plan to argue that it is appropriate to use a decision table where $p$ is true in every possibility only if one knows that $p$. Now one way to argue for that is as follows.

1. It's appropriate to do something (partially) constitutive of believing that $p$ only if it is appropriate to believe that $p$.
2. Using a decision table where $p$ is true in every possibility is (partially) constitutive of believing that $p$.
3. It is only appropriate to believe that $p$ if one knows that $p$.
4. Therefore, it is appropriate to use a decision table where $p$ is true in every possibility only if one knows that $p$.

I think that argument is not implausible, but the premises are much more controversial than I'd like to appeal to at this stage. So instead I'm going to argue directly for the conclusion. And this argument will provide some indirect support for the theory of belief that I've offered. After all, if knowledge is a constraint on what we can use in decisions in this way, then this needs to be explained. And at that point we might look to an argument like this one not as a potential justification of the conclusion, but as an explanation of it.

Arguing for the conclusion directly also helps motivate something like the converse of the conclusion. If $p$ is known, then there is a certain kind of mistake one can not make by using a decision table where $p$ is true in all columns. We might naturally enough call this mistake an epistemic mistake. We might be making other mistakes; for a lot of decision problems, and a lot of things we know, the optimal decision table will be neutral on whether those claims are true. Including everything we know in every decision table leads to clutter. But I suspect we can distinguish between decision tables that are epistemically flawed - because they treat as ruled out things that should not be ruled out - and decision tables that are flawed in virtue of being cluttered. The possibility of making this distinction should become clearer as we go along.

As always, let's start with an example. Professor Dec is teaching introductory decision theory to her undergraduate class. She is trying to introduce the notion of a dominant choice. So she introduces the following problem, with two states, $S_1$ and $S_2$, and two choices, $C_1$ and $C_2$, as is normal for introductory problems.

  ------- -------- --------
           $S_1$    $S_2$
    $C_1$  -$200   $1000
    $C_2$  -$100   $1500
  ------- -------- --------

She's hoping that the students will see that $C_1$ and $C_2$ are bets, but $C_2$ is clearly the better bet. If $S_1$ is actual, then both bets lose, but $C_2$ loses less money. If $S_2$ is actual, then both bets win, but $C_2$ wins more. So $C_2$ is better. That analysis is clearly wrong if the state is causally dependent on the choice, and controversial if the states are evidentially dependent on the choices. But Professor Dec has not given any reason for the students to think that the states are dependent on the choices in either way, and in fact the students don't worry about that kind of dependence.

That doesn't mean, however, that the students all adopt the analysis that Professor Dec wants them to. One student, Stu, is particularly unwilling to accept that $C_2$ is better than $C_1$. He thinks, on the basis of his experience, that when more than $1000 is on the line,
people aren't as reliable about paying out on bets. So while $C_1$ is guaranteed to deliver $1000 if $S_2$, if the agent bets on $C_2$, she might face some difficulty in collecting on her money.

Given the context, i.e., that they are in an undergraduate decision theory class, it seems that Stu has misunderstood the question that Professor Dec intended to ask. But it is a little harder than it first seems to specify just exactly what Stu's mistake is. It isn't that he thinks Professor Dec has misdescribed the situation. It isn't that he thinks the agent won't collect $1500 if she chooses $C_2$ and is in $S_2$. He just thinks that she *might* not be able to collect it, so the expected payout might really be a little less than $1500.

But Stu is not the only problem that Professor Dec has. She also has trouble convincing Dom of the argument. He thinks there should be a third state added, $S_3$. In $S_3$, there is a vengeful God who is about to end the world, and take everyone who chose $C_1$ to heaven, while sending everyone who chose $C_2$ to hell. Since heaven is better than hell, $C_2$ does not dominate $C_1$; it is worse in $S_3$. If decision theory is to be useful, we must say something about why we can leave states like $S_3$ off the decision table.

So in order to teach decision theory, Professor Dec has to answer two questions.

1. What makes it legitimate to write something on the decision table, such as the '$1500' we write in the bottom right cell of Dec's table?
2.  What makes it legitimate to leave something off a decision table, such as leaving Dom's state $S_3$ off the table?

When we've talked about decision tables so far, the focus has been on what tables thinkers actually use. Here we are switching the focus to talk about what tables they should use. And the claim is going to be that what they should use is determined by what they know.

To get to that conclusion, start with a much simpler problem. Mireille is out of town on a holiday, and she faces the following decision choice concerning what to do with a token in her hand.

  --------------------- -------------
             **Choice**  **Outcome**
     Put token on table  Win $1000
    Put token in pocket  Win nothing
  --------------------- -------------

This looks easy, especially if we've taken Professor Dec's class. Putting the token on the table dominates putting the token in her pocket. It returns $1000, versus no gain. So she should put the token on the table.

I've left Mireille's story fairly schematic; let's fill in some of the details. Mireille is on holiday at a casino. It's a fair casino; the probabilities of the outcomes of each of the games is just what you'd expect. And Mireille knows this. The table she's standing at is a roulette table. The token is a chip from the casino worth $1000. Putting the token on the table means placing a bet. As it turns out, it means placing a bet on the roulette wheel landing on 28. If that bet wins she gets her token back and another token of the same value. There are many other bets she could make, but Mireille has decided not to make all but one
of them. Since her birthday is the 28$^{\text{th}}$, she is tempted to put a bet on 28; that's the only bet she is considering. If she makes this bet, the objective chance of her winning is $\nicefrac{1}{38}$, and she knows this. As a matter of fact she will win, but she doesn't know this. (This is why the description in the table I presented above is truthful, though frightfully misleading.) As you can see, the odds on this bet are terrible. She should have a chance of winning around $\nicefrac{1}{2}$ to justify placing this bet. (It's a very unfair casino, but what can you expect at a vacation resort?) So the above table, which makes it look like placing the bet is the dominant, and hence rational, option, is misleading.

Just how is the table misleading though? It isn't because what is says is false. If Mireille puts the token on the table she wins $1000; and if she doesn't, she stays where she is. It isn't, or isn't just, that Mireille doesn't believe the table reflects what will happen if she places the bet. As it turns out, Mireille is smart, so she doesn't form beliefs about chance events like roulette wheels. But even if she did, that wouldn't change how misleading the table is. The table suggests that it is rational for Mireille to put the token on the table. In fact, that is irrational. And it would still be irrational if Mireille believes, irrationally, that the wheel will land on 28.

A better suggestion is that the table is misleading because Mireille doesn't know that it accurately depicts the choice she faced. If she did know that these were the outcomes to putting the token on the table versus in her pocket, it would be rational for her to put it on the table. If we take it as understood in a presentation of a decision problem that the agent knows that the table accurately depicts the outcomes of various choices in different states, then we can tell a plausible story about the miscommunication between Professor Dec and her students. Stu was assuming that if the agent wins $1500, she might not be able to easily collect. That is, he was assuming that the agent does not know that she'll get $1500 if she chooses $C_2$ and is in state $S_2$. Professor Dec, if she's anything like other decision theory professors, will have assumed that the agent did know exactly that. And the miscommunication between Professor Dec and Dom also concerns knowledge. When Dec wrote that table up, she was saying that the agent knew that $S_1$ or $S_2$ obtained. And when she says it is best to take dominating options, she means that it is best to take options that one knows to have better outcomes. So here are the answers to Stu and Dom's challenges.

- It is legitimate to write something on the decision table, such as the '$1500' we write in the bottom right cell of Dec's table, iff the decision maker knows it to be true.
-  It is legitimate to leave something off a decision table, such as leaving Dom's state $S_3$ off the table, iff the decision maker knows it not to obtain.

Perhaps those answers are not correct, but what we can clearly see by reflecting on these cases is that the standard presentation of a decision problem presupposes not just that the table states what will happen, but the agent stands in some special doxastic relationship to the information explicitly on the table (such as that the chooser in Professor Dec's example will get $1500 if $C_2$ and $S_2$) and implied by where the table ends (such as that $S_3$ will not happen). 

Could that relationship be weaker than knowledge? Could it, for example, be rational true belief. There are a couple of reasons to think that it couldn't be.

For one thing, there is the example of Coraline friom the previous chapter. She rationally believes that $p$, but can't rationally take a bet that wins if $p$. So the table that just has one column, where $p$ is true, can't be appropriate. And it isn't appropriate because it doesn't properly represent the risk she runs.

But even if you didn't agree with me about Coraline, you should be sceptical about this proposal. Think about the reasons @Williamson2000 [Ch. 9] gives for thinking justified true belief can't be enough for evidence. To put Williamson's argument in Lewisian terms, knowledge is a much more natural relation than rational (or justified) true belief. And when ascribing contents, especially contents of tacitly held beliefs, we should strongly prefer
to ascribe more rather than less natural contents.

So the 'special doxastic relationship' is not weaker than knowledge. Could it be stronger? Could it be, for example, that the relationship is certainty, or some kind of iterated knowledge? Plausibly in some game-theoretic settings it is stronger - it involves not just knowing that the table is accurate, but knowing that the other player knows the table is accurate. In some cases, the standard treatment of games will require positing even more iterations of knowledge. For convenience, it is sometimes explicitly stated that iterations continue indefinitely, so each party knows the table is correct, and knows each party knows this, and knows each party knows that, and knows each party knows *that*, and so on. An early example of this in philosophy is in the work by David @Lewis1969 on convention. But it is usually acknowledged (again in a tradition extending back at least to Lewis) that only the first few iterations are actually needed in any problem, and it seems a mistake to attribute more iterations than are actually used in deriving solutions to any particular game.

The reason that would be a mistake is that we want game theory, and decision theory, to be applicable to real-life situations. There is very little that we know, and know that we know, and know we know we know, and so on indefinitely [@Williamson2000, Ch. 4]. There is, perhaps, even less that we are certain of. If we only could say that a person is making a particular decision when they stand in these very strong relationships to the parameters of the decision table, then people will almost never be making the kinds of decision we study in decision theory. Since decision theory and game theory are not meant to be that impractical, I conclude that the 'special doxastic relationship' cannot be that strong. It could be that in some games, the special relationship will involve a few iterations of knowledge, but in decision problems, where the epistemic states of others are irrelevant, even that is unnecessary, and simple knowledge seems sufficient.

It might be argued here that we shouldn't expect to apply decision theory directly to real-life problems, but only to idealised versions of them, so it would be acceptable to, for instance, require that the things we put in the table are, say, things that have probability exactly 1. In real life, virtually nothing has probability 1. In an idealisation, many things do. But to argue this way seems to involve using 'idealisation' in an unnatural sense. There is a sense in which, whenever we treat something with non-maximal probability as simply given in a decision problem that we're ignoring, or abstracting away from, some complication. But we aren't idealising. On the contrary, we're modelling the agent as if they were irrationally certain in some things which are merely very very probable.

So it's better to say that any application of decision theory to a real-life problem will involve ignoring certain (counterfactual) logical or metaphysical possibilities in which the decision table is not actually true. But not any old abstraction will do. We can't ignore just anything, at least not if we want a good model. Which abstractions are acceptable? The response I've offered to Dom's challenge suggests an answer to this: we can abstract away from any possibility in which something the agent actually knows is false. I don't have a knock-down argument that this is the best of all possible abstractions, but nor do I know of any alternative answer to the question which abstractions are acceptable which is nearly as plausible.

We might be tempted to say that we can abstract away from anything such that the difference between its probability and 1 doesn't make a difference to the ultimate answer to the decision problem. More carefully, the idea would be that we can have the decision table represent that $p$ iff $p$ is true and treating $\Pr(p)$ as 1 rather than its actual value doesn't change what the agent should do. I think this is the most plausible story one could tell about decision tables if one didn't like the knowledge first story that I tell. But I also don't think it works, because of cases like the following.

Luc is lucky; he's in a casino where they are offering better than fair odds on roulette. Although the chance of winning any bet is , if Luc bets $10, and his bet wins, he will win $400. (That's the only bet on offer.) Luc, like Mireille, is considering betting on 28. As it turns out, 28 won't come up, although since this is a fair roulette wheel, Luc doesn't know this. Luc, like most agents, has a declining marginal utility for money. He currently has $1,000, and for any amount of money $x$, Luc gets utility $u(x) = x^{\nicefrac{1}{2}}$ out of having $x$. So Luc's current utility (from money) is, roughly, 31.622. If he bets and loses, his utility will be, roughly, 31.464. And if he bets and wins, his utility will be, roughly, 37.417. So he stands to gain about 5.794, and to lose about 0.159. So he stands to gain about 36.5 as much as he stands to lose. Since the odds of winning are less than $\nicefrac{1}{36.5}$, his expected utility goes down if he takes the bet, so he shouldn't take it. Of course, if the probability of losing was 1, and not merely $\nicefrac{37}{38}$, he shouldn't take the bet too. Does that mean it is acceptable, in presenting Luc's decision problem, to leave off the table any possibility of him winning, since he won't win, and setting the probability of losing to 1 rather than $\nicefrac{37}{38}$ doesn't change the decision he should make? Of course not; that would horribly misstate the situation Luc finds himself in. It would misrepresent how sensitive Luc's choice is to his utility function, and to the size of the stakes. If Luc's utility function was $u(x) = x^{\nicefrac{3}{4}}$, then he should take the bet. If his utility function is unchanged, but the bet was $1 against $40, rather than $10 against $400, he should take the bet. Leaving off the possibility of winning hides these facts, and badly misrepresents Luc's situation.

I've argued that the states we can 'leave off' a decision table are the states that the agent knows not to obtain. The argument is largely by elimination. If we can only leave off things that have probability 1, then decision theory would be useless; but it isn't. If we say we can leave off things if setting their probability at 1 is an acceptable idealisation, we need a theory of acceptable idealisations. If this is to be a rival to my theory, the idealisation had better not be it's acceptable to treat anything known as having probability 1. But the most natural alternative idealisation badly misrepresents Luc's case. If we say that what can be left off is not what's known not to obtain, but what is, say, justifiably truly believed not to obtain, we need an argument for why people would naturally use such an unnatural standard. This doesn't even purport to be a conclusive argument, but these considerations point me towards thinking that knowledge determines what we can leave off.

I also cheated a little in making this argument. When I described Mireille in the casino, I made a few explicit comments about her information states. And every time, I said that she knew various propositions. It seemed plausible at the time that this is enough to think those propositions should be incorporated into the table we use to represent her decision. That's some evidence against the idea that more than knowledge, perhaps iterated knowledge or certainty, is needed before we add propositions to the decision table.